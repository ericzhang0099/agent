# 因果推理研究 - 思维导图

## 1. Judea Pearl因果推断理论

### 1.1 因果层级 (Ladder of Causation)
```
Level 3: 反事实 (Counterfactuals)
    └─ "如果我当时...会怎样?"
    └─ P(Y_x | X', Y')

Level 2: 干预 (Intervention)  
    └─ "如果我做X，会怎样?"
    └─ P(Y | do(X))

Level 1: 关联 (Association)
    └─ "观察到X，对Y的了解?"
    └─ P(Y | X)
```

### 1.2 核心概念
- **潜在结果 (Potential Outcomes)**: Y(0), Y(1)
- **因果效应**: Y(1) - Y(0)
- **基本问题**: 无法同时观测Y(0)和Y(1)
- **SUTVA**: 无干扰、无多版本处理
- **一致性**: Y = Y(A)

### 1.3 三大框架
```
┌─────────────────────────────────────────┐
│  潜在结果框架 (Rubin)                   │
│  - 反事实推理                           │
│  - 可忽略性假设                         │
│  - 适合实验设计                         │
├─────────────────────────────────────────┤
│  结构方程模型 (Pearl)                   │
│  - 确定性函数                           │
│  - 外生噪声                             │
│  - 机制建模                             │
├─────────────────────────────────────────┤
│  因果图模型 (DAG)                       │
│  - 图形化表示                           │
│  - D-分离                               │
│  - 直观易懂                             │
└─────────────────────────────────────────┘
```

## 2. 因果图模型 (Causal Graphs)

### 2.1 DAG基础
```
节点 (Nodes): 随机变量 V = {V₁, ..., Vₚ}
边 (Edges): 有向边 E ⊆ V × V
属性: 无环 (Acyclic)
```

### 2.2 图术语
```
父节点 Pa(Vⱼ) ──┐
               ├──→ Vⱼ
子节点 Ch(Vⱼ) ──┘

祖先 An(Vⱼ) ──→ ... ──→ Vⱼ ──→ ... ──→ 后代 De(Vⱼ)
```

### 2.3 D-分离 (D-Separation)
```
路径阻塞条件:
┌────────────────────────────────────────┐
│ 链 X→M→Y      │ M∈C  → 阻塞           │
│ 分叉 X←M→Y    │ M∈C  → 阻塞           │
│ 碰撞 X→M←Y    │ M∉C且De(M)∩C=∅ → 阻塞 │
└────────────────────────────────────────┘
```

### 2.4 马尔可夫性质
```
分解: P(V₁,...,Vₚ) = ∏ⱼ P(Vⱼ | Pa(Vⱼ))

全局马尔可夫: C d-分离 A,B ⇒ A ⟂ B | C

忠实性: A ⟂ B | C ⇔ C d-分离 A,B
```

### 2.5 后门准则 (Backdoor Criterion)
```
条件:
1. Z中没有X的后代
2. Z阻塞所有X到Y的后门路径

调整公式:
P(Y|do(X=x)) = ∑_z P(Y|X=x,Z=z)P(Z=z)
```

## 3. 反事实推理 (Counterfactuals)

### 3.1 反事实定义
```
结构方程模型中的反事实:

观测世界: Y = f_Y(X, ε_Y)
反事实世界: Y(x) = f_Y(x, ε_Y)

关键: 噪声ε_Y保持不变!
```

### 3.2 三步推理
```
1. 外生变量估计 (Abduction)
   └─ 使用观测证据估计噪声

2. 干预 (Action)
   └─ 修改结构方程 (do(X=x))

3. 预测 (Prediction)
   └─ 使用估计的噪声计算结果
```

### 3.3 反事实查询类型
```
┌────────────────────────────────────────┐
│ ETT: E[Y(1)-Y(0)|A=1]                  │
│   治疗对治疗的效应                      │
├────────────────────────────────────────┤
│ PN: P(Y(0)=0|A=1,Y=1)                  │
│   必要性概率                            │
├────────────────────────────────────────┤
│ PS: P(Y(1)=1|A=0,Y=0)                  │
│   充分性概率                            │
├────────────────────────────────────────┤
│ PNS: P(Y(1)=1,Y(0)=0)                  │
│   必要且充分概率                        │
└────────────────────────────────────────┘
```

### 3.4 中介分析
```
自然直接效应: E[Y(1,M(0)) - Y(0,M(0))]
自然间接效应: E[Y(1,M(1)) - Y(1,M(0))]

总效应 = 直接效应 + 间接效应
```

## 4. Do-Calculus计算

### 4.1 符号
```
G:       原始因果图
G_X̄:     移除X入边的图
G_X̲:     移除X出边的图
do(x):   干预操作符
```

### 4.2 三条规则

#### 规则1: 观测的插入/删除
```
P(y|do(x),z,w) = P(y|do(x),w)

条件: Y ⟂ Z | X,W 在 G_X̄ 中
```

#### 规则2: 行动/观测交换
```
P(y|do(x),do(z),w) = P(y|do(x),z,w)

条件: Y ⟂ Z | X,W 在 G_X̄,Z̲ 中

特例 (后门准则):
P(y|do(x),w) = P(y|x,w)
条件: Y ⟂ X | W 在 G_X̲ 中
```

#### 规则3: 行动的插入/删除
```
P(y|do(x),do(z),w) = P(y|do(x),w)

条件: Y ⟂ Z | X,W 在 G_X̄,Z̄(W) 中

其中 Z(W) = Z中不是W祖先的节点
```

### 4.3 后门调整推导
```
P(y|do(x))
= ∑_z P(y|do(x),z)P(z|do(x))    [全概率]
= ∑_z P(y|do(x),z)P(z)           [规则3: Z ⟂ X in G_X̄]
= ∑_z P(y|x,z)P(z)               [规则2: Y ⟂ X|Z in G_X̲]
```

### 4.4 前门准则
```
适用条件:
1. M阻塞所有X到Y的有向路径
2. X到M无后门路径
3. M到Y的后门路径被X阻塞

公式:
P(y|do(x)) = ∑_m P(m|x) ∑_{x'} P(y|m,x')P(x')
```

## 5. 因果发现算法 (PC/GES)

### 5.1 因果发现概述
```
目标: 从观测数据推断因果结构

方法分类:
┌────────────────────────────────────────┐
│ 基于约束 (Constraint-based)            │
│ - PC算法                               │
│ - FCI算法                              │
│ - 使用条件独立性测试                    │
├────────────────────────────────────────┤
│ 基于评分 (Score-based)                 │
│ - GES算法                              │
│ - NOTEARS                              │
│ - 优化评分函数                          │
├────────────────────────────────────────┤
│ 基于函数因果模型 (FCM)                  │
│ - LiNGAM                               │
│ - ANM (Additive Noise Model)           │
│ - 利用非高斯性/非线性                   │
└────────────────────────────────────────┘
```

### 5.2 PC算法
```
输入: 数据D, 显著性水平α
输出: CPDAG

步骤:
1. 骨架学习
   └─ 从完全图开始
   └─ 条件独立性测试
   └─ 移除独立变量间的边
   
2. V-结构定向
   └─ X-Y-Z且Y不在分离集
   └─ 定向为 X→Y←Z
   
3. 定向传播
   └─ 避免有向环
   └─ 尽可能多地定向边

假设:
- 因果充分性
- 忠实性
- 正确的CI测试
```

### 5.3 GES算法
```
输入: 数据D, 评分函数
输出: CPDAG

步骤:
1. 前向阶段
   └─ 从空图开始
   └─ 贪婪添加边
   └─ 最大化评分函数
   
2. 后向阶段
   └─ 贪婪删除边
   └─ 进一步优化评分

评分函数:
- BIC: log P(D|G) - (d/2)log n
- BDeu: 贝叶斯评分

性质:
- 一致性 (大样本下收敛)
- 局部最优
```

### 5.4 算法比较
```
┌──────────┬─────────────┬─────────────┐
│ 特性      │ PC算法       │ GES算法      │
├──────────┼─────────────┼─────────────┤
│ 类型      │ 基于约束     │ 基于评分     │
│ 复杂度    │ O(p^q)       │ 多项式       │
│ 样本效率  │ 需要大样本   │ 中等样本     │
│ 错误传播  │ 会传播       │ 较鲁棒       │
│ 输出      │ CPDAG        │ CPDAG        │
└──────────┴─────────────┴─────────────┘
```

### 5.5 其他方法
```
LiNGAM:
- 假设: 线性 + 非高斯噪声
- 方法: ICA
- 优势: 可识别方向

FCI:
- 处理未观测混杂
- 输出PAG
- 更一般化

NOTEARS:
- 连续优化
- 约束无环性
- 计算效率高
```

## 6. 应用场景

### 6.1 医学研究
```
问题: 药物X对疾病Y的效应?
混杂: 年龄、性别、病史
方法: 后门调整、倾向得分匹配
```

### 6.2 经济学
```
问题: 教育对收入的效应?
混杂: 能力、家庭背景
方法: 工具变量、前门准则
```

### 6.3 机器学习
```
问题: 模型泛化、公平性
应用: 因果发现、反事实解释
方法: 因果表示学习
```

## 7. 关键公式总结

### 7.1 因果效应
```
ATE = E[Y(1) - Y(0)]
CATE = E[Y(1) - Y(0) | X=x]
ETT = E[Y(1) - Y(0) | A=1]
```

### 7.2 识别公式
```
后门调整:
P(y|do(x)) = ∑_z P(y|x,z)P(z)

前门调整:
P(y|do(x)) = ∑_m P(m|x)∑_{x'}P(y|m,x')P(x')

IPW:
E[Y|do(X=1)] = E[Y*A/e(X)]
其中 e(X) = P(A=1|X)
```

### 7.3 结构方程
```
Vⱼ = fⱼ(Pa(Vⱼ), εⱼ)

干预 do(Vⱼ=v):
将方程替换为 Vⱼ = v
```

## 8. 学习路径

```
入门:
├─ 《The Book of Why》(Pearl)
├─ 《Causal Inference in Statistics》(Pearl et al.)
└─ 理解因果层级和基本直觉

进阶:
├─ 《Causality》(Pearl)
├─ 《Elements of Causal Inference》(Peters et al.)
└─ 掌握do-calculus和识别理论

深入:
├─ 《Causation, Prediction, and Search》(Spirtes et al.)
├─ 因果发现算法
└─ 反事实和中介分析

实践:
├─ DoWhy库 (Microsoft)
├─ causal-learn库
└─ 实际项目应用
```
