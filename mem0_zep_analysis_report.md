# Mem0 & Zep (Graphiti) å¼€æºä»£ç æ ¸å¿ƒç®—æ³•ç ”ç©¶æŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ·±å…¥åˆ†æäº†Mem0å’ŒZep(Graphiti)ä¸¤ä¸ªé¢†å…ˆçš„å¼€æºAIè®°å¿†ç³»ç»Ÿçš„æ ¸å¿ƒç®—æ³•ï¼Œä»¥åŠPineconeå‘é‡æ•°æ®åº“çš„æœ€ä½³å®è·µã€‚é€šè¿‡å¯¹æ¯”ç ”ç©¶ï¼Œæå–äº†å¯ç›´æ¥åº”ç”¨äºç”Ÿäº§ç¯å¢ƒçš„å…³é”®æŠ€æœ¯å’Œæ¶æ„è®¾è®¡ã€‚

**å…³é”®å‘ç°ï¼š**
- Mem0åœ¨æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŠ è½½é€Ÿåº¦å¿«86.5%ï¼Œèµ„æºæ¶ˆè€—æ›´ä½
- Zep(Graphiti)åœ¨å¤æ‚æ—¶åºæ¨ç†ä»»åŠ¡ä¸Šå‡†ç¡®ç‡é«˜18.5%ï¼Œä½†æˆæœ¬æ›´é«˜
- æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾è°±+å…¨æ–‡ï¼‰æ˜¯è¡Œä¸šå…±è¯†çš„æœ€ä½³å®è·µ

---

## 1. Mem0 æ ¸å¿ƒç®—æ³•åˆ†æ

### 1.1 æ¶æ„æ¦‚è¿°

Mem0é‡‡ç”¨**ä¸‰å±‚å­˜å‚¨æ¶æ„**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Mem0 Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Vector Databases (19+ providers supported)    â”‚
â”‚     - Semantic similarity search                        â”‚
â”‚     - Cosine similarity / Euclidean distance            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Graph Database (Optional Neo4j)               â”‚
â”‚     - Entity relationship tracking                      â”‚
â”‚     - Multi-hop traversal                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: SQLite History                                â”‚
â”‚     - Complete audit trail                              â”‚
â”‚     - Memory operation logging                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒåˆ›æ–°ï¼šLLM-as-Memory-Manager

Mem0å°†LLMä½œä¸ºæ™ºèƒ½è®°å¿†ç®¡ç†å™¨ï¼Œè€Œéå•çº¯çš„å†…å®¹ç”Ÿæˆå™¨ï¼š

**è®°å¿†ç®¡ç†å†³ç­–ç±»å‹ï¼š**
- `ADD` - æ·»åŠ æ–°è®°å¿†
- `UPDATE` - æ›´æ–°ç°æœ‰è®°å¿†
- `DELETE` - åˆ é™¤è¿‡æ—¶è®°å¿†
- `NONE` - æ— éœ€æ“ä½œ

### 1.3 è®°å¿†å‹ç¼©å¼•æ“ç®—æ³•

**ç®—æ³•æµç¨‹ï¼š**

```python
# Mem0 è®°å¿†å‹ç¼©æ ¸å¿ƒé€»è¾‘ï¼ˆä¼ªä»£ç ï¼‰
class MemoryCompressor:
    def compress_memory(self, messages, user_id):
        """
        1. æå–å…³é”®äº‹å®
        2. æ£€æµ‹çŸ›ç›¾ä¿¡æ¯
        3. åˆå¹¶ç›¸ä¼¼è®°å¿†
        4. ç”Ÿæˆå‹ç¼©è¡¨ç¤º
        """
        # Step 1: ä½¿ç”¨LLMæå–ç»“æ„åŒ–äº‹å®
        facts = self.llm.extract_facts(messages)
        
        # Step 2: æ£€ç´¢ç›¸å…³ç°æœ‰è®°å¿†
        existing_memories = self.vector_store.search(
            query=messages,
            user_id=user_id,
            top_k=10
        )
        
        # Step 3: LLMå†³ç­–ï¼ˆADD/UPDATE/DELETE/NONEï¼‰
        decisions = self.llm.evaluate_memories(
            new_facts=facts,
            existing_memories=existing_memories
        )
        
        # Step 4: æ‰§è¡Œå†³ç­–
        for decision in decisions:
            if decision.action == "ADD":
                self.add_memory(decision.fact, user_id)
            elif decision.action == "UPDATE":
                self.update_memory(decision.memory_id, decision.fact)
            elif decision.action == "DELETE":
                self.delete_memory(decision.memory_id)
        
        return decisions
```

**å…³é”®ä¼˜åŒ–ç‚¹ï¼š**
- æ¯æ¬¡æ“ä½œéœ€è¦2+æ¬¡LLMè°ƒç”¨
- ä½¿ç”¨SQLiteè®°å½•å®Œæ•´æ“ä½œå†å²
- æ”¯æŒ19+ç§å‘é‡æ•°æ®åº“åç«¯

### 1.4 å‘é‡+å›¾è°±æ··åˆæ£€ç´¢

**æ£€ç´¢æµç¨‹ï¼š**

```python
class HybridRetriever:
    def search(self, query, user_id, top_k=10):
        # 1. å‘é‡è¯­ä¹‰æœç´¢
        vector_results = self.vector_store.similarity_search(
            query=query,
            user_id=user_id,
            top_k=top_k
        )
        
        # 2. å›¾è°±å…³ç³»æœç´¢ï¼ˆå¦‚å¯ç”¨ï¼‰
        if self.graph_enabled:
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“
            entities = self.extract_entities(query)
            
            # å›¾è°±éå†è·å–ç›¸å…³å®ä½“
            graph_results = self.graph_store.traverse(
                entities=entities,
                depth=2,  # 2-hop traversal
                user_id=user_id
            )
            
            # 3. ç»“æœèåˆï¼ˆReciprocal Rank Fusionï¼‰
            combined = self.reciprocal_rank_fusion(
                vector_results, 
                graph_results
            )
            return combined
        
        return vector_results
```

### 1.5 è‡ªé€‚åº”ä¸ªæ€§åŒ–æœºåˆ¶

**å¤šå±‚çº§è®°å¿†æ¶æ„ï¼š**

```python
# ä¼šè¯æ ‡è¯†ç¬¦å±‚çº§
memory.add(messages, user_id="john_doe")      # ç”¨æˆ·çº§ï¼ˆæŒä¹…ï¼‰
memory.add(messages, agent_id="support_v2")   # ä»£ç†çº§ï¼ˆè¡Œä¸ºç‰¹å®šï¼‰
memory.add(messages, run_id="session_123")    # è¿è¡Œçº§ï¼ˆä¸´æ—¶ï¼‰
```

**è®°å¿†ä¼˜å…ˆçº§ç®—æ³•ï¼š**
- é«˜é¢‘è®¿é—®è®°å¿†è‡ªåŠ¨æå‡æƒé‡
- æ—¶é—´è¡°å‡å› å­ï¼šolder memories get lower priority
- ç›¸å…³æ€§åé¦ˆå¾ªç¯

---

## 2. Zep (Graphiti) æ ¸å¿ƒç®—æ³•åˆ†æ

### 2.1 æ—¶åºçŸ¥è¯†å›¾è°±æ¶æ„

Zepé‡‡ç”¨**ä¸‰å±‚å­å›¾æ¶æ„**ç®¡ç†æ—¶åºä¿¡æ¯ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Zep Temporal Knowledge Graph                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Community Subgraph (ğ’¢c) - æœ€é«˜å±‚                           â”‚
â”‚     â”œâ”€â”€ ä½¿ç”¨Label Propagationç®—æ³•æ£€æµ‹ç¤¾åŒº                    â”‚
â”‚     â”œâ”€â”€ ç¤¾åŒºæ‘˜è¦ï¼ˆMap-Reduceç”Ÿæˆï¼‰                          â”‚
â”‚     â””â”€â”€ å…¨å±€ä¸Šä¸‹æ–‡ç†è§£                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Semantic Entity Subgraph (ğ’¢s) - è¯­ä¹‰å±‚                     â”‚
â”‚     â”œâ”€â”€ Entity Nodes: åç§° + æ‘˜è¦                          â”‚
â”‚     â”œâ”€â”€ Entity Edges: å…³ç³»ç±»å‹ + Fact                      â”‚
â”‚     â””â”€â”€ æ—¶åºå±æ€§: valid_at, invalid_at                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Episode Subgraph (ğ’¢e) - åŸå§‹æ•°æ®å±‚                         â”‚
â”‚     â”œâ”€â”€ Episodic Nodes: åŸå§‹æ¶ˆæ¯/æ–‡æœ¬/JSON                 â”‚
â”‚     â”œâ”€â”€ Episodic Edges: è¿æ¥åˆ°æå–çš„å®ä½“                   â”‚
â”‚     â””â”€â”€ åŒå‘ç´¢å¼•: æ”¯æŒæº¯æºå¼•ç”¨                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 åŒæ—¶åºæ¨¡å‹ï¼ˆBi-temporal Modelï¼‰

**æ ¸å¿ƒåˆ›æ–°ï¼š** åŒºåˆ†ä¸¤ä¸ªæ—¶é—´çº¿

| æ—¶é—´çº¿ | ç¬¦å· | ç”¨é€” |
|--------|------|------|
| äº‹ä»¶æ—¶é—´ | T | äº‹å®å‘ç”Ÿçš„æ—¶é—´ï¼ˆvalid_at/invalid_atï¼‰ |
| äº‹åŠ¡æ—¶é—´ | T' | æ•°æ®å½•å…¥ç³»ç»Ÿçš„æ—¶é—´ï¼ˆcreated/expiredï¼‰ |

```python
# æ—¶åºäº‹å®ç®¡ç†æ ¸å¿ƒä»£ç ï¼ˆæ¦‚å¿µå®ç°ï¼‰
class TemporalFactManager:
    def add_fact(self, fact, reference_timestamp):
        """
        æ·»åŠ å¸¦æ—¶åºä¿¡æ¯çš„äº‹å®
        """
        # 1. æå–æ—¶åºä¿¡æ¯
        temporal_info = self.extract_temporal(
            fact=fact,
            reference_time=reference_timestamp
        )
        
        # 2. æ£€æŸ¥å†²çª/è¿‡æ—¶è¾¹
        existing_edges = self.find_similar_edges(fact)
        
        for edge in existing_edges:
            if self.is_contradictory(fact, edge):
                # 3. å¤±æ•ˆæ—§è¾¹
                self.invalidate_edge(
                    edge_id=edge.id,
                    invalid_at=temporal_info.valid_at
                )
        
        # 4. åˆ›å»ºæ–°è¾¹
        new_edge = self.create_edge(
            fact=fact,
            valid_at=temporal_info.valid_at,
            invalid_at=temporal_info.invalid_at,
            created_at=now()  # T' timeline
        )
        
        return new_edge
```

### 2.3 å®ä½“å…³ç³»æŠ½å–ç®—æ³•

**æŠ½å–æµç¨‹ï¼ˆå››æ­¥æµæ°´çº¿ï¼‰ï¼š**

```python
# Step 1: å®ä½“æŠ½å–
ENTITY_EXTRACTION_PROMPT = """
Given the conversation, extract entity nodes from the CURRENT MESSAGE:
Guidelines:
1. ALWAYS extract the speaker/actor as the first node
2. Extract other significant entities, concepts, or actors
3. DO NOT create nodes for relationships or actions
4. DO NOT create nodes for temporal information
5. Be as explicit as possible in node names
"""

# Step 2: å®ä½“æ¶ˆæ­§ï¼ˆEntity Resolutionï¼‰
ENTITY_RESOLUTION_PROMPT = """
Given EXISTING NODES and NEW NODE, determine if they represent the same entity.
Return: is_duplicate (bool), existing_uuid (if duplicate), merged_name
"""

# Step 3: äº‹å®æŠ½å–
FACT_EXTRACTION_PROMPT = """
Given MESSAGES and ENTITIES, extract all facts between the provided entities:
- Each fact represents a clear relationship between two DISTINCT nodes
- relation_type: concise, all-caps description (e.g., LOVES, WORKS_FOR)
- fact: detailed description with all relevant information
"""

# Step 4: äº‹å®æ¶ˆæ­§ï¼ˆFact Resolutionï¼‰
FACT_RESOLUTION_PROMPT = """
Determine if New Edge represents the same factual information as any Existing Edge.
Facts don't need to be identical, just express the same information.
"""

# Step 5: æ—¶åºæŠ½å–
TEMPORAL_EXTRACTION_PROMPT = """
Extract time information from the fact:
- valid_at: when the relationship became true
- invalid_at: when the relationship stopped being true
Use ISO 8601 format. Calculate actual dates from relative mentions.
"""
```

### 2.4 ä¸Šä¸‹æ–‡ç»„è£…ç®—æ³•

**ä¸‰æ­¥æ£€ç´¢æµç¨‹ï¼š**

```
Query (Î±) â†’ Search (Ï†) â†’ Rerank (Ï) â†’ Constructor (Ï‡) â†’ Context (Î²)
```

**1. æœç´¢é˜¶æ®µ (Ï†)ï¼š**

```python
class GraphSearcher:
    def search(self, query, scopes=['edges', 'nodes', 'communities']):
        results = {
            'edges': [],      # â„°s - è¯­ä¹‰è¾¹ï¼ˆäº‹å®ï¼‰
            'nodes': [],      # ğ’©s - å®ä½“èŠ‚ç‚¹
            'communities': [] # ğ’©c - ç¤¾åŒºèŠ‚ç‚¹
        }
        
        # 1.1 è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ï¼ˆCosineï¼‰
        query_embedding = self.embed(query)
        if 'edges' in scopes:
            results['edges'] = self.vector_search(
                embedding=query_embedding,
                index='fact_embeddings',
                top_k=20
            )
        
        # 1.2 å…¨æ–‡æœç´¢ï¼ˆBM25ï¼‰
        if 'nodes' in scopes:
            results['nodes'] = self.fulltext_search(
                query=query,
                fields=['name', 'summary'],
                top_k=20
            )
        
        # 1.3 å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰
        if self.use_bfs:
            # "Land and Expand" ç­–ç•¥
            seed_nodes = results['nodes'][:5]
            bfs_results = self.bfs_traversal(
                seed_nodes=seed_nodes,
                depth=2
            )
            results['edges'].extend(bfs_results)
        
        return results
```

**2. é‡æ’åºé˜¶æ®µ (Ï)ï¼š**

```python
class Reranker:
    def rerank(self, query, search_results, method='rrf'):
        """
        æ”¯æŒçš„é‡æ’åºæ–¹æ³•ï¼š
        - RRF (Reciprocal Rank Fusion): èåˆå¤šåˆ—è¡¨ç»“æœ
        - MMR (Maximal Marginal Relevance): å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
        - Cross-encoder: LLMé‡æ’åºï¼Œè´¨é‡æœ€é«˜ä½†æˆæœ¬æœ€é«˜
        - Node Distance: åŸºäºå›¾è·ç¦»çš„åç½®
        - Episode Mentions: åŸºäºæåŠé¢‘ç‡
        """
        if method == 'rrf':
            return self.reciprocal_rank_fusion(search_results)
        elif method == 'mmr':
            return self.maximal_marginal_relevance(
                query=query,
                results=search_results,
                lambda_param=0.5  # å¹³è¡¡ç›¸ä¼¼åº¦å’Œå¤šæ ·æ€§
            )
        elif method == 'cross_encoder':
            return self.cross_encoder_rerank(
                query=query,
                results=search_results
            )
```

**3. æ„é€ é˜¶æ®µ (Ï‡)ï¼š**

```python
class ContextConstructor:
    def construct(self, reranked_results):
        """
        å°†èŠ‚ç‚¹å’Œè¾¹è½¬æ¢ä¸ºLLMå¯ç”¨çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        # æ·»åŠ äº‹å®ï¼ˆå¸¦æ—¶åºèŒƒå›´ï¼‰
        facts_str = "\n".join([
            f"{edge.fact} (Date range: {edge.valid_at} - {edge.invalid_at or 'present'})"
            for edge in reranked_results['edges']
        ])
        context_parts.append(f"<FACTS>\n{facts_str}\n</FACTS>")
        
        # æ·»åŠ å®ä½“æ‘˜è¦
        entities_str = "\n".join([
            f"{node.name}: {node.summary}"
            for node in reranked_results['nodes']
        ])
        context_parts.append(f"<ENTITIES>\n{entities_str}\n</ENTITIES>")
        
        # æ·»åŠ ç¤¾åŒºæ‘˜è¦ï¼ˆå¦‚å¯ç”¨ï¼‰
        if reranked_results.get('communities'):
            communities_str = "\n".join([
                f"{comm.name}: {comm.summary}"
                for comm in reranked_results['communities']
            ])
            context_parts.append(f"<COMMUNITIES>\n{communities_str}\n</COMMUNITIES>")
        
        return "\n\n".join(context_parts)
```

### 2.5 ç¤¾åŒºæ£€æµ‹ä¸åŠ¨æ€æ›´æ–°

**Label Propagation åŠ¨æ€æ‰©å±•ï¼š**

```python
class CommunityManager:
    def build_communities(self):
        """ä½¿ç”¨Label Propagationç®—æ³•æ£€æµ‹ç¤¾åŒº"""
        communities = self.graph.run("""
            CALL gds.labelPropagation.stream('entity-graph')
            YIELD nodeId, communityId
            RETURN communityId, collect(nodeId) as members
        """)
        
        # ä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆæ‘˜è¦
        for comm in communities:
            summary = self.generate_community_summary(comm.members)
            self.create_community_node(
                members=comm.members,
                summary=summary,
                name=self.extract_keywords(summary)
            )
    
    def dynamic_update(self, new_entity):
        """åŠ¨æ€æ·»åŠ æ–°å®ä½“åˆ°ç¤¾åŒºï¼ˆæ— éœ€é‡æ–°è®¡ç®—ï¼‰"""
        # æŸ¥çœ‹é‚»å±…èŠ‚ç‚¹çš„ç¤¾åŒº
        neighbor_communities = self.get_neighbor_communities(new_entity)
        
        # é€‰æ‹©å¤šæ•°é‚»å±…æ‰€å±çš„ç¤¾åŒº
        if neighbor_communities:
            assigned_community = max(set(neighbor_communities), 
                                    key=neighbor_communities.count)
            self.add_to_community(new_entity, assigned_community)
            self.update_community_summary(assigned_community)
        else:
            # åˆ›å»ºæ–°ç¤¾åŒº
            self.create_new_community(new_entity)
```

---

## 3. Pinecone æœ€ä½³å®è·µ

### 3.1 ç´¢å¼•è®¾è®¡ä¼˜åŒ–

**æ··åˆç´¢å¼•æ¶æ„ï¼š**

```python
# Pinecone æ··åˆæœç´¢ç´¢å¼•è®¾è®¡
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="YOUR_API_KEY")

# åˆ›å»ºæ”¯æŒå¯†é›†+ç¨€ç–å‘é‡çš„æ··åˆç´¢å¼•
index = pc.create_index(
    name="hybrid-memory-index",
    dimension=1536,  # å¯†é›†å‘é‡ç»´åº¦ï¼ˆå¦‚OpenAI text-embedding-3-largeï¼‰
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    ),
    # å¯ç”¨ç¨€ç–å‘é‡æ”¯æŒï¼ˆç”¨äºBM25å…³é”®è¯æœç´¢ï¼‰
    vector_type="dense"  # æˆ–ä½¿ç”¨ "sparse" / "hybrid"
)
```

**è®°å½•ç»“æ„è®¾è®¡ï¼š**

```python
# ç»“æ„åŒ–IDè®¾è®¡
{
    "_id": "user123#memory#2024-01-15#001",  # user_id#type#date#sequence
    "chunk_text": "ç”¨æˆ·åå¥½ç´ é£Ÿä¸»ä¹‰é¥®é£Ÿ...",
    
    # å…ƒæ•°æ®ç”¨äºè¿‡æ»¤
    "user_id": "user123",
    "memory_type": "preference",
    "created_at": "2024-01-15T10:30:00Z",
    "category": "dietary",
    "confidence": 0.95,
    "access_count": 5,
    "last_accessed": "2024-01-20T15:22:00Z",
    
    # å…³è”ä¿¡æ¯
    "related_entities": ["ç´ é£Ÿ", "å¥åº·", "ç¯ä¿"],
    "source_session": "session_456"
}
```

### 3.2 æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜

**æ··åˆæœç´¢å®ç°ï¼š**

```python
class PineconeHybridSearch:
    def __init__(self, index):
        self.index = index
        
    def hybrid_search(self, query, user_id, top_k=10, alpha=0.5):
        """
        alpha: å¹³è¡¡å¯†é›†å’Œç¨€ç–å‘é‡çš„æƒé‡
               0.0 = çº¯å…³é”®è¯æœç´¢
               1.0 = çº¯è¯­ä¹‰æœç´¢
               0.5 = å‡è¡¡æ··åˆ
        """
        # 1. ç”Ÿæˆå¯†é›†å‘é‡ï¼ˆè¯­ä¹‰ï¼‰
        dense_vector = self.embeddings.embed(query)
        
        # 2. ç”Ÿæˆç¨€ç–å‘é‡ï¼ˆBM25ï¼‰
        sparse_vector = self.bm25_encode(query)
        
        # 3. æ‰§è¡Œæ··åˆæŸ¥è¯¢
        results = self.index.query(
            namespace="memories",
            vector=dense_vector,
            sparse_vector=sparse_vector,
            top_k=top_k,
            filter={
                "user_id": {"$eq": user_id}
            },
            include_metadata=True,
            # æ··åˆæƒé‡é…ç½®
            alpha=alpha
        )
        
        return results
```

**å…ƒæ•°æ®è¿‡æ»¤ä¼˜åŒ–ï¼š**

```python
# é«˜æ•ˆè¿‡æ»¤ç­–ç•¥
# 1. æ—¶é—´èŒƒå›´è¿‡æ»¤
recent_memories = index.query(
    vector=query_vector,
    filter={
        "created_at": {"$gte": "2024-01-01"},
        "user_id": {"$eq": "user123"}
    }
)

# 2. åˆ†ç±»+ç½®ä¿¡åº¦ç»„åˆè¿‡æ»¤
high_confidence_prefs = index.query(
    vector=query_vector,
    filter={
        "$and": [
            {"category": {"$eq": "preference"}},
            {"confidence": {"$gte": 0.8}},
            {"user_id": {"$eq": "user123"}}
        ]
    }
)

# 3. ä½¿ç”¨INæ“ä½œç¬¦è¿›è¡Œå¤šå€¼åŒ¹é…
multi_category = index.query(
    vector=query_vector,
    filter={
        "category": {"$in": ["preference", "fact", "goal"]}
    }
)
```

### 3.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

| ä¼˜åŒ–ç»´åº¦ | å»ºè®® | é¢„æœŸæ”¶ç›Š |
|----------|------|----------|
| ç´¢å¼•åˆ†åŒº | æŒ‰user_idä½¿ç”¨namespaceéš”ç¦» | æŸ¥è¯¢é€Ÿåº¦æå‡50%+ |
| å‘é‡ç»´åº¦ | ä½¿ç”¨text-embedding-3-small (1536d) | æˆæœ¬é™ä½90%ï¼Œç²¾åº¦æŸå¤±<2% |
| æ‰¹é‡æ“ä½œ | upsertä½¿ç”¨æ‰¹æ¬¡ï¼ˆ100-1000æ¡ï¼‰ | ååé‡æå‡10x |
| å…ƒæ•°æ®ç´¢å¼• | ä»…ç´¢å¼•å¸¸ç”¨è¿‡æ»¤å­—æ®µ | å­˜å‚¨æˆæœ¬é™ä½30% |
| ç¨€ç–å‘é‡ | å¯¹å…³é”®è¯æ•æ„Ÿåœºæ™¯å¯ç”¨ | å…³é”®è¯å¬å›ç‡æå‡40% |

---

## 4. å¯ç›´æ¥é›†æˆçš„ä»£ç ç‰‡æ®µ

### 4.1 Mem0é£æ ¼è®°å¿†å‹ç¼©

```python
import json
from typing import List, Dict, Literal
from dataclasses import dataclass

@dataclass
class MemoryDecision:
    action: Literal["ADD", "UPDATE", "DELETE", "NONE"]
    content: str
    memory_id: str = None
    reason: str = None

class MemoryCompressor:
    """Mem0é£æ ¼çš„è®°å¿†å‹ç¼©å¼•æ“"""
    
    def __init__(self, llm_client, vector_store):
        self.llm = llm_client
        self.store = vector_store
        
    def compress(self, messages: List[Dict], user_id: str) -> List[MemoryDecision]:
        # 1. æå–äº‹å®
        extraction_prompt = """
        ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å…³é”®äº‹å®ï¼ˆç”¨æˆ·åå¥½ã€ä¸ªäººä¿¡æ¯ã€é‡è¦äº‹ä»¶ç­‰ï¼‰ã€‚
        è¿”å›JSONæ•°ç»„æ ¼å¼: [{"fact": "...", "category": "...", "importance": 1-10}]
        
        å¯¹è¯:
        {messages}
        """
        
        facts = self.llm.extract(extraction_prompt.format(
            messages=json.dumps(messages, ensure_ascii=False)
        ))
        
        # 2. æ£€ç´¢ç›¸å…³è®°å¿†
        query = " ".join([m["content"] for m in messages[-3:]])
        existing = self.store.search(query, user_id=user_id, top_k=5)
        
        # 3. å†³ç­–
        decision_prompt = """
        åŸºäºæ–°æå–çš„äº‹å®å’Œç°æœ‰è®°å¿†ï¼Œå†³å®šæ¯ä¸ªäº‹å®çš„æ“ä½œ:
        - ADD: æ·»åŠ ä¸ºæ–°è®°å¿†
        - UPDATE: æ›´æ–°ç°æœ‰è®°å¿†ï¼ˆæä¾›memory_idï¼‰
        - DELETE: åˆ é™¤è¿‡æ—¶è®°å¿†ï¼ˆæä¾›memory_idï¼‰
        - NONE: æ— éœ€æ“ä½œ
        
        æ–°äº‹å®: {facts}
        ç°æœ‰è®°å¿†: {existing}
        
        è¿”å›JSON: {{"decisions": [{{"action": "...", "content": "...", "memory_id": "...", "reason": "..."}}]}}
        """
        
        decisions = self.llm.decide(decision_prompt.format(
            facts=json.dumps(facts, ensure_ascii=False),
            existing=json.dumps(existing, ensure_ascii=False)
        ))
        
        return [MemoryDecision(**d) for d in decisions["decisions"]]
```

### 4.2 Zepé£æ ¼æ—¶åºçŸ¥è¯†å›¾è°±

```python
from datetime import datetime
from typing import Optional, List
import neo4j

class TemporalKnowledgeGraph:
    """Zepé£æ ¼çš„æ—¶åºçŸ¥è¯†å›¾è°±"""
    
    def __init__(self, neo4j_driver):
        self.driver = neo4j_driver
        
    def add_episode(self, content: str, user_id: str, 
                    timestamp: datetime, episode_type: str = "message"):
        """æ·»åŠ åŸå§‹å¯¹è¯è®°å½•ï¼ˆEpisodeï¼‰"""
        with self.driver.session() as session:
            session.run("""
                CREATE (e:Episode {
                    id: randomUUID(),
                    content: $content,
                    user_id: $user_id,
                    created_at: $timestamp,
                    type: $episode_type
                })
                RETURN e.id as episode_id
            """, content=content, user_id=user_id, 
                 timestamp=timestamp.isoformat(), episode_type=episode_type)
    
    def add_fact(self, subject: str, predicate: str, object: str,
                 valid_at: datetime, invalid_at: Optional[datetime] = None,
                 source_episode_id: str = None):
        """æ·»åŠ å¸¦æ—¶åºçš„äº‹å®ï¼ˆEdgeï¼‰"""
        with self.driver.session() as session:
            # 1. åˆ›å»ºæˆ–è·å–å®ä½“èŠ‚ç‚¹
            session.run("""
                MERGE (s:Entity {name: $subject})
                MERGE (o:Entity {name: $object})
                
                // 2. åˆ›å»ºæ—¶åºè¾¹
                CREATE (s)-[r:FACT {
                    id: randomUUID(),
                    predicate: $predicate,
                    valid_at: $valid_at,
                    invalid_at: $invalid_at,
                    created_at: datetime()
                }]->(o)
                
                // 3. è¿æ¥åˆ°æºepisode
                WITH r
                MATCH (e:Episode {id: $episode_id})
                CREATE (e)-[:EXTRACTED_FROM]->(r)
                
                RETURN r.id as fact_id
            """, subject=subject, predicate=predicate, object=object,
                 valid_at=valid_at.isoformat(),
                 invalid_at=invalid_at.isoformat() if invalid_at else None,
                 episode_id=source_episode_id)
    
    def invalidate_fact(self, fact_id: str, invalid_at: datetime):
        """ä½¿äº‹å®å¤±æ•ˆï¼ˆå¤„ç†ä¿¡æ¯æ›´æ–°ï¼‰"""
        with self.driver.session() as session:
            session.run("""
                MATCH ()-[r:FACT {id: $fact_id}]->()
                SET r.invalid_at = $invalid_at
            """, fact_id=fact_id, invalid_at=invalid_at.isoformat())
    
    def temporal_search(self, query: str, user_id: str, 
                       as_of: Optional[datetime] = None) -> List[Dict]:
        """æ—¶åºæ„ŸçŸ¥æœç´¢"""
        with self.driver.session() as session:
            # æ„å»ºæ—¶åºè¿‡æ»¤æ¡ä»¶
            time_filter = ""
            if as_of:
                time_filter = """
                    AND r.valid_at <= $as_of 
                    AND (r.invalid_at IS NULL OR r.invalid_at > $as_of)
                """
            
            result = session.run(f"""
                // 1. è¯­ä¹‰æœç´¢æ‰¾åˆ°ç›¸å…³å®ä½“
                CALL db.index.vector.queryNodes('entity-embeddings', 10, $query_embedding)
                YIELD node as matched_entity
                
                // 2. æ‰¾åˆ°ç›¸å…³äº‹å®ï¼ˆå¸¦æ—¶åºè¿‡æ»¤ï¼‰
                MATCH (matched_entity)-[r:FACT]-(related:Entity)
                WHERE matched_entity.user_id = $user_id
                {time_filter}
                
                // 3. è¿”å›æœ‰æ•ˆäº‹å®
                RETURN matched_entity.name as subject,
                       r.predicate as predicate,
                       related.name as object,
                       r.valid_at as valid_from,
                       r.invalid_at as valid_until
                ORDER BY r.valid_at DESC
            """, query_embedding=self.embed(query), 
                 user_id=user_id, 
                 as_of=as_of.isoformat() if as_of else None)
            
            return [dict(record) for record in result]
```

### 4.3 æ··åˆæ£€ç´¢èåˆ

```python
import numpy as np
from typing import List, Dict

class HybridRetriever:
    """å‘é‡+å›¾è°±+å…¨æ–‡æ··åˆæ£€ç´¢"""
    
    def __init__(self, vector_store, graph_store, fulltext_index):
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.fulltext = fulltext_index
        
    def reciprocal_rank_fusion(self, results_lists: List[List[Dict]], 
                                k: int = 60) -> List[Dict]:
        """RRFç®—æ³•èåˆå¤šè·¯å¬å›ç»“æœ"""
        scores = {}
        
        for results in results_lists:
            for rank, item in enumerate(results):
                doc_id = item["id"]
                if doc_id not in scores:
                    scores[doc_id] = {"item": item, "score": 0}
                # RRFå…¬å¼: 1 / (k + rank)
                scores[doc_id]["score"] += 1.0 / (k + rank + 1)
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        fused = sorted(scores.values(), key=lambda x: x["score"], reverse=True)
        return [x["item"] for x in fused]
    
    def search(self, query: str, user_id: str, top_k: int = 10) -> List[Dict]:
        # 1. å‘é‡è¯­ä¹‰æœç´¢
        vector_results = self.vector_store.search(
            query=query, user_id=user_id, top_k=top_k
        )
        
        # 2. å…¨æ–‡å…³é”®è¯æœç´¢
        fulltext_results = self.fulltext.search(
            query=query, filters={"user_id": user_id}, top_k=top_k
        )
        
        # 3. å›¾è°±å…³ç³»æœç´¢
        graph_results = self.graph_store.traverse(
            query=query, user_id=user_id, depth=2, top_k=top_k
        )
        
        # 4. èåˆç»“æœ
        combined = self.reciprocal_rank_fusion([
            vector_results,
            fulltext_results,
            graph_results
        ])
        
        return combined[:top_k]
```

---

## 5. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 5.1 æ¶æ„è®¾è®¡å€Ÿé‰´

**æ¨èæ¶æ„ï¼ˆèåˆMem0+Zepä¼˜ç‚¹ï¼‰ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Memory System                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Layer                                                  â”‚
â”‚    â”œâ”€â”€ Memory.add(messages) â†’ å¼‚æ­¥å¤„ç†                      â”‚
â”‚    â”œâ”€â”€ Memory.search(query) â†’ æ··åˆæ£€ç´¢                      â”‚
â”‚    â””â”€â”€ Memory.get_history(user_id) â†’ å®¡è®¡æ—¥å¿—               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Processing Layer                                           â”‚
â”‚    â”œâ”€â”€ Extractor: LLMæå–å®ä½“/äº‹å®/æ—¶åº                     â”‚
â”‚    â”œâ”€â”€ Compressor: è®°å¿†å‹ç¼©/å»é‡/æ›´æ–°å†³ç­–                   â”‚
â”‚    â””â”€â”€ Embedder: ç”Ÿæˆå‘é‡è¡¨ç¤º                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Storage Layer                                              â”‚
â”‚    â”œâ”€â”€ Vector Store (Pinecone/Qdrant): è¯­ä¹‰æ£€ç´¢             â”‚
â”‚    â”œâ”€â”€ Graph DB (Neo4j): å…³ç³»éå†+æ—¶åºç®¡ç†                  â”‚
â”‚    â””â”€â”€ Document Store (MongoDB): åŸå§‹æ•°æ®+å…ƒæ•°æ®            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Retrieval Layer                                            â”‚
â”‚    â”œâ”€â”€ Semantic Search: å‘é‡ç›¸ä¼¼åº¦                          â”‚
â”‚    â”œâ”€â”€ Full-text Search: BM25/TF-IDF                        â”‚
â”‚    â”œâ”€â”€ Graph Traversal: BFS/å¤šè·³å…³ç³»                        â”‚
â”‚    â””â”€â”€ Reranker: RRF/MMR/Cross-encoder                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 æˆæœ¬-å‡†ç¡®ç‡æƒè¡¡

æ ¹æ®å­¦æœ¯ç ”ç©¶æ•°æ®ï¼š

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | å»¶è¿Ÿ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|------|----------|
| çº¯å‘é‡(Mem0) | 7.5% | ä½ | ä½ | é«˜å¹¶å‘ã€é¢„ç®—æ•æ„Ÿ |
| çº¯å›¾è°±(Zep) | 11.1% | ä¸­ | é«˜ | å¤æ‚æ¨ç†ã€æ—¶åºå…³é”® |
| æ··åˆæ–¹æ¡ˆ | 10%+ | ä¸­ | ä¸­ | å¹³è¡¡é€‰æ‹©ï¼ˆæ¨èï¼‰ |
| å…¨ä¸Šä¸‹æ–‡ | 98% | é«˜ | æé«˜ | çŸ­å¯¹è¯ã€ç²¾åº¦ä¼˜å…ˆ |

**ä¼˜åŒ–å»ºè®®ï¼š**

1. **åˆ†å±‚å­˜å‚¨ç­–ç•¥**
   - çƒ­æ•°æ®ï¼ˆæœ€è¿‘7å¤©ï¼‰ï¼šå†…å­˜+å‘é‡ç´¢å¼•
   - æ¸©æ•°æ®ï¼ˆ7-90å¤©ï¼‰ï¼šå‘é‡æ•°æ®åº“
   - å†·æ•°æ®ï¼ˆ90å¤©+ï¼‰ï¼šå¯¹è±¡å­˜å‚¨+æŒ‰éœ€åŠ è½½

2. **æ™ºèƒ½ç¼“å­˜**
   - é«˜é¢‘æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆRedisï¼‰
   - ç”¨æˆ·ç”»åƒç¼“å­˜ï¼ˆæœ€è¿‘è®¿é—®çš„å®ä½“/åå¥½ï¼‰

3. **å¼‚æ­¥å¤„ç†**
   - è®°å¿†å†™å…¥å¼‚æ­¥åŒ–ï¼ˆé˜Ÿåˆ—å¤„ç†ï¼‰
   - ç¤¾åŒºæ£€æµ‹/æ‘˜è¦ç”Ÿæˆåå°ä»»åŠ¡

4. **æ¨¡å‹é€‰æ‹©**
   - å®ä½“æŠ½å–ï¼šè½»é‡çº§æ¨¡å‹ï¼ˆå¦‚Phi-3ï¼‰
   - é‡æ’åºï¼šä¸“ç”¨cross-encoder
   - åµŒå…¥ï¼štext-embedding-3-smallï¼ˆæ€§ä»·æ¯”æœ€ä¼˜ï¼‰

---

## 6. æ€»ç»“

### æ ¸å¿ƒç®—æ³•æå–

1. **è®°å¿†å‹ç¼©**ï¼šLLMå†³ç­–é©±åŠ¨çš„ADD/UPDATE/DELETEæœºåˆ¶
2. **æ··åˆæ£€ç´¢**ï¼šå‘é‡è¯­ä¹‰+å›¾è°±å…³ç³»+å…¨æ–‡å…³é”®è¯çš„ä¸‰è·¯å¬å›
3. **æ—¶åºç®¡ç†**ï¼šåŒæ—¶é—´çº¿æ¨¡å‹ï¼ˆäº‹ä»¶æ—¶é—´+äº‹åŠ¡æ—¶é—´ï¼‰
4. **ç»“æœèåˆ**ï¼šRRF/MMRé‡æ’åºç®—æ³•
5. **åŠ¨æ€ç¤¾åŒº**ï¼šLabel Propagationå¢é‡æ›´æ–°

### å¯ç›´æ¥é›†æˆçš„ç»„ä»¶

- è®°å¿†å‹ç¼©å¼•æ“ï¼ˆä»£ç ç‰‡æ®µ4.1ï¼‰
- æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆä»£ç ç‰‡æ®µ4.2ï¼‰
- æ··åˆæ£€ç´¢èåˆï¼ˆä»£ç ç‰‡æ®µ4.3ï¼‰

### æ¶æ„è®¾è®¡å»ºè®®

- **çŸ­æœŸ**ï¼šä½¿ç”¨Mem0é£æ ¼çš„å‘é‡+è½»é‡å‹ç¼©ï¼ˆå¿«é€Ÿè½åœ°ï¼‰
- **é•¿æœŸ**ï¼šå¼•å…¥Zepé£æ ¼çš„æ—¶åºå›¾è°±ï¼ˆå¤æ‚åœºæ™¯ï¼‰
- **å­˜å‚¨**ï¼šPineconeæ··åˆç´¢å¼•+Neo4jæ—¶åºå›¾è°±
- **æ£€ç´¢**ï¼šä¸‰è·¯å¬å›+RRFé‡æ’åº

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: 2025å¹´2æœˆ*
*æ•°æ®æ¥æº: Mem0 GitHub, Zep Graphitiè®ºæ–‡, Pineconeå®˜æ–¹æ–‡æ¡£, å­¦æœ¯ç ”ç©¶è®ºæ–‡*
