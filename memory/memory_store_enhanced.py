#!/usr/bin/env python3
"""
è®°å¿†å‹ç¼©ç³»ç»Ÿä¸ç°æœ‰Chromaå­˜å‚¨çš„é›†æˆé€‚é…å™¨
å®ç°æ— ç¼è¿ç§»å’Œå¢å¼ºåŠŸèƒ½
"""

import os
import sys
from typing import List, Dict, Optional, Any
from datetime import datetime

# å¯¼å…¥ç°æœ‰ç»„ä»¶
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from chroma_store import ChromaMemoryStore, MemoryRecord as ChromaMemoryRecord
from retrieval_service import MemoryRetrievalService, SearchResult
from memory_compression_system import (
    MemoryCompressionSystem, 
    MemoryRecord as CompressedMemoryRecord,
    MemoryImportanceScorer,
    MemoryCompressor,
    HybridRetriever,
    TextProcessor
)


class EnhancedMemoryStore:
    """
    å¢å¼ºç‰ˆè®°å¿†å­˜å‚¨
    æ•´åˆChromaå‘é‡å­˜å‚¨ + æ™ºèƒ½å‹ç¼©ç³»ç»Ÿ
    """
    
    def __init__(self, persist_dir: str = "./chroma_db", 
                 compression_enabled: bool = True):
        self.persist_dir = persist_dir
        self.compression_enabled = compression_enabled
        
        # åŸºç¡€Chromaå­˜å‚¨
        self.chroma_store = ChromaMemoryStore(persist_dir)
        
        # å‹ç¼©ç³»ç»Ÿ
        self.compression_system = MemoryCompressionSystem(
            storage_dir=os.path.join(persist_dir, "compressed"),
            config={
                "hot_storage_days": 7,
                "warm_storage_days": 30,
                "compression_threshold_high": 4.0,
                "compression_threshold_medium": 2.0,
                "compression_threshold_low": 1.0,
            }
        )
        
        # å°è¯•åŠ è½½å·²æœ‰æ•°æ®
        self.compression_system.load()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_added": 0,
            "total_compressed": 0,
            "total_searches": 0,
            "avg_compression_ratio": 1.0
        }
    
    def add_memory(self, content: str, source: str, memory_type: str,
                   metadata: Dict[str, Any] = None) -> str:
        """
        æ·»åŠ è®°å¿†ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æµç¨‹:
        1. æ·»åŠ åˆ°Chromaå‘é‡å­˜å‚¨ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰
        2. æ·»åŠ åˆ°å‹ç¼©ç³»ç»Ÿï¼ˆç”¨äºé‡è¦æ€§è¯„åˆ†å’Œå‹ç¼©ï¼‰
        """
        metadata = metadata or {}
        
        # 1. æ·»åŠ åˆ°Chromaå­˜å‚¨
        chroma_id = self.chroma_store.add_memory(content, source, memory_type, metadata)
        
        # 2. æ·»åŠ åˆ°å‹ç¼©ç³»ç»Ÿ
        if self.compression_enabled:
            compressed_memory = self.compression_system.add_memory(
                content=content,
                source=source,
                memory_type=memory_type,
                metadata=metadata
            )
            
            # å°†å‹ç¼©ä¿¡æ¯åŒæ­¥åˆ°metadata
            metadata['compressed_id'] = compressed_memory.id
            metadata['importance_score'] = compressed_memory.importance_score
            metadata['compression_level'] = compressed_memory.compression_level
        
        self.stats["total_added"] += 1
        
        return chroma_id
    
    def search(self, query: str, mode: str = "enhanced", n_results: int = 5,
               memory_type: Optional[str] = None) -> List[Dict]:
        """
        å¢å¼ºæ£€ç´¢
        
        æ¨¡å¼:
        - "semantic": çº¯è¯­ä¹‰æ£€ç´¢ï¼ˆChromaï¼‰
        - "keyword": çº¯å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰
        - "hybrid": æ··åˆæ£€ç´¢ï¼ˆChromaåŸç”Ÿï¼‰
        - "enhanced": å¢å¼ºæ··åˆï¼ˆè¯­ä¹‰+å…³é”®è¯+é‡è¦æ€§åŠ æƒï¼‰
        """
        self.stats["total_searches"] += 1
        
        if mode == "enhanced" and self.compression_enabled:
            # ä½¿ç”¨å¢å¼ºæ£€ç´¢ï¼ˆæ•´åˆå‹ç¼©ç³»ç»Ÿçš„é‡è¦æ€§è¯„åˆ†ï¼‰
            return self._enhanced_search(query, n_results, memory_type)
        else:
            # ä½¿ç”¨åŸç”ŸChromaæ£€ç´¢
            if mode == "semantic":
                return self.chroma_store.search_semantic(query, n_results, memory_type)
            elif mode == "keyword":
                return self.chroma_store.search_keyword(query, n_results)
            else:
                return self.chroma_store.search_hybrid(query, n_results, memory_type)
    
    def _enhanced_search(self, query: str, n_results: int,
                         memory_type: Optional[str] = None) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢å®ç°"""
        # 1. è·å–Chromaçš„è¯­ä¹‰æ£€ç´¢ç»“æœ
        semantic_results = self.chroma_store.search_semantic(
            query, n_results * 2, memory_type
        )
        
        # 2. è·å–å…³é”®è¯æ£€ç´¢ç»“æœ
        keyword_results = self.chroma_store.search_keyword(query, n_results * 2)
        
        # 3. åˆå¹¶å¹¶åŠ æƒ
        fused_scores = {}
        
        # è¯­ä¹‰ç»“æœåŠ æƒ
        for result in semantic_results:
            doc_id = result['id']
            similarity = 1 - result.get('distance', 0)
            fused_scores[doc_id] = {
                'score': similarity * 0.6,  # è¯­ä¹‰æƒé‡60%
                'data': result
            }
        
        # å…³é”®è¯ç»“æœåŠ æƒ
        for result in keyword_results:
            doc_id = result['id']
            keyword_score = min(result.get('score', 0) / 10, 1.0)
            if doc_id in fused_scores:
                fused_scores[doc_id]['score'] += keyword_score * 0.3  # å…³é”®è¯æƒé‡30%
            else:
                fused_scores[doc_id] = {
                    'score': keyword_score * 0.3,
                    'data': result
                }
        
        # 4. é‡è¦æ€§åŠ æƒï¼ˆä»å‹ç¼©ç³»ç»Ÿè·å–ï¼‰
        for doc_id in fused_scores:
            # å°è¯•è·å–é‡è¦æ€§è¯„åˆ†
            importance = self._get_importance_for_doc(doc_id)
            if importance > 0:
                # é«˜é‡è¦æ€§è®°å¿†è·å¾—é¢å¤–åŠ æˆï¼ˆæœ€é«˜10%ï¼‰
                fused_scores[doc_id]['score'] *= (1 + importance * 0.02)
        
        # 5. æ’åºå¹¶è¿”å›
        sorted_results = sorted(
            fused_scores.items(),
            key=lambda x: x[1]['score'],
            reverse=True
        )[:n_results]
        
        return [
            {
                'id': item[0],
                'enhanced_score': item[1]['score'],
                **item[1]['data']
            }
            for item in sorted_results
        ]
    
    def _get_importance_for_doc(self, doc_id: str) -> float:
        """è·å–æ–‡æ¡£çš„é‡è¦æ€§è¯„åˆ†"""
        # ä»metadataä¸­æå–
        try:
            if hasattr(self.chroma_store, 'collection'):
                result = self.chroma_store.collection.get(ids=[doc_id])
                if result and result.get('metadatas'):
                    metadata = result['metadatas'][0]
                    return metadata.get('importance_score', 0)
        except:
            pass
        return 0
    
    def compress_all(self) -> Dict:
        """æ‰§è¡Œå…¨é‡å‹ç¼©"""
        if not self.compression_enabled:
            return {"error": "Compression is disabled"}
        
        # å»é‡
        dedup_result = self.compression_system.deduplicate()
        
        # å‹ç¼©
        compress_result = self.compression_system.compress_all()
        
        # ä¿å­˜
        self.compression_system.save()
        
        return {
            "deduplication": dedup_result,
            "compression": compress_result
        }
    
    def get_memory_stats(self) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        chroma_stats = self.chroma_store.get_stats()
        compression_stats = self.compression_system.get_stats()
        
        return {
            "chroma": chroma_stats,
            "compression": compression_stats,
            "operations": self.stats
        }
    
    def migrate_from_chroma(self) -> Dict:
        """ä»ç°æœ‰Chromaå­˜å‚¨è¿ç§»æ•°æ®åˆ°å‹ç¼©ç³»ç»Ÿ"""
        if not hasattr(self.chroma_store, 'collection'):
            return {"error": "Chroma collection not available"}
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_data = self.chroma_store.collection.get()
            
            if not all_data or not all_data.get('documents'):
                return {"migrated": 0}
            
            migrated = 0
            for i, doc in enumerate(all_data['documents']):
                doc_id = all_data['ids'][i]
                metadata = all_data['metadatas'][i] if all_data.get('metadatas') else {}
                
                # æ·»åŠ åˆ°å‹ç¼©ç³»ç»Ÿ
                self.compression_system.add_memory(
                    content=doc,
                    source=metadata.get('source', 'unknown'),
                    memory_type=metadata.get('memory_type', 'general'),
                    metadata=metadata
                )
                migrated += 1
            
            # ä¿å­˜
            self.compression_system.save()
            
            return {"migrated": migrated}
            
        except Exception as e:
            return {"error": str(e)}


class MemoryOptimizer:
    """
    è®°å¿†ä¼˜åŒ–å™¨
    å®šæœŸæ‰§è¡Œå‹ç¼©ã€å»é‡ã€ç´¢å¼•ä¼˜åŒ–
    """
    
    def __init__(self, store: EnhancedMemoryStore):
        self.store = store
        self.optimization_log = []
    
    def run_optimization(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "steps": []
        }
        
        # Step 1: å»é‡
        print("ğŸ—‘ï¸  Step 1: å»é‡æ£€æµ‹...")
        dedup_result = self.store.compression_system.deduplicate()
        results["steps"].append({"name": "deduplication", "result": dedup_result})
        print(f"   å‘ç°é‡å¤: {dedup_result['duplicates_found']}")
        
        # Step 2: é‡æ–°è®¡ç®—é‡è¦æ€§
        print("ğŸ“Š Step 2: é‡æ–°è®¡ç®—é‡è¦æ€§è¯„åˆ†...")
        for memory in self.store.compression_system.memories.values():
            scorer = MemoryImportanceScorer()
            importance, factors = scorer.calculate_importance(memory)
            memory.importance_score = importance
            memory.importance_factors = factors
        print(f"   å·²æ›´æ–° {len(self.store.compression_system.memories)} æ¡è®°å¿†çš„é‡è¦æ€§è¯„åˆ†")
        
        # Step 3: å‹ç¼©
        print("ğŸ’¾ Step 3: æ‰§è¡Œå‹ç¼©...")
        compress_result = self.store.compression_system.compress_all()
        results["steps"].append({"name": "compression", "result": compress_result})
        print(f"   å‹ç¼©ç‡: {compress_result['compression_ratio']:.2%}")
        
        # Step 4: ä¿å­˜
        print("ğŸ’¾ Step 4: ä¿å­˜ä¼˜åŒ–ç»“æœ...")
        self.store.compression_system.save()
        
        # Step 5: é‡å»ºç´¢å¼•
        print("ğŸ” Step 5: é‡å»ºæ£€ç´¢ç´¢å¼•...")
        memories = list(self.store.compression_system.memories.values())
        self.store.compression_system.retriever.build_index(memories)
        print(f"   å·²ç´¢å¼• {len(memories)} æ¡è®°å¿†")
        
        self.optimization_log.append(results)
        return results
    
    def get_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        if not self.optimization_log:
            return "No optimization history"
        
        latest = self.optimization_log[-1]
        stats = self.store.get_memory_stats()
        
        report = f"""
# è®°å¿†ä¼˜åŒ–æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {latest['timestamp']}

## å­˜å‚¨ç»Ÿè®¡
- æ€»è®°å¿†æ•°: {stats['compression'].get('total_memories', 0)}
- åŸå§‹å¤§å°: {stats['compression'].get('total_original_bytes', 0)} bytes
- å‹ç¼©åå¤§å°: {stats['compression'].get('total_compressed_bytes', 0)} bytes
- å‹ç¼©ç‡: {stats['compression'].get('overall_compression_ratio', 0):.2%}
- å¹³å‡é‡è¦æ€§: {stats['compression'].get('avg_importance_score', 0):.2f}

## ä¼˜åŒ–æ­¥éª¤
"""
        for step in latest['steps']:
            report += f"\n### {step['name']}\n"
            for key, value in step['result'].items():
                report += f"- {key}: {value}\n"
        
        return report


def demo_integration():
    """æ¼”ç¤ºé›†æˆç³»ç»Ÿ"""
    print("=" * 60)
    print("å¢å¼ºè®°å¿†å­˜å‚¨ç³»ç»Ÿ - é›†æˆæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¢å¼ºå­˜å‚¨
    store = EnhancedMemoryStore(
        persist_dir="./chroma_db_enhanced",
        compression_enabled=True
    )
    
    print("\nğŸ“¥ æ·»åŠ ç¤ºä¾‹è®°å¿†...")
    
    # æ·»åŠ ä¸€äº›ç¤ºä¾‹è®°å¿†
    memories = [
        {
            "content": "ä»Šå¤©å®Œæˆäº†è®°å¿†ç³»ç»Ÿçš„è®¾è®¡æ–‡æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ã€‚è‘£äº‹é•¿å…°å±±æ‰¹å‡†äº†é¡¹ç›®è®¡åˆ’ã€‚",
            "source": "memory/design_doc.md",
            "type": "milestone",
            "metadata": {"user_marked_important": True}
        },
        {
            "content": "æ—¥å¸¸å¼€å‘è¿›åº¦æ›´æ–°ï¼šå®Œæˆäº†ChromaDBçš„é›†æˆæµ‹è¯•ï¼Œä¿®å¤äº†3ä¸ªbugã€‚",
            "source": "dev/daily_update.md",
            "type": "daily",
            "metadata": {}
        },
        {
            "content": "æŠ€æœ¯è°ƒç ”ï¼šå¯¹æ¯”äº†Pineconeã€Weaviateå’ŒChromaDBä¸‰ä¸ªå‘é‡æ•°æ®åº“ï¼Œæœ€ç»ˆé€‰æ‹©ChromaDBå› ä¸ºéƒ¨ç½²ç®€å•ã€‚",
            "source": "research/vector_db_comparison.md",
            "type": "research",
            "metadata": {}
        },
        {
            "content": "è®°å¿†å‹ç¼©ç®—æ³•è®¾è®¡ï¼šé‡‡ç”¨åˆ†å±‚å‹ç¼©ç­–ç•¥ï¼Œæ ¹æ®é‡è¦æ€§è¯„åˆ†å†³å®šå‹ç¼©çº§åˆ«ã€‚",
            "source": "design/compression_algorithm.md",
            "type": "design",
            "metadata": {"user_marked_important": True}
        },
        {
            "content": "å›¢é˜Ÿå‘¨ä¼šè®°å½•ï¼šè®¨è®ºäº†ä¸‹å‘¨çš„å¼€å‘è®¡åˆ’ï¼Œåˆ†é…äº†ä»»åŠ¡ã€‚",
            "source": "meetings/weekly.md",
            "type": "meeting",
            "metadata": {}
        }
    ]
    
    for mem in memories:
        store.add_memory(
            content=mem["content"],
            source=mem["source"],
            memory_type=mem["type"],
            metadata=mem["metadata"]
        )
    
    print(f"  å·²æ·»åŠ  {len(memories)} æ¡è®°å¿†")
    
    print("\nğŸ“Š å½“å‰ç»Ÿè®¡:")
    stats = store.get_memory_stats()
    print(f"  Chromaæ–‡æ¡£æ•°: {stats['chroma'].get('total_documents', 0)}")
    print(f"  å‹ç¼©ç³»ç»Ÿè®°å¿†æ•°: {stats['compression'].get('total_memories', 0)}")
    print(f"  å¹³å‡é‡è¦æ€§: {stats['compression'].get('avg_importance_score', 0):.2f}")
    
    print("\nğŸ” æµ‹è¯•æ£€ç´¢:")
    queries = ["è®°å¿†å‹ç¼©", "ChromaDB", "é‡è¦é‡Œç¨‹ç¢‘"]
    for query in queries:
        print(f"\n  æŸ¥è¯¢: '{query}'")
        results = store.search(query, mode="enhanced", n_results=2)
        for r in results:
            print(f"    â†’ ç›¸å…³åº¦: {r.get('enhanced_score', 0):.3f} | "
                  f"æ¥æº: {r.get('metadata', {}).get('source', 'unknown')}")
    
    print("\nâš¡ æ‰§è¡Œä¼˜åŒ–...")
    optimizer = MemoryOptimizer(store)
    opt_result = optimizer.run_optimization()
    
    print("\nğŸ“ˆ ä¼˜åŒ–åç»Ÿè®¡:")
    final_stats = store.get_memory_stats()
    print(f"  å‹ç¼©ç‡: {final_stats['compression'].get('overall_compression_ratio', 0):.2%}")
    print(f"  å‹ç¼©çº§åˆ«åˆ†å¸ƒ: {final_stats['compression'].get('compression_level_distribution', {})}")
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    
    return store, optimizer


if __name__ == "__main__":
    demo_integration()
