#!/usr/bin/env python3
"""
Pineconeå‘é‡æ•°æ®åº“é›†æˆæ¨¡å—
é˜¶æ®µ1: ç´§æ€¥å‡çº§ - å‘é‡æ•°æ®åº“è¿ç§»
"""

import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import numpy as np

# å°è¯•å¯¼å…¥Pinecone
try:
    from pinecone import Pinecone, ServerlessSpec
    PINECONE_AVAILABLE = True
except ImportError:
    PINECONE_AVAILABLE = False
    print("âš ï¸ Pinecone not installed. Run: pip install pinecone-client")

# å°è¯•å¯¼å…¥OpenAI Embeddings
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥sentence-transformersä½œä¸ºfallback
try:
    from sentence_transformers import SentenceTransformer
    ST_AVAILABLE = True
except ImportError:
    ST_AVAILABLE = False


@dataclass
class VectorMemoryRecord:
    """å‘é‡è®°å¿†è®°å½•"""
    id: str
    content: str
    embedding: List[float]
    metadata: Dict[str, Any]
    timestamp: datetime
    user_id: str = "default"
    session_id: str = ""
    importance_score: float = 0.0


class EmbeddingProvider:
    """åµŒå…¥å‘é‡æä¾›å™¨ - æ”¯æŒå¤šç§åç«¯"""
    
    def __init__(self, provider: str = "auto"):
        self.provider = provider
        self.model = None
        self.dimension = 1536  # é»˜è®¤OpenAIç»´åº¦
        
        if provider == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨åç«¯
            if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
                self.provider = "openai"
                self.dimension = 1536
            elif ST_AVAILABLE:
                self.provider = "sentence-transformers"
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                self.dimension = 384
            else:
                self.provider = "hash"
                self.dimension = 384
        
        elif provider == "sentence-transformers" and ST_AVAILABLE:
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            self.dimension = 384
    
    def embed(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        if self.provider == "openai":
            return self._embed_openai(text)
        elif self.provider == "sentence-transformers" and self.model:
            return self._embed_st(text)
        else:
            return self._embed_hash(text)
    
    def _embed_openai(self, text: str) -> List[float]:
        """ä½¿ç”¨OpenAI APIç”ŸæˆåµŒå…¥"""
        try:
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.embeddings.create(
                model="text-embedding-3-large",
                input=text[:8000]  # é™åˆ¶é•¿åº¦
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"OpenAI embedding error: {e}")
            return self._embed_hash(text)
    
    def _embed_st(self, text: str) -> List[float]:
        """ä½¿ç”¨Sentence-Transformersç”ŸæˆåµŒå…¥"""
        embedding = self.model.encode(text[:8000])
        return embedding.tolist()
    
    def _embed_hash(self, text: str) -> List[float]:
        """ä½¿ç”¨å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§åµŒå…¥ (fallback)"""
        # åŸºäºå…³é”®è¯å“ˆå¸Œçš„ç®€å•åµŒå…¥
        import re
        words = re.findall(r'\b\w+\b', text.lower())
        
        vector = np.zeros(self.dimension)
        for word in words[:100]:  # é™åˆ¶è¯æ•°
            hash_val = int(hashlib.md5(word.encode()).hexdigest(), 16)
            for i in range(self.dimension):
                if (hash_val >> (i % 32)) & 1:
                    vector[i] += 1
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector.tolist()
    
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥"""
        return [self.embed(text) for text in texts]


class PineconeMemoryStore:
    """
    Pineconeå‘é‡æ•°æ®åº“å­˜å‚¨
    å®ç°è¯­ä¹‰æ£€ç´¢ã€å…ƒæ•°æ®è¿‡æ»¤ã€ç”¨æˆ·éš”ç¦»
    """
    
    def __init__(self, 
                 api_key: Optional[str] = None,
                 index_name: str = "kimi-claw-memory",
                 dimension: int = 1536,
                 metric: str = "cosine",
                 cloud: str = "aws",
                 region: str = "us-east-1"):
        """
        åˆå§‹åŒ–Pineconeå­˜å‚¨
        
        Args:
            api_key: Pinecone APIå¯†é’¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡PINECONE_API_KEYè·å–)
            index_name: ç´¢å¼•åç§°
            dimension: å‘é‡ç»´åº¦ (OpenAI: 1536, MiniLM: 384)
            metric: ç›¸ä¼¼åº¦åº¦é‡ (cosine/euclidean/dotproduct)
            cloud: äº‘æœåŠ¡æä¾›å•† (aws/gcp/azure)
            region: åŒºåŸŸ
        """
        if not PINECONE_AVAILABLE:
            raise ImportError("Pinecone not installed. Run: pip install pinecone-client")
        
        self.api_key = api_key or os.getenv("PINECONE_API_KEY")
        if not self.api_key:
            raise ValueError("Pinecone API key required. Set PINECONE_API_KEY env var.")
        
        self.index_name = index_name
        self.dimension = dimension
        self.metric = metric
        self.cloud = cloud
        self.region = region
        
        # åˆå§‹åŒ–Pineconeå®¢æˆ·ç«¯
        self.pc = Pinecone(api_key=self.api_key)
        
        # åˆå§‹åŒ–åµŒå…¥æä¾›å™¨
        self.embedder = EmbeddingProvider()
        self.dimension = self.embedder.dimension
        
        # è·å–æˆ–åˆ›å»ºç´¢å¼•
        self.index = self._get_or_create_index()
    
    def _get_or_create_index(self):
        """è·å–æˆ–åˆ›å»ºç´¢å¼•"""
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        existing_indexes = self.pc.list_indexes()
        index_names = [idx.name for idx in existing_indexes]
        
        if self.index_name not in index_names:
            print(f"Creating new Pinecone index: {self.index_name}")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric=self.metric,
                spec=ServerlessSpec(cloud=self.cloud, region=self.region)
            )
            print(f"âœ… Index '{self.index_name}' created successfully")
        else:
            print(f"Using existing index: {self.index_name}")
        
        return self.pc.Index(self.index_name)
    
    def add_memory(self, 
                   content: str,
                   metadata: Dict[str, Any] = None,
                   user_id: str = "default",
                   session_id: str = "",
                   memory_type: str = "general") -> str:
        """
        æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            content: è®°å¿†å†…å®¹
            metadata: é¢å¤–å…ƒæ•°æ®
            user_id: ç”¨æˆ·ID (ç”¨äºæ•°æ®éš”ç¦»)
            session_id: ä¼šè¯ID
            memory_type: è®°å¿†ç±»å‹ (episodic/semantic/mid_term/long_term)
        
        Returns:
            è®°å¿†ID
        """
        # ç”Ÿæˆå”¯ä¸€ID
        memory_id = f"mem_{hashlib.md5(f'{content}:{user_id}:{datetime.now()}'.encode()).hexdigest()[:16]}"
        
        # ç”ŸæˆåµŒå…¥
        embedding = self.embedder.embed(content)
        
        # æ„å»ºå…ƒæ•°æ®
        meta = {
            "user_id": user_id,
            "session_id": session_id,
            "memory_type": memory_type,
            "content": content[:1000],  # é™åˆ¶å­˜å‚¨é•¿åº¦
            "timestamp": datetime.now().isoformat(),
            **(metadata or {})
        }
        
        # ä¸Šä¼ åˆ°Pinecone
        self.index.upsert(
            vectors=[{
                "id": memory_id,
                "values": embedding,
                "metadata": meta
            }]
        )
        
        return memory_id
    
    def add_memories_batch(self, 
                          memories: List[Dict[str, Any]],
                          batch_size: int = 100) -> List[str]:
        """
        æ‰¹é‡æ·»åŠ è®°å¿†
        
        Args:
            memories: è®°å¿†åˆ—è¡¨,æ¯é¡¹åŒ…å«content, metadataç­‰
            batch_size: æ‰¹é‡å¤§å°
        
        Returns:
            è®°å¿†IDåˆ—è¡¨
        """
        ids = []
        
        for i in range(0, len(memories), batch_size):
            batch = memories[i:i+batch_size]
            
            vectors = []
            for mem in batch:
                memory_id = f"mem_{hashlib.md5(f'{mem['content']}:{mem.get('user_id', 'default')}:{datetime.now()}'.encode()).hexdigest()[:16]}"
                ids.append(memory_id)
                
                embedding = self.embedder.embed(mem['content'])
                
                meta = {
                    "user_id": mem.get("user_id", "default"),
                    "session_id": mem.get("session_id", ""),
                    "memory_type": mem.get("memory_type", "general"),
                    "content": mem['content'][:1000],
                    "timestamp": datetime.now().isoformat(),
                    **mem.get("metadata", {})
                }
                
                vectors.append({
                    "id": memory_id,
                    "values": embedding,
                    "metadata": meta
                })
            
            self.index.upsert(vectors=vectors)
        
        return ids
    
    def search(self, 
               query: str,
               user_id: Optional[str] = None,
               memory_type: Optional[str] = None,
               top_k: int = 5,
               filter_dict: Optional[Dict] = None) -> List[Dict]:
        """
        è¯­ä¹‰æœç´¢è®°å¿†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            user_id: ç”¨æˆ·IDè¿‡æ»¤
            memory_type: è®°å¿†ç±»å‹è¿‡æ»¤
            top_k: è¿”å›ç»“æœæ•°
            filter_dict: é¢å¤–è¿‡æ»¤æ¡ä»¶
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedder.embed(query)
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_conditions = filter_dict or {}
        if user_id:
            filter_conditions["user_id"] = {"$eq": user_id}
        if memory_type:
            filter_conditions["memory_type"] = {"$eq": memory_type}
        
        # æ‰§è¡ŒæŸ¥è¯¢
        if filter_conditions:
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                filter=filter_conditions,
                include_metadata=True
            )
        else:
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True
            )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for match in results.matches:
            formatted_results.append({
                "id": match.id,
                "score": match.score,
                "content": match.metadata.get("content", ""),
                "metadata": {k: v for k, v in match.metadata.items() if k != "content"}
            })
        
        return formatted_results
    
    def get_memory(self, memory_id: str) -> Optional[Dict]:
        """è·å–å•ä¸ªè®°å¿†"""
        try:
            result = self.index.fetch(ids=[memory_id])
            if result.vectors:
                vector_data = result.vectors[memory_id]
                return {
                    "id": vector_data.id,
                    "metadata": vector_data.metadata,
                    "values": vector_data.values
                }
            return None
        except Exception as e:
            print(f"Error fetching memory: {e}")
            return None
    
    def delete_memory(self, memory_id: str) -> bool:
        """åˆ é™¤è®°å¿†"""
        try:
            self.index.delete(ids=[memory_id])
            return True
        except Exception as e:
            print(f"Error deleting memory: {e}")
            return False
    
    def delete_user_memories(self, user_id: str) -> bool:
        """åˆ é™¤ç”¨æˆ·çš„æ‰€æœ‰è®°å¿†"""
        try:
            self.index.delete(filter={"user_id": {"$eq": user_id}})
            return True
        except Exception as e:
            print(f"Error deleting user memories: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.index.describe_index_stats()
        return {
            "total_vectors": stats.total_vector_count,
            "dimension": stats.dimension,
            "index_fullness": getattr(stats, 'index_fullness', 0),
            "namespaces": list(stats.namespaces.keys()) if stats.namespaces else []
        }
    
    def migrate_from_files(self, 
                          memory_dir: str = "./memory",
                          user_id: str = "default") -> Dict:
        """
        ä»Markdownæ–‡ä»¶è¿ç§»è®°å¿†åˆ°Pinecone
        
        Args:
            memory_dir: è®°å¿†æ–‡ä»¶ç›®å½•
            user_id: ç”¨æˆ·ID
        
        Returns:
            è¿ç§»ç»Ÿè®¡
        """
        import glob
        import re
        
        memories = []
        
        # éå†æ‰€æœ‰Markdownæ–‡ä»¶
        for filepath in glob.glob(os.path.join(memory_dir, "**/*.md"), recursive=True):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–æ–‡ä»¶åä½œä¸ºç±»å‹æç¤º
                filename = os.path.basename(filepath)
                
                # ç¡®å®šè®°å¿†ç±»å‹
                memory_type = "general"
                if "episodic" in filepath:
                    memory_type = "episodic"
                elif "semantic" in filepath:
                    memory_type = "semantic"
                elif "mid-term" in filepath:
                    memory_type = "mid_term"
                elif "long-term" in filepath:
                    memory_type = "long_term"
                
                # åˆ†å‰²é•¿æ–‡æ¡£
                sections = re.split(r'\n#+ ', content)
                for i, section in enumerate(sections):
                    if len(section.strip()) < 50:
                        continue
                    
                    memories.append({
                        "content": section[:2000],  # é™åˆ¶é•¿åº¦
                        "metadata": {
                            "source_file": filepath,
                            "section_index": i
                        },
                        "user_id": user_id,
                        "memory_type": memory_type
                    })
                    
            except Exception as e:
                print(f"Error reading {filepath}: {e}")
        
        # æ‰¹é‡ä¸Šä¼ 
        if memories:
            ids = self.add_memories_batch(memories)
            return {
                "migrated_count": len(ids),
                "file_count": len(glob.glob(os.path.join(memory_dir, "**/*.md"), recursive=True)),
                "memory_ids": ids[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            }
        
        return {"migrated_count": 0, "file_count": 0}


class HybridRetriever:
    """
    æ··åˆæ£€ç´¢å™¨
    ç»“åˆPineconeå‘é‡æ£€ç´¢ + æœ¬åœ°å…³é”®è¯æ£€ç´¢
    """
    
    def __init__(self, pinecone_store: PineconeMemoryStore):
        self.vector_store = pinecone_store
        self.keyword_weight = 0.3
        self.vector_weight = 0.7
    
    def search(self, 
               query: str,
               user_id: Optional[str] = None,
               top_k: int = 5) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢
        
        æµç¨‹:
        1. å‘é‡æ£€ç´¢è·å–å€™é€‰é›†
        2. è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        3. èåˆé‡æ’åº
        """
        # 1. å‘é‡æ£€ç´¢ (æ‰©å¤§å€™é€‰é›†)
        vector_results = self.vector_store.search(
            query=query,
            user_id=user_id,
            top_k=top_k * 2
        )
        
        # 2. è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        query_keywords = set(self._extract_keywords(query))
        
        scored_results = []
        for result in vector_results:
            content = result.get("content", "")
            content_keywords = set(self._extract_keywords(content))
            
            # å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = len(query_keywords & content_keywords) / max(len(query_keywords), 1)
            
            # å‘é‡ç›¸ä¼¼åº¦åˆ†æ•° (å·²å½’ä¸€åŒ–)
            vector_score = result.get("score", 0)
            
            # èåˆåˆ†æ•°
            fused_score = (
                vector_score * self.vector_weight +
                keyword_score * self.keyword_weight
            )
            
            scored_results.append({
                **result,
                "keyword_score": keyword_score,
                "vector_score": vector_score,
                "fused_score": fused_score
            })
        
        # 3. æŒ‰èåˆåˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x["fused_score"], reverse=True)
        
        return scored_results[:top_k]
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        import re
        words = re.findall(r'\b\w+\b', text.lower())
        
        # åœç”¨è¯è¿‡æ»¤
        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 
                     'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–'}
        
        return [w for w in words if w not in stopwords and len(w) > 1]


def demo():
    """æ¼”ç¤ºPineconeè®°å¿†å­˜å‚¨"""
    print("=" * 60)
    print("Pineconeå‘é‡æ•°æ®åº“æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("PINECONE_API_KEY"):
        print("\nâš ï¸ æœªè®¾ç½®PINECONE_API_KEYç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®: export PINECONE_API_KEY='your-api-key'")
        print("\næ¼”ç¤ºæ¨¡å¼: ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ...")
        return demo_mock()
    
    # åˆ›å»ºå­˜å‚¨å®ä¾‹
    store = PineconeMemoryStore(
        index_name="kimi-claw-memory-demo",
        dimension=384  # ä½¿ç”¨hash-basedåµŒå…¥
    )
    
    print("\nğŸ“¥ æ·»åŠ ç¤ºä¾‹è®°å¿†...")
    
    sample_memories = [
        {
            "content": "è‘£äº‹é•¿å…°å±±æ‰¹å‡†äº†è®°å¿†ç³»ç»Ÿå‡çº§é¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æˆ˜ç•¥å†³ç­–ã€‚",
            "memory_type": "episodic",
            "metadata": {"importance": "high", "decision": True}
        },
        {
            "content": "Pineconeæ˜¯ä¸€ä¸ªæ‰˜ç®¡çš„å‘é‡æ•°æ®åº“æœåŠ¡ï¼Œæä¾›ä½å»¶è¿Ÿçš„è¯­ä¹‰æ£€ç´¢ã€‚",
            "memory_type": "semantic",
            "metadata": {"topic": "vector_database"}
        },
        {
            "content": "ä»Šå¤©å®Œæˆäº†ChromaDBçš„éƒ¨ç½²ï¼Œæ€§èƒ½æµ‹è¯•æ˜¾ç¤ºæ£€ç´¢å»¶è¿Ÿçº¦1.6msã€‚",
            "memory_type": "episodic",
            "metadata": {"project": "memory_system", "milestone": True}
        },
        {
            "content": "å‘é‡æ•°æ®åº“çš„é€‰æ‹©æ ‡å‡†ï¼šå»¶è¿Ÿã€å¯æ‰©å±•æ€§ã€æˆæœ¬ã€æ˜“ç”¨æ€§ã€‚",
            "memory_type": "semantic",
            "metadata": {"topic": "evaluation_criteria"}
        },
        {
            "content": "å›¢é˜Ÿå‘¨ä¼šè®¨è®ºäº†ä¸‹å‘¨çš„å¼€å‘è®¡åˆ’ï¼Œé‡ç‚¹æ˜¯Pineconeé›†æˆã€‚",
            "memory_type": "episodic",
            "metadata": {"meeting": True}
        }
    ]
    
    ids = []
    for mem in sample_memories:
        mid = store.add_memory(
            content=mem["content"],
            memory_type=mem["memory_type"],
            metadata=mem["metadata"]
        )
        ids.append(mid)
        print(f"  âœ… {mid[:20]}... | {mem['memory_type']}")
    
    print(f"\nğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
    stats = store.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ” è¯­ä¹‰æ£€ç´¢æµ‹è¯•:")
    
    test_queries = [
        "è®°å¿†ç³»ç»Ÿå‡çº§",
        "å‘é‡æ•°æ®åº“é€‰æ‹©",
        "å›¢é˜Ÿä¼šè®®",
        "Pineconeæ€§èƒ½"
    ]
    
    for query in test_queries:
        print(f"\n  æŸ¥è¯¢: '{query}'")
        results = store.search(query, top_k=3)
        for r in results:
            print(f"    â†’ åˆ†æ•°: {r['score']:.3f} | ç±»å‹: {r['metadata'].get('memory_type', 'unknown')}")
            print(f"      å†…å®¹: {r['content'][:60]}...")
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    
    return store


def demo_mock():
    """æ¨¡æ‹Ÿæ¼”ç¤º (æ— Pineconeæ—¶)"""
    print("\n[æ¨¡æ‹Ÿæ¨¡å¼] å±•ç¤ºé¢„æœŸåŠŸèƒ½...")
    
    # æ¨¡æ‹ŸåµŒå…¥æä¾›å™¨
    embedder = EmbeddingProvider()
    
    sample_texts = [
        "è‘£äº‹é•¿å…°å±±æ‰¹å‡†äº†è®°å¿†ç³»ç»Ÿå‡çº§é¡¹ç›®",
        "Pineconeæ˜¯ä¸€ä¸ªæ‰˜ç®¡çš„å‘é‡æ•°æ®åº“",
        "ChromaDBéƒ¨ç½²å®Œæˆ"
    ]
    
    print("\nğŸ“ åµŒå…¥å‘é‡ç¤ºä¾‹:")
    for text in sample_texts:
        embedding = embedder.embed(text)
        print(f"  '{text[:30]}...' -> ç»´åº¦: {len(embedding)}, å‰5å€¼: {embedding[:5]}")
    
    print("\nâœ… æ¨¡æ‹Ÿæ¼”ç¤ºå®Œæˆ")
    print("\nè¦è¿è¡Œå®Œæ•´æ¼”ç¤ºï¼Œè¯·:")
    print("1. æ³¨å†ŒPinecone: https://pinecone.io")
    print("2. è·å–APIå¯†é’¥")
    print("3. è®¾ç½®ç¯å¢ƒå˜é‡: export PINECONE_API_KEY='your-key'")
    print("4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")


if __name__ == "__main__":
    demo()
