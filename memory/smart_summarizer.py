#!/usr/bin/env python3
"""
æ™ºèƒ½æ‘˜è¦ç³»ç»Ÿ v3.0
å®ç°åˆ†å±‚å‹ç¼©ã€é‡è¦æ€§è¯„åˆ†ã€è‡ªé€‚åº”æ‘˜è¦
"""

import re
import heapq
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import numpy as np


@dataclass
class SummaryLevel:
    """æ‘˜è¦çº§åˆ«å®šä¹‰"""
    level: int
    name: str
    description: str
    target_ratio: float  # ç›®æ ‡å‹ç¼©ç‡
    min_importance: float  # æœ€ä½é‡è¦æ€§é˜ˆå€¼


# é¢„å®šä¹‰æ‘˜è¦çº§åˆ«
SUMMARY_LEVELS = {
    0: SummaryLevel(0, "å®Œæ•´", "ä¿ç•™å®Œæ•´åŸæ–‡", 1.0, 4.5),
    1: SummaryLevel(1, "è¯¦ç»†", "ä¿ç•™80%ä¿¡æ¯", 0.8, 3.5),
    2: SummaryLevel(2, "è¦ç‚¹", "ä¿ç•™50%ä¿¡æ¯", 0.5, 2.5),
    3: SummaryLevel(3, "ç²¾ç®€", "ä¿ç•™20%ä¿¡æ¯", 0.2, 1.5),
    4: SummaryLevel(4, "ç´¢å¼•", "ä»…ä¿ç•™å…ƒæ•°æ®", 0.0, 0.0),
}


class TextProcessor:
    """æ–‡æœ¬å¤„ç†å·¥å…·"""
    
    # åœç”¨è¯
    STOPWORDS = {
        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
        'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
        'ought', 'used', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by',
        'from', 'as', 'into', 'through', 'during', 'before', 'after', 'above',
        'below', 'between', 'under', 'again', 'further', 'then', 'once', 'here',
        'there', 'when', 'where', 'why', 'how', 'all', 'each', 'few', 'more',
        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
        'same', 'so', 'than', 'too', 'very', 'just', 'çš„', 'äº†', 'åœ¨', 'æ˜¯',
        'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
        'è¿™', 'é‚£', 'è¿™äº›', 'é‚£äº›', 'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ'
    }
    
    # å†³ç­–å…³é”®è¯
    DECISION_KEYWORDS = [
        'å†³ç­–', 'å†³å®š', 'é€‰æ‹©', 'æ‰¹å‡†', 'æ‹’ç»', 'å…³é”®', 'é‡è¦', 'æˆ˜ç•¥',
        'decision', 'decide', 'choose', 'approve', 'reject', 'critical',
        'important', 'strategic', 'approve', 'authorize', 'finalize'
    ]
    
    # è¡ŒåŠ¨å…³é”®è¯
    ACTION_KEYWORDS = [
        'å®Œæˆ', 'å®æ–½', 'å¯åŠ¨', 'éƒ¨ç½²', 'å‘å¸ƒ', 'ä¸Šçº¿', 'äº¤ä»˜',
        'complete', 'implement', 'launch', 'deploy', 'release', 'deliver'
    ]
    
    @classmethod
    def split_sentences(cls, text: str) -> List[str]:
        """åˆ†å‰²å¥å­ (æ”¯æŒä¸­è‹±æ–‡)"""
        # ä¸­æ–‡å¥å­ç»“æŸç¬¦
        chinese_ends = r'[ã€‚ï¼ï¼Ÿï¼›]'
        # è‹±æ–‡å¥å­ç»“æŸç¬¦
        english_ends = r'[.!?;]'
        
        # ä¿æŠ¤å¸¸è§ç¼©å†™
        text = re.sub(r'(Mr|Mrs|Dr|Prof|Inc|Ltd|Jr|Sr|vs|Vol|vol)\.', r'\1&lt;DOT&gt;', text)
        
        # åˆ†å‰²å¥å­
        pattern = f'({chinese_ends}|{english_ends})'
        sentences = re.split(pattern, text)
        
        # åˆå¹¶åˆ†å‰²ç¬¦å’Œå¥å­
        result = []
        i = 0
        while i < len(sentences):
            if i + 1 < len(sentences) and re.match(pattern, sentences[i + 1]):
                result.append(sentences[i] + sentences[i + 1])
                i += 2
            else:
                if sentences[i].strip():
                    result.append(sentences[i])
                i += 1
        
        # æ¢å¤ç¼©å†™
        result = [s.replace('&lt;DOT&gt;', '.') for s in result]
        
        return [s.strip() for s in result if s.strip()]
    
    @classmethod
    def extract_keywords(cls, text: str, top_k: int = 20) -> List[str]:
        """æå–å…³é”®è¯ (TF-IDFè¿‘ä¼¼)"""
        words = re.findall(r'\b\w+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered = [w for w in words 
                   if w not in cls.STOPWORDS and len(w) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = defaultdict(int)
        for word in filtered:
            word_freq[word] += 1
        
        # è¿”å›Top-K
        top_words = heapq.nlargest(top_k, word_freq.items(), key=lambda x: x[1])
        return [word for word, _ in top_words]
    
    @classmethod
    def extract_entities(cls, text: str) -> List[Dict]:
        """æå–å®ä½“ (ç®€åŒ–ç‰ˆ)"""
        entities = []
        
        # æå–å¼•å·å†…çš„å†…å®¹
        quoted = re.findall(r'["""]([^"""]+)["""]', text)
        for q in quoted:
            entities.append({"name": q, "type": "QUOTED"})
        
        # æå–å¤§å†™å•è¯ç»„åˆ (å¯èƒ½æ˜¯ä¸“æœ‰åè¯)
        capitalized = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        for c in capitalized:
            if c.lower() not in cls.STOPWORDS:
                entities.append({"name": c, "type": "PROPER_NOUN"})
        
        # æå–æ•°å­—å’Œåº¦é‡
        numbers = re.findall(r'\b\d+(?:\.\d+)?\s*(?:%|percent|ä¸ª|æ¡|æ¬¡|å¤©|å°æ—¶|åˆ†é’Ÿ)\b', text)
        for n in numbers:
            entities.append({"name": n, "type": "METRIC"})
        
        # å»é‡
        seen = set()
        unique_entities = []
        for e in entities:
            key = e["name"].lower()
            if key not in seen:
                seen.add(key)
                unique_entities.append(e)
        
        return unique_entities[:20]
    
    @classmethod
    def calculate_similarity(cls, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (Jaccard)"""
        keywords1 = set(cls.extract_keywords(text1, top_k=50))
        keywords2 = set(cls.extract_keywords(text2, top_k=50))
        
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)
        
        return intersection / union if union > 0 else 0.0


class ImportanceScorer:
    """
    è®°å¿†é‡è¦æ€§è¯„åˆ†å™¨
    
    è¯„åˆ†ç»´åº¦:
    1. ç”¨æˆ·æ˜¾å¼æ ‡è®° (æƒé‡1.0)
    2. å†³ç­–å…³é”®åº¦ (æƒé‡1.2)
    3. è®¿é—®é¢‘ç‡ (æƒé‡0.8)
    4. ä¿¡æ¯å¯†åº¦ (æƒé‡0.6)
    5. æ—¶æ•ˆæ€§ (æƒé‡0.7)
    6. è¡ŒåŠ¨å¯¼å‘ (æƒé‡0.5)
    """
    
    def __init__(self):
        self.weights = {
            'user_explicit': 1.0,
            'decision_critical': 1.2,
            'access_frequency': 0.8,
            'information_density': 0.6,
            'recency': 0.7,
            'action_oriented': 0.5,
        }
    
    def calculate(self, 
                  content: str,
                  access_count: int = 0,
                  user_marked: bool = False,
                  age_days: int = 0) -> Tuple[float, Dict]:
        """
        è®¡ç®—é‡è¦æ€§è¯„åˆ†
        
        Returns:
            (æ€»åˆ†, å„ç»´åº¦å¾—åˆ†)
        """
        scores = {}
        
        # 1. ç”¨æˆ·æ˜¾å¼æ ‡è®° (0-1.0)
        scores['user_explicit'] = 1.0 if user_marked else 0
        
        # 2. å†³ç­–å…³é”®åº¦ (0-1.0)
        content_lower = content.lower()
        decision_matches = sum(1 for kw in TextProcessor.DECISION_KEYWORDS 
                              if kw in content_lower)
        scores['decision_critical'] = min(decision_matches * 0.25, 1.0)
        
        # 3. è®¿é—®é¢‘ç‡ (0-1.0)
        scores['access_frequency'] = min(access_count / 20, 1.0)
        
        # 4. ä¿¡æ¯å¯†åº¦ (0-1.0)
        entities = TextProcessor.extract_entities(content)
        keywords = TextProcessor.extract_keywords(content, top_k=50)
        entity_score = min(len(entities) * 0.1, 0.5)
        keyword_score = min(len(keywords) / 20, 0.5)
        scores['information_density'] = entity_score + keyword_score
        
        # 5. æ—¶æ•ˆæ€§ - è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿ (0-1.0)
        scores['recency'] = np.exp(-age_days / 30)  # 30å¤©åŠè¡°æœŸ
        
        # 6. è¡ŒåŠ¨å¯¼å‘ (0-1.0)
        action_matches = sum(1 for kw in TextProcessor.ACTION_KEYWORDS 
                            if kw in content_lower)
        scores['action_oriented'] = min(action_matches * 0.3, 1.0)
        
        # è®¡ç®—åŠ æƒæ€»åˆ† (0-5.0)
        total = sum(scores[k] * self.weights[k] for k in scores)
        
        return round(min(total, 5.0), 2), scores
    
    def get_compression_level(self, importance: float) -> int:
        """æ ¹æ®é‡è¦æ€§ç¡®å®šå‹ç¼©çº§åˆ«"""
        for level in sorted(SUMMARY_LEVELS.keys()):
            sl = SUMMARY_LEVELS[level]
            if importance >= sl.min_importance:
                return level
        return 4  # é»˜è®¤æœ€ä½çº§åˆ«


class SmartSummarizer:
    """
    æ™ºèƒ½æ‘˜è¦å™¨
    å®ç°åˆ†å±‚æ‘˜è¦ã€æå–å¼æ‘˜è¦ã€å…³é”®è¦ç‚¹æå–
    """
    
    def __init__(self):
        self.text_processor = TextProcessor()
        self.importance_scorer = ImportanceScorer()
    
    def summarize(self, 
                  content: str,
                  importance: float = 0,
                  level: Optional[int] = None) -> Dict:
        """
        æ™ºèƒ½æ‘˜è¦ä¸»å‡½æ•°
        
        Args:
            content: åŸæ–‡å†…å®¹
            importance: é‡è¦æ€§è¯„åˆ† (0-5)
            level: æŒ‡å®šæ‘˜è¦çº§åˆ« (0-4), Noneåˆ™è‡ªåŠ¨é€‰æ‹©
        
        Returns:
            æ‘˜è¦ç»“æœå­—å…¸
        """
        # ç¡®å®šå‹ç¼©çº§åˆ«
        if level is None:
            level = self.importance_scorer.get_compression_level(importance)
        
        summary_level = SUMMARY_LEVELS.get(level, SUMMARY_LEVELS[4])
        
        # æ ¹æ®çº§åˆ«ç”Ÿæˆæ‘˜è¦
        if level == 0:
            return self._level_0_full(content, summary_level)
        elif level == 1:
            return self._level_1_detailed(content, summary_level)
        elif level == 2:
            return self._level_2_keypoints(content, summary_level)
        elif level == 3:
            return self._level_3_concise(content, summary_level)
        else:
            return self._level_4_index(content, summary_level)
    
    def _level_0_full(self, content: str, level: SummaryLevel) -> Dict:
        """çº§åˆ«0: å®Œæ•´ä¿ç•™"""
        return {
            "level": 0,
            "level_name": level.name,
            "full_text": content,
            "summary": None,
            "keypoints": [],
            "entities": self.text_processor.extract_entities(content),
            "compression_ratio": 1.0,
            "original_length": len(content),
            "compressed_length": len(content)
        }
    
    def _level_1_detailed(self, content: str, level: SummaryLevel) -> Dict:
        """çº§åˆ«1: è¯¦ç»†æ‘˜è¦ (ä¿ç•™80%)"""
        sentences = self.text_processor.split_sentences(content)
        
        if len(sentences) <= 3:
            return self._level_0_full(content, level)
        
        # é€‰æ‹©æœ€é‡è¦çš„80%å¥å­
        k = max(1, int(len(sentences) * level.target_ratio))
        selected = self._select_top_sentences(sentences, k)
        
        summary = ' '.join(selected)
        
        return {
            "level": 1,
            "level_name": level.name,
            "full_text": content,  # ä»ä¿ç•™å®Œæ•´æ–‡æœ¬
            "summary": summary,
            "keypoints": selected[:5],
            "entities": self.text_processor.extract_entities(content),
            "compression_ratio": len(summary) / len(content) if content else 1.0,
            "original_length": len(content),
            "compressed_length": len(summary)
        }
    
    def _level_2_keypoints(self, content: str, level: SummaryLevel) -> Dict:
        """çº§åˆ«2: å…³é”®è¦ç‚¹ (ä¿ç•™50%)"""
        sentences = self.text_processor.split_sentences(content)
        
        if len(sentences) <= 5:
            keypoints = sentences
        else:
            # é€‰æ‹©æœ€é‡è¦çš„50%å¥å­ä½œä¸ºè¦ç‚¹
            k = max(1, int(len(sentences) * level.target_ratio))
            keypoints = self._select_top_sentences(sentences, k)
        
        # ç”Ÿæˆç®€çŸ­æ‘˜è¦
        summary = ' '.join(keypoints[:3]) if keypoints else ""
        
        return {
            "level": 2,
            "level_name": level.name,
            "full_text": None,  # ä¸ä¿ç•™å®Œæ•´æ–‡æœ¬
            "summary": summary,
            "keypoints": keypoints,
            "entities": self.text_processor.extract_entities(content),
            "compression_ratio": sum(len(k) for k in keypoints) / len(content) if content else 0,
            "original_length": len(content),
            "compressed_length": sum(len(k) for k in keypoints)
        }
    
    def _level_3_concise(self, content: str, level: SummaryLevel) -> Dict:
        """çº§åˆ«3: ç²¾ç®€ (ä¿ç•™20%)"""
        # æå–å…³é”®å®ä½“
        entities = self.text_processor.extract_entities(content)
        
        # æå–æœ€é‡è¦çš„å¥å­
        sentences = self.text_processor.split_sentences(content)
        
        if len(sentences) <= 2:
            keypoints = sentences
        else:
            k = max(1, int(len(sentences) * level.target_ratio))
            keypoints = self._select_top_sentences(sentences, k)
        
        # æ„å»ºç²¾ç®€æè¿°
        entity_names = [e["name"] for e in entities[:5]]
        concise = f"æ¶‰åŠ: {', '.join(entity_names)}" if entity_names else ""
        
        return {
            "level": 3,
            "level_name": level.name,
            "full_text": None,
            "summary": concise,
            "keypoints": keypoints,
            "entities": entities,
            "compression_ratio": 0.2,
            "original_length": len(content),
            "compressed_length": len(concise) + sum(len(k) for k in keypoints)
        }
    
    def _level_4_index(self, content: str, level: SummaryLevel) -> Dict:
        """çº§åˆ«4: ä»…ç´¢å¼•"""
        entities = self.text_processor.extract_entities(content)
        keywords = self.text_processor.extract_keywords(content, top_k=10)
        
        return {
            "level": 4,
            "level_name": level.name,
            "full_text": None,
            "summary": None,
            "keypoints": [],
            "entities": entities,
            "keywords": keywords,
            "compression_ratio": 0.0,
            "original_length": len(content),
            "compressed_length": 0
        }
    
    def _select_top_sentences(self, sentences: List[str], k: int) -> List[str]:
        """é€‰æ‹©æœ€é‡è¦çš„kä¸ªå¥å­"""
        if len(sentences) <= k:
            return sentences
        
        # è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        all_text = ' '.join(sentences)
        global_keywords = set(self.text_processor.extract_keywords(all_text, top_k=30))
        
        sentence_scores = []
        for i, sent in enumerate(sentences):
            score = 0
            
            # å…³é”®è¯åŒ¹é…
            sent_keywords = set(self.text_processor.extract_keywords(sent, top_k=10))
            score += len(sent_keywords & global_keywords) * 2
            
            # ä½ç½®æƒé‡
            if i == 0:  # é¦–å¥
                score += 5
            elif i == len(sentences) - 1:  # å°¾å¥
                score += 3
            elif i < len(sentences) * 0.2:  # å‰20%
                score += 2
            
            # å†³ç­–å…³é”®è¯åŠ åˆ†
            sent_lower = sent.lower()
            for kw in TextProcessor.DECISION_KEYWORDS:
                if kw in sent_lower:
                    score += 3
            
            # è¡ŒåŠ¨å…³é”®è¯åŠ åˆ†
            for kw in TextProcessor.ACTION_KEYWORDS:
                if kw in sent_lower:
                    score += 2
            
            # é•¿åº¦æƒ©ç½š
            sent_len = len(sent)
            if sent_len < 10:
                score -= 2
            elif sent_len > 200:
                score -= 1
            
            sentence_scores.append((i, score))
        
        # é€‰æ‹©Top-K
        top_sentences = heapq.nlargest(k, sentence_scores, key=lambda x: x[1])
        top_sentences.sort(key=lambda x: x[0])  # æŒ‰åŸæ–‡é¡ºåºæ’åˆ—
        
        return [sentences[i] for i, _ in top_sentences]
    
    def batch_summarize(self, 
                       contents: List[str],
                       importances: Optional[List[float]] = None) -> List[Dict]:
        """æ‰¹é‡æ‘˜è¦"""
        results = []
        for i, content in enumerate(contents):
            importance = importances[i] if importances and i < len(importances) else 0
            results.append(self.summarize(content, importance))
        return results


class MemoryCompressionEngine:
    """
    è®°å¿†å‹ç¼©å¼•æ“
    æ•´åˆé‡è¦æ€§è¯„åˆ†å’Œæ™ºèƒ½æ‘˜è¦
    """
    
    def __init__(self, target_compression: float = 0.6):
        self.summarizer = SmartSummarizer()
        self.scorer = ImportanceScorer()
        self.target_compression = target_compression
    
    def compress(self, 
                 content: str,
                 access_count: int = 0,
                 user_marked: bool = False,
                 age_days: int = 0) -> Dict:
        """
        å‹ç¼©è®°å¿†
        
        Returns:
            å‹ç¼©ç»“æœï¼ŒåŒ…å«æ‰€æœ‰å±‚çº§çš„å†…å®¹
        """
        # 1. è®¡ç®—é‡è¦æ€§
        importance, factors = self.scorer.calculate(
            content=content,
            access_count=access_count,
            user_marked=user_marked,
            age_days=age_days
        )
        
        # 2. æ ¹æ®é‡è¦æ€§ç¡®å®šå‹ç¼©çº§åˆ«
        level = self.scorer.get_compression_level(importance)
        
        # 3. ç”Ÿæˆæ‘˜è¦
        summary_result = self.summarizer.summarize(content, importance, level)
        
        return {
            "importance_score": importance,
            "importance_factors": factors,
            "compression_level": level,
            **summary_result
        }
    
    def decompress(self, compressed: Dict) -> str:
        """
        è§£å‹è®°å¿† (è·å–æœ€ä½³å¯ç”¨å†…å®¹)
        """
        if compressed.get("full_text"):
            return compressed["full_text"]
        elif compressed.get("summary"):
            return compressed["summary"]
        elif compressed.get("keypoints"):
            return '\n'.join(compressed["keypoints"])
        elif compressed.get("entities"):
            entities = [e["name"] for e in compressed["entities"]]
            return f"ç›¸å…³å®ä½“: {', '.join(entities)}"
        else:
            return "[å·²å½’æ¡£è®°å¿†]"
    
    def get_stats(self, compressed_memories: List[Dict]) -> Dict:
        """è·å–å‹ç¼©ç»Ÿè®¡"""
        if not compressed_memories:
            return {}
        
        total_original = sum(m.get("original_length", 0) for m in compressed_memories)
        total_compressed = sum(m.get("compressed_length", 0) for m in compressed_memories)
        
        level_distribution = defaultdict(int)
        for m in compressed_memories:
            level_distribution[m.get("compression_level", 4)] += 1
        
        return {
            "total_memories": len(compressed_memories),
            "total_original_bytes": total_original,
            "total_compressed_bytes": total_compressed,
            "compression_ratio": total_compressed / total_original if total_original > 0 else 0,
            "level_distribution": dict(level_distribution),
            "avg_importance": sum(m.get("importance_score", 0) for m in compressed_memories) / len(compressed_memories)
        }


def demo():
    """æ¼”ç¤ºæ™ºèƒ½æ‘˜è¦ç³»ç»Ÿ"""
    print("=" * 70)
    print("æ™ºèƒ½æ‘˜è¦ç³»ç»Ÿ v3.0 - æ¼”ç¤º")
    print("=" * 70)
    
    # æµ‹è¯•æ–‡æœ¬
    test_memories = [
        {
            "content": """
            2026-02-27 é‡è¦å†³ç­–è®°å½•
            
            ä»Šå¤©è‘£äº‹é•¿å…°å±±åœ¨æˆ˜ç•¥ä¼šè®®ä¸Šåšå‡ºäº†å…³é”®å†³ç­–ï¼šæ­£å¼å¯åŠ¨KCGSè®°å¿†ç³»ç»Ÿå‡çº§é¡¹ç›®ã€‚
            è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æˆ˜ç•¥æ„ä¹‰çš„å†³å®šï¼Œå°†å†³å®šå…¬å¸æœªæ¥3å¹´çš„æŠ€æœ¯ç«äº‰åŠ›ã€‚
            
            é¡¹ç›®ç›®æ ‡ï¼š
            1. å®ç°è®°å¿†å‹ç¼©ï¼Œå‡å°‘60%å­˜å‚¨ç©ºé—´
            2. ä¼˜åŒ–æ£€ç´¢æ•ˆç‡ï¼Œæå‡4å€é€Ÿåº¦
            3. å»ºç«‹é‡è¦æ€§è¯„åˆ†æœºåˆ¶
            4. é›†æˆçŸ¥è¯†å›¾è°±èƒ½åŠ›
            
            é¢„ç®—ï¼š500ä¸‡å…ƒ
            æ—¶é—´çº¿ï¼š3ä¸ªæœˆå®Œæˆæ ¸å¿ƒåŠŸèƒ½
            å›¢é˜Ÿï¼š10äººç²¾è‹±å›¢é˜Ÿ
            
            å…°å±±å¼ºè°ƒï¼š"è¿™æ˜¯å…¬å¸æœ€é‡è¦çš„æŠ€æœ¯æŠ•èµ„ä¹‹ä¸€ï¼Œå¿…é¡»å…¨åŠ›ä»¥èµ´ã€‚"
            """,
            "user_marked": True,
            "access_count": 15,
            "age_days": 0
        },
        {
            "content": """
            æ—¥å¸¸å¼€å‘æ—¥å¿— - 2026-02-27
            
            ä»Šå¤©å®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š
            - ä¿®å¤äº†3ä¸ªbug
            - å®Œæˆäº†ChromaDBçš„æ€§èƒ½æµ‹è¯•
            - ç¼–å†™äº†æŠ€æœ¯æ–‡æ¡£
            
            æ˜å¤©è®¡åˆ’ï¼š
            - å¼€å§‹Pineconeé›†æˆ
            - ç»§ç»­ä¼˜åŒ–æ£€ç´¢ç®—æ³•
            
            æ²¡æœ‰é‡åˆ°é‡å¤§é—®é¢˜ã€‚
            """,
            "user_marked": False,
            "access_count": 2,
            "age_days": 0
        },
        {
            "content": """
            å›¢é˜Ÿå‘¨ä¼šè®°å½•
            
            å‚ä¼šäººå‘˜ï¼šCEO Kimi Claw, ç ”å‘è´Ÿè´£äºº, äº§å“ç»ç†
            æ—¶é—´ï¼š2026-02-27 10:00
            
            è®¨è®ºå†…å®¹ï¼š
            - å›é¡¾ä¸Šå‘¨è¿›åº¦
            - è®¨è®ºä¸‹å‘¨è®¡åˆ’
            - èµ„æºåˆ†é…è°ƒæ•´
            
            ä¸»è¦æ˜¯ä¾‹è¡Œæ²Ÿé€šï¼Œæ²¡æœ‰é‡è¦å†³ç­–ã€‚
            """,
            "user_marked": False,
            "access_count": 1,
            "age_days": 7
        }
    ]
    
    engine = MemoryCompressionEngine()
    
    print("\nğŸ“Š è®°å¿†å‹ç¼©æ¼”ç¤º\n")
    print("-" * 70)
    
    compressed_memories = []
    
    for i, mem in enumerate(test_memories, 1):
        print(f"\nã€è®°å¿† {i}ã€‘")
        
        # å‹ç¼©
        result = engine.compress(
            content=mem["content"],
            access_count=mem["access_count"],
            user_marked=mem["user_marked"],
            age_days=mem["age_days"]
        )
        
        compressed_memories.append(result)
        
        print(f"  é‡è¦æ€§è¯„åˆ†: {result['importance_score']}/5.0")
        print(f"  å‹ç¼©çº§åˆ«: {result['compression_level']} ({result['level_name']})")
        print(f"  å‹ç¼©ç‡: {result['compression_ratio']:.1%}")
        print(f"  åŸå§‹é•¿åº¦: {result['original_length']} å­—ç¬¦")
        print(f"  å‹ç¼©å: {result['compressed_length']} å­—ç¬¦")
        
        if result.get("keypoints"):
            print(f"\n  å…³é”®è¦ç‚¹:")
            for j, point in enumerate(result["keypoints"][:3], 1):
                print(f"    {j}. {point[:50]}...")
        
        if result.get("entities"):
            entities = [e["name"] for e in result["entities"][:5]]
            print(f"\n  å…³é”®å®ä½“: {', '.join(entities)}")
    
    print("\n" + "=" * 70)
    print("ğŸ“ˆ å‹ç¼©ç»Ÿè®¡")
    print("=" * 70)
    
    stats = engine.get_stats(compressed_memories)
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2%}" if "ratio" in key else f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")
    
    print("\n" + "=" * 70)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)


if __name__ == "__main__":
    demo()
