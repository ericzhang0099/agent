#!/usr/bin/env python3
"""
é•¿æœŸè®°å¿†å‹ç¼©ä¼˜åŒ–ç³»ç»Ÿ v2.0
å®ç°æ™ºèƒ½å‹ç¼©ã€é‡è¦æ€§è¯„åˆ†ã€æ··åˆæ£€ç´¢
"""

import os
import json
import hashlib
import re
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, asdict, field
from collections import defaultdict
import heapq

# é…ç½®
DEFAULT_CONFIG = {
    "hot_storage_days": 7,
    "warm_storage_days": 30,
    "compression_threshold_high": 4.0,
    "compression_threshold_medium": 2.0,
    "compression_threshold_low": 1.0,
    "recency_half_life_days": 30,
    "time_decay_half_life": 60,
    "semantic_similarity_threshold": 0.85,
    "max_summary_ratio": 0.6,
    "keyword_weight": 0.3,
    "semantic_weight": 0.7,
}

# ç¡®ä¿æ‰€æœ‰é…ç½®é¡¹éƒ½æœ‰é»˜è®¤å€¼
DEFAULT_CONFIG_FULL = {
    **DEFAULT_CONFIG,
    "embedding_dim": 384,
    "max_candidates": 100,
    "summary_sentences": 3,
}


@dataclass
class MemoryRecord:
    """ä¼˜åŒ–åçš„è®°å¿†è®°å½•ç»“æ„"""
    
    # åŸºç¡€æ ‡è¯†
    id: str
    parent_id: Optional[str] = None
    
    # å†…å®¹ï¼ˆåˆ†å±‚å­˜å‚¨ï¼‰
    content_full: Optional[str] = None
    content_summary: Optional[str] = None
    content_keypoints: List[str] = field(default_factory=list)
    
    # å‹ç¼©å…ƒæ•°æ®
    compression_level: int = 5  # 0-5, 5=æœªå‹ç¼©
    compression_ratio: float = 1.0
    original_length: int = 0
    compressed_length: int = 0
    
    # é‡è¦æ€§è¯„åˆ†
    importance_score: float = 0.0
    importance_factors: Dict[str, float] = field(default_factory=dict)
    
    # è®¿é—®ç»Ÿè®¡
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    access_pattern: List[datetime] = field(default_factory=list)
    
    # åˆ†ç±»æ ‡ç­¾
    memory_type: str = "general"
    categories: List[str] = field(default_factory=list)
    entities: List[str] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    
    # æ—¶é—´æˆ³
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    compressed_at: Optional[datetime] = None
    
    # æ¥æºè¿½è¸ª
    source: str = ""
    context: Dict[str, Any] = field(default_factory=dict)
    
    # ç”¨æˆ·äº¤äº’
    user_marked_important: bool = False
    user_notes: Optional[str] = None
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰"""
        data = asdict(self)
        # è½¬æ¢datetimeä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
        for key in ['created_at', 'updated_at', 'last_accessed', 'compressed_at']:
            if data[key]:
                data[key] = data[key].isoformat() if isinstance(data[key], datetime) else data[key]
        for key in ['access_pattern']:
            data[key] = [d.isoformat() if isinstance(d, datetime) else d for d in data[key]]
        return data
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'MemoryRecord':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        # è½¬æ¢ISOæ ¼å¼å­—ç¬¦ä¸²ä¸ºdatetime
        for key in ['created_at', 'updated_at', 'last_accessed', 'compressed_at']:
            if data.get(key):
                data[key] = datetime.fromisoformat(data[key])
        for key in ['access_pattern']:
            if data.get(key):
                data[key] = [datetime.fromisoformat(d) for d in data[key]]
        return cls(**data)


class TextProcessor:
    """æ–‡æœ¬å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def split_sentences(text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­"""
        # æ”¯æŒä¸­è‹±æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    @staticmethod
    def extract_keywords(text: str, top_k: int = 10) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•TF-IDFè¿‘ä¼¼ï¼‰"""
        # åˆ†è¯
        words = re.findall(r'\b\w+\b', text.lower())
        
        # åœç”¨è¯è¿‡æ»¤
        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 
                     'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–'}
        words = [w for w in words if w not in stopwords and len(w) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        # è¿”å›Top-K
        return [word for word, _ in heapq.nlargest(top_k, word_freq.items(), key=lambda x: x[1])]
    
    @staticmethod
    def extract_entities(text: str) -> List[str]:
        """æå–å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäºå¤§å†™å’Œå¼•å·ï¼‰"""
        # æå–å¼•å·å†…çš„å†…å®¹
        quoted = re.findall(r'["""]([^"""]+)["""]', text)
        # æå–å¤§å†™å•è¯ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼‰
        capitalized = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        # åˆå¹¶å»é‡
        entities = list(set(quoted + capitalized))
        return entities[:20]  # é™åˆ¶æ•°é‡
    
    @staticmethod
    def calculate_similarity(text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºJaccardï¼‰"""
        set1 = set(TextProcessor.extract_keywords(text1, top_k=50))
        set2 = set(TextProcessor.extract_keywords(text2, top_k=50))
        
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0


class MemoryImportanceScorer:
    """
    è®°å¿†é‡è¦æ€§è¯„åˆ†ç®—æ³•
    
    è¯„åˆ†ç»´åº¦:
    1. è®¿é—®é¢‘ç‡ (30%)
    2. å†³ç­–å…³é”®åº¦ (25%)
    3. ä¿¡æ¯å¯†åº¦ (20%)
    4. æ—¶æ•ˆæ€§ (15%)
    5. ç”¨æˆ·æ˜¾å¼æ ‡è®° (10%)
    """
    
    DECISION_KEYWORDS = [
        'å†³ç­–', 'å†³å®š', 'é€‰æ‹©', 'æ‰¹å‡†', 'æ‹’ç»', 'å…³é”®', 'é‡è¦',
        'decision', 'decide', 'choose', 'approve', 'reject', 'critical', 'important'
    ]
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_CONFIG
    
    def calculate_importance(self, memory: MemoryRecord) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—è®°å¿†é‡è¦æ€§è¯„åˆ†
        
        Returns:
            (æ€»åˆ†, å„ç»´åº¦å¾—åˆ†è¯¦æƒ…)
        """
        factors = {}
        
        # 1. è®¿é—®é¢‘ç‡åˆ†æ•° (0-1.5)
        access_score = min(memory.access_count / 10, 1.5)
        factors['access_frequency'] = round(access_score, 2)
        
        # 2. å†³ç­–å…³é”®åº¦ (0-1.25)
        content_lower = memory.content_full.lower() if memory.content_full else ""
        decision_matches = sum(1 for kw in self.DECISION_KEYWORDS if kw in content_lower)
        decision_score = min(decision_matches * 0.25, 1.25)
        factors['decision_criticality'] = round(decision_score, 2)
        
        # 3. ä¿¡æ¯å¯†åº¦ (0-1.0)
        entities = memory.entities or TextProcessor.extract_entities(content_lower)
        keywords = memory.keywords or TextProcessor.extract_keywords(content_lower, top_k=50)
        
        entity_score = min(len(entities) * 0.1, 0.5)
        keyword_score = min(len(keywords) / 20, 0.5)
        density_score = entity_score + keyword_score
        factors['information_density'] = round(density_score, 2)
        
        # 4. æ—¶æ•ˆæ€§ (0-0.75) - æŒ‡æ•°è¡°å‡
        days_old = (datetime.now() - memory.created_at).days
        recency_score = 0.75 * np.exp(-days_old / self.config['recency_half_life_days'])
        factors['recency'] = round(recency_score, 2)
        
        # 5. ç”¨æˆ·æ ‡è®° (0-0.5)
        user_score = 0.5 if memory.user_marked_important else 0
        factors['user_marked'] = user_score
        
        # æ€»åˆ† (0-5)
        total_score = access_score + decision_score + density_score + recency_score + user_score
        
        return round(total_score, 2), factors
    
    def batch_score(self, memories: List[MemoryRecord]) -> List[Tuple[MemoryRecord, float, Dict]]:
        """æ‰¹é‡è¯„åˆ†"""
        results = []
        for memory in memories:
            score, factors = self.calculate_importance(memory)
            results.append((memory, score, factors))
        return results


class MemoryCompressor:
    """
    è®°å¿†æ™ºèƒ½å‹ç¼©ç®—æ³•
    
    å‹ç¼©ç­–ç•¥:
    - 5åˆ†: ä¿ç•™å®Œæ•´æ–‡æœ¬ + å®Œæ•´å…ƒæ•°æ®
    - 4åˆ†: ä¿ç•™å®Œæ•´æ–‡æœ¬ + ç²¾ç®€å…ƒæ•°æ®
    - 3åˆ†: ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ (ä¿ç•™60%ä¿¡æ¯)
    - 2åˆ†: æå–å…³é”®è¦ç‚¹ (ä¿ç•™30%ä¿¡æ¯)
    - 1åˆ†: æå–æ ¸å¿ƒå®ä½“å’Œå…³ç³»
    - 0åˆ†: ä»…ä¿ç•™ç´¢å¼•ï¼Œå½’æ¡£åˆ°å†·å­˜å‚¨
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_CONFIG
        self.text_processor = TextProcessor()
    
    def compress(self, memory: MemoryRecord, importance: float) -> MemoryRecord:
        """
        æ ¹æ®é‡è¦æ€§è¯„åˆ†å‹ç¼©è®°å¿†
        
        Returns:
            å‹ç¼©åçš„è®°å¿†è®°å½•ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
        """
        if memory.content_full is None:
            return memory
        
        original_content = memory.content_full
        original_length = len(original_content)
        memory.original_length = original_length
        
        if importance >= 4.0:
            # é«˜é‡è¦æ€§: å®Œæ•´ä¿ç•™
            memory.compression_level = 5
            memory.compression_ratio = 1.0
            memory.compressed_length = original_length
            
        elif importance >= 3.0:
            # ä¸­é«˜é‡è¦æ€§: ç»“æ„åŒ–æ‘˜è¦
            summary = self._generate_summary(original_content, ratio=self.config['max_summary_ratio'])
            memory.content_summary = summary
            memory.compression_level = 3
            memory.compression_ratio = len(summary) / original_length
            memory.compressed_length = len(summary)
            
        elif importance >= 2.0:
            # ä¸­ç­‰é‡è¦æ€§: å…³é”®è¦ç‚¹
            keypoints = self._extract_key_points(original_content)
            memory.content_keypoints = keypoints
            memory.content_summary = None  # æ¸…é™¤æ‘˜è¦å±‚
            memory.compression_level = 2
            compressed_text = ' '.join(keypoints)
            memory.compression_ratio = len(compressed_text) / original_length
            memory.compressed_length = len(compressed_text)
            
        elif importance >= 1.0:
            # ä½é‡è¦æ€§: æ ¸å¿ƒå®ä½“
            entities = self.text_processor.extract_entities(original_content)
            memory.entities = entities
            memory.content_keypoints = []  # æ¸…é™¤è¦ç‚¹å±‚
            memory.content_summary = None
            memory.compression_level = 1
            compressed_text = ', '.join(entities)
            memory.compression_ratio = len(compressed_text) / original_length if original_length > 0 else 0
            memory.compressed_length = len(compressed_text)
            
        else:
            # æä½é‡è¦æ€§: ä»…ä¿ç•™ç´¢å¼•
            memory.content_full = None  # æ¸…é™¤å®Œæ•´å†…å®¹
            memory.content_summary = None
            memory.content_keypoints = []
            memory.compression_level = 0
            memory.compression_ratio = 0.0
            memory.compressed_length = 0
        
        memory.compressed_at = datetime.now()
        memory.updated_at = datetime.now()
        
        return memory
    
    def _generate_summary(self, text: str, ratio: float = 0.6) -> str:
        """ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼ˆåŸºäºå¥å­é‡è¦æ€§ï¼‰"""
        sentences = self.text_processor.split_sentences(text)
        
        if len(sentences) <= 3:
            return text
        
        # è®¡ç®—å¥å­é‡è¦æ€§
        sentence_scores = {}
        keywords = set(self.text_processor.extract_keywords(text, top_k=30))
        
        for i, sent in enumerate(sentences):
            score = 0
            # å…³é”®è¯åŒ¹é…
            sent_keywords = set(self.text_processor.extract_keywords(sent, top_k=10))
            score += len(sent_keywords & keywords) * 2
            
            # ä½ç½®æƒé‡ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
            if i == 0:
                score += 5
            elif i == len(sentences) - 1:
                score += 3
            elif i < len(sentences) * 0.2:
                score += 2
            
            # é•¿åº¦æƒ©ç½šï¼ˆè¿‡é•¿æˆ–è¿‡çŸ­çš„å¥å­å‡åˆ†ï¼‰
            sent_len = len(sent)
            if sent_len < 10:
                score -= 2
            elif sent_len > 200:
                score -= 1
            
            sentence_scores[i] = score
        
        # é€‰æ‹©Top-Kå¥å­
        k = max(1, int(len(sentences) * ratio))
        top_sentences = heapq.nlargest(k, sentence_scores.items(), key=lambda x: x[1])
        top_sentences.sort(key=lambda x: x[0])  # æŒ‰åŸæ–‡é¡ºåºæ’åˆ—
        
        summary = ' '.join([sentences[i] for i, _ in top_sentences])
        return summary
    
    def _extract_key_points(self, text: str) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        sentences = self.text_processor.split_sentences(text)
        
        if len(sentences) <= 5:
            return sentences
        
        # é€‰æ‹©æœ€é‡è¦çš„30%å¥å­ä½œä¸ºå…³é”®è¦ç‚¹
        keywords = set(self.text_processor.extract_keywords(text, top_k=20))
        sentence_scores = []
        
        for i, sent in enumerate(sentences):
            sent_keywords = set(self.text_processor.extract_keywords(sent, top_k=5))
            score = len(sent_keywords & keywords)
            if i == 0:  # é¦–å¥åŠ åˆ†
                score += 3
            sentence_scores.append((i, score))
        
        # é€‰æ‹©Top 30%
        k = max(1, int(len(sentences) * 0.3))
        top_sentences = heapq.nlargest(k, sentence_scores, key=lambda x: x[1])
        top_sentences.sort(key=lambda x: x[0])
        
        return [sentences[i] for i, _ in top_sentences]
    
    def decompress(self, memory: MemoryRecord) -> str:
        """
        è§£å‹è®°å¿†ï¼ˆè·å–æœ€ä½³å¯ç”¨å†…å®¹ï¼‰
        
        Returns:
            å¯ç”¨çš„æ–‡æœ¬å†…å®¹
        """
        if memory.content_full:
            return memory.content_full
        elif memory.content_summary:
            return memory.content_summary
        elif memory.content_keypoints:
            return '\n'.join(memory.content_keypoints)
        elif memory.entities:
            return 'ç›¸å…³å®ä½“: ' + ', '.join(memory.entities)
        else:
            return f"[å·²å½’æ¡£è®°å¿†: {memory.id}]"


class DuplicateDetector:
    """é‡å¤è®°å¿†æ£€æµ‹å™¨"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.text_processor = TextProcessor()
    
    def find_duplicates(self, memories: List[MemoryRecord]) -> List[Tuple[str, str, float]]:
        """
        æŸ¥æ‰¾é‡å¤çš„è®°å¿†å¯¹
        
        Returns:
            [(id1, id2, similarity), ...]
        """
        duplicates = []
        n = len(memories)
        
        for i in range(n):
            for j in range(i + 1, n):
                mem1 = memories[i]
                mem2 = memories[j]
                
                content1 = mem1.content_full or mem1.content_summary or ''
                content2 = mem2.content_full or mem2.content_summary or ''
                
                if not content1 or not content2:
                    continue
                
                similarity = self.text_processor.calculate_similarity(content1, content2)
                
                if similarity >= self.similarity_threshold:
                    duplicates.append((mem1.id, mem2.id, similarity))
        
        return duplicates
    
    def merge_duplicates(self, mem1: MemoryRecord, mem2: MemoryRecord) -> MemoryRecord:
        """åˆå¹¶é‡å¤çš„è®°å¿†ï¼ˆä¿ç•™ä¿¡æ¯æ›´å®Œæ•´çš„ç‰ˆæœ¬ï¼‰"""
        # é€‰æ‹©æ›´é•¿çš„ã€æ›´æ–°çš„ç‰ˆæœ¬
        if mem1.created_at > mem2.created_at:
            newer, older = mem1, mem2
        else:
            newer, older = mem2, mem1
        
        # åˆå¹¶è®¿é—®ç»Ÿè®¡
        newer.access_count += older.access_count
        newer.access_pattern.extend(older.access_pattern)
        newer.access_pattern.sort()
        
        # åˆå¹¶å®ä½“å’Œå…³é”®è¯
        newer.entities = list(set(newer.entities + older.entities))
        newer.keywords = list(set(newer.keywords + older.keywords))
        
        # å¦‚æœæ—§ç‰ˆæœ¬æœ‰ç”¨æˆ·æ ‡è®°ï¼Œä¿ç•™
        if older.user_marked_important:
            newer.user_marked_important = True
        
        newer.updated_at = datetime.now()
        
        return newer


class HybridRetriever:
    """
    è¯­ä¹‰ + å…³é”®è¯ + é‡è¦æ€§ æ··åˆæ£€ç´¢å¼•æ“
    
    æ£€ç´¢æµç¨‹:
    1. å…³é”®è¯é¢„è¿‡æ»¤ (å¿«é€Ÿç¼©å°å€™é€‰é›†)
    2. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®— (ç²¾ç¡®åŒ¹é…)
    3. é‡è¦æ€§åŠ æƒ (ä¼˜å…ˆé«˜ä»·å€¼è®°å¿†)
    4. æ—¶é—´è¡°å‡è°ƒæ•´ (å¹³è¡¡æ–°æ—§è®°å¿†)
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or DEFAULT_CONFIG
        self.text_processor = TextProcessor()
        
        # ç´¢å¼•ç»“æ„
        self.inverted_index: Dict[str, Set[str]] = defaultdict(set)
        self.importance_index: Dict[float, Set[str]] = defaultdict(set)
        self.time_index: Dict[str, Set[str]] = defaultdict(set)
        self.type_index: Dict[str, Set[str]] = defaultdict(set)
        
        # å‘é‡ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å…³é”®è¯å‘é‡ï¼‰
        self.vector_index: Dict[str, np.ndarray] = {}
    
    def build_index(self, memories: List[MemoryRecord]):
        """æ„å»ºç´¢å¼•"""
        for memory in memories:
            self._index_memory(memory)
    
    def _index_memory(self, memory: MemoryRecord):
        """ç´¢å¼•å•ä¸ªè®°å¿†"""
        # å€’æ’ç´¢å¼•
        keywords = memory.keywords or []
        for kw in keywords:
            self.inverted_index[kw].add(memory.id)
        
        # é‡è¦æ€§ç´¢å¼•
        importance_bucket = round(memory.importance_score)
        self.importance_index[importance_bucket].add(memory.id)
        
        # æ—¶é—´ç´¢å¼•
        date_key = memory.created_at.strftime('%Y-%m-%d')
        self.time_index[date_key].add(memory.id)
        
        # ç±»å‹ç´¢å¼•
        self.type_index[memory.memory_type].add(memory.id)
        
        # å‘é‡ç´¢å¼•ï¼ˆå…³é”®è¯çš„one-hotè¿‘ä¼¼ï¼‰
        self.vector_index[memory.id] = self._compute_vector(memory)
    
    def _compute_vector(self, memory: MemoryRecord) -> np.ndarray:
        """è®¡ç®—è®°å¿†çš„å‘é‡è¡¨ç¤ºï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨å…³é”®è¯çš„å“ˆå¸Œä½œä¸ºå‘é‡
        keywords = memory.keywords or []
        vector = np.zeros(128)
        
        for kw in keywords:
            # ä½¿ç”¨å“ˆå¸Œå€¼å¡«å……å‘é‡
            hash_val = int(hashlib.md5(kw.encode()).hexdigest(), 16)
            for i in range(128):
                if (hash_val >> i) & 1:
                    vector[i] += 1
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector
    
    def retrieve(self, query: str, memories: Dict[str, MemoryRecord], 
                 top_k: int = 5, memory_type: Optional[str] = None) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ä¸»å‡½æ•°
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            memories: è®°å¿†å­—å…¸ {id: MemoryRecord}
            top_k: è¿”å›ç»“æœæ•°é‡
            memory_type: å¯é€‰çš„è®°å¿†ç±»å‹è¿‡æ»¤
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # Step 1: å…³é”®è¯é¢„è¿‡æ»¤
        query_keywords = self.text_processor.extract_keywords(query, top_k=10)
        candidate_ids = self._keyword_filter(query_keywords, memory_type)
        
        if not candidate_ids:
            return []
        
        # Step 2: è®¡ç®—æŸ¥è¯¢å‘é‡
        query_vector = self._compute_query_vector(query_keywords)
        
        # Step 3: è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®— + é‡è¦æ€§åŠ æƒ + æ—¶é—´è¡°å‡
        scored_results = []
        
        for mem_id in candidate_ids:
            if mem_id not in memories:
                continue
            
            memory = memories[mem_id]
            
            # è¯­ä¹‰ç›¸ä¼¼åº¦
            mem_vector = self.vector_index.get(mem_id, np.zeros(128))
            semantic_sim = np.dot(query_vector, mem_vector)
            
            # å…³é”®è¯åŒ¹é…åº¦
            keyword_match = len(set(query_keywords) & set(memory.keywords or []))
            keyword_score = min(keyword_match / len(query_keywords), 1.0) if query_keywords else 0
            
            # æ··åˆåŸºç¡€åˆ†
            base_score = (semantic_sim * self.config['semantic_weight'] + 
                         keyword_score * self.config['keyword_weight'])
            
            # é‡è¦æ€§åŠ æƒ
            importance_boost = 1 + memory.importance_score * 0.2
            weighted_score = base_score * importance_boost
            
            # æ—¶é—´è¡°å‡è°ƒæ•´
            age_days = (datetime.now() - memory.created_at).days
            time_decay = np.exp(-age_days / self.config['time_decay_half_life'])
            final_score = weighted_score * 0.7 + weighted_score * time_decay * 0.3
            
            scored_results.append({
                'id': mem_id,
                'score': final_score,
                'semantic_score': semantic_sim,
                'keyword_score': keyword_score,
                'importance': memory.importance_score,
                'memory': memory
            })
        
        # æ’åºå¹¶è¿”å›Top-K
        scored_results.sort(key=lambda x: x['score'], reverse=True)
        return scored_results[:top_k]
    
    def _keyword_filter(self, query_keywords: List[str], 
                        memory_type: Optional[str] = None) -> Set[str]:
        """å…³é”®è¯é¢„è¿‡æ»¤"""
        candidate_ids = set()
        
        for kw in query_keywords:
            candidate_ids.update(self.inverted_index.get(kw, set()))
        
        # ç±»å‹è¿‡æ»¤
        if memory_type and candidate_ids:
            type_ids = self.type_index.get(memory_type, set())
            candidate_ids = candidate_ids & type_ids
        
        return candidate_ids
    
    def _compute_query_vector(self, keywords: List[str]) -> np.ndarray:
        """è®¡ç®—æŸ¥è¯¢å‘é‡"""
        vector = np.zeros(128)
        
        for kw in keywords:
            hash_val = int(hashlib.md5(kw.encode()).hexdigest(), 16)
            for i in range(128):
                if (hash_val >> i) & 1:
                    vector[i] += 1
        
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector


class MemoryCompressionSystem:
    """
    è®°å¿†å‹ç¼©ç³»ç»Ÿä¸»ç±»
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€æ¥å£
    """
    
    def __init__(self, storage_dir: str = "./compressed_memory", config: Dict = None):
        self.storage_dir = storage_dir
        # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
        self.config = {**DEFAULT_CONFIG, **(config or {})}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.scorer = MemoryImportanceScorer(self.config)
        self.compressor = MemoryCompressor(self.config)
        self.duplicate_detector = DuplicateDetector(
            self.config.get('semantic_similarity_threshold', 0.85)
        )
        self.retriever = HybridRetriever(self.config)
        
        # å†…å­˜å­˜å‚¨
        self.memories: Dict[str, MemoryRecord] = {}
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(storage_dir, exist_ok=True)
    
    def add_memory(self, content: str, source: str, memory_type: str = "general",
                   metadata: Dict = None) -> MemoryRecord:
        """æ·»åŠ æ–°è®°å¿†"""
        # ç”ŸæˆID
        memory_id = hashlib.md5(f"{content}:{source}:{datetime.now()}".encode()).hexdigest()[:16]
        
        # æå–å…ƒæ•°æ®
        metadata = metadata or {}
        keywords = TextProcessor.extract_keywords(content, top_k=20)
        entities = TextProcessor.extract_entities(content)
        
        # åˆ›å»ºè®°å¿†è®°å½•
        memory = MemoryRecord(
            id=memory_id,
            content_full=content,
            memory_type=memory_type,
            source=source,
            keywords=keywords,
            entities=entities,
            categories=metadata.get('categories', []),
            context=metadata.get('context', {}),
            user_marked_important=metadata.get('user_marked_important', False)
        )
        
        # è®¡ç®—é‡è¦æ€§å¹¶å‹ç¼©
        importance, factors = self.scorer.calculate_importance(memory)
        memory.importance_score = importance
        memory.importance_factors = factors
        
        memory = self.compressor.compress(memory, importance)
        
        # å­˜å‚¨
        self.memories[memory_id] = memory
        
        return memory
    
    def search(self, query: str, top_k: int = 5, 
               memory_type: Optional[str] = None) -> List[Dict]:
        """æœç´¢è®°å¿†"""
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        results = self.retriever.retrieve(query, self.memories, top_k, memory_type)
        
        for result in results:
            memory = result['memory']
            memory.access_count += 1
            memory.last_accessed = datetime.now()
            memory.access_pattern.append(datetime.now())
        
        return results
    
    def compress_all(self, target_ratio: float = 0.6):
        """å‹ç¼©æ‰€æœ‰è®°å¿†"""
        for memory in self.memories.values():
            # é‡æ–°è®¡ç®—é‡è¦æ€§ï¼ˆè€ƒè™‘æ–°çš„è®¿é—®ç»Ÿè®¡ï¼‰
            importance, factors = self.scorer.calculate_importance(memory)
            memory.importance_score = importance
            memory.importance_factors = factors
            
            # é‡æ–°å‹ç¼©
            self.compressor.compress(memory, importance)
        
        # è®¡ç®—å‹ç¼©ç»Ÿè®¡
        total_original = sum(m.original_length for m in self.memories.values())
        total_compressed = sum(m.compressed_length for m in self.memories.values())
        actual_ratio = total_compressed / total_original if total_original > 0 else 0
        
        return {
            'total_memories': len(self.memories),
            'total_original_bytes': total_original,
            'total_compressed_bytes': total_compressed,
            'compression_ratio': actual_ratio,
            'target_ratio': target_ratio
        }
    
    def deduplicate(self) -> Dict:
        """å»é‡"""
        duplicates = self.duplicate_detector.find_duplicates(list(self.memories.values()))
        
        removed_ids = []
        for id1, id2, similarity in duplicates:
            if id1 in self.memories and id2 in self.memories:
                merged = self.duplicate_detector.merge_duplicates(
                    self.memories[id1], self.memories[id2]
                )
                self.memories[id1] = merged
                del self.memories[id2]
                removed_ids.append(id2)
        
        return {
            'duplicates_found': len(duplicates),
            'removed_ids': removed_ids
        }
    
    def save(self):
        """ä¿å­˜åˆ°ç£ç›˜"""
        data = {
            'memories': {k: v.to_dict() for k, v in self.memories.items()},
            'config': self.config,
            'saved_at': datetime.now().isoformat()
        }
        
        filepath = os.path.join(self.storage_dir, 'memory_store.json')
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self):
        """ä»ç£ç›˜åŠ è½½"""
        filepath = os.path.join(self.storage_dir, 'memory_store.json')
        if not os.path.exists(filepath):
            return False
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.memories = {
            k: MemoryRecord.from_dict(v) for k, v in data['memories'].items()
        }
        self.config = data.get('config', DEFAULT_CONFIG)
        
        # é‡å»ºç´¢å¼•
        self.retriever.build_index(list(self.memories.values()))
        
        return True
    
    def get_stats(self) -> Dict:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.memories:
            return {'total_memories': 0}
        
        total_original = sum(m.original_length for m in self.memories.values())
        total_compressed = sum(m.compressed_length for m in self.memories.values())
        
        compression_levels = defaultdict(int)
        for m in self.memories.values():
            compression_levels[m.compression_level] += 1
        
        return {
            'total_memories': len(self.memories),
            'total_original_bytes': total_original,
            'total_compressed_bytes': total_compressed,
            'overall_compression_ratio': total_compressed / total_original if total_original > 0 else 0,
            'compression_level_distribution': dict(compression_levels),
            'avg_importance_score': sum(m.importance_score for m in self.memories.values()) / len(self.memories),
            'total_access_count': sum(m.access_count for m in self.memories.values())
        }


# ============ æµ‹è¯•ä¸æ¼”ç¤º ============

def demo():
    """æ¼”ç¤ºè®°å¿†å‹ç¼©ç³»ç»Ÿçš„åŠŸèƒ½"""
    print("=" * 60)
    print("é•¿æœŸè®°å¿†å‹ç¼©ä¼˜åŒ–ç³»ç»Ÿ v2.0 - åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = MemoryCompressionSystem(storage_dir="./demo_memory")
    
    # ç¤ºä¾‹è®°å¿†æ•°æ®
    sample_memories = [
        {
            "content": """
            2026-02-27 é‡è¦å†³ç­–è®°å½•
            
            ä»Šå¤©è‘£äº‹é•¿å…°å±±å†³å®šå¯åŠ¨è®°å¿†ç³»ç»Ÿä¼˜åŒ–é¡¹ç›®ã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®å†³ç­–ï¼Œå°†æ˜¾è‘—æå‡å›¢é˜Ÿçš„é•¿æœŸè®°å¿†èƒ½åŠ›ã€‚
            é¡¹ç›®ç›®æ ‡åŒ…æ‹¬ï¼š
            1. å®ç°è®°å¿†å‹ç¼©ï¼Œå‡å°‘60%å­˜å‚¨ç©ºé—´
            2. ä¼˜åŒ–æ£€ç´¢æ•ˆç‡ï¼Œæå‡4å€é€Ÿåº¦
            3. å»ºç«‹é‡è¦æ€§è¯„åˆ†æœºåˆ¶
            
            è¿™æ˜¯ä¸€ä¸ªæˆ˜ç•¥æ€§å†³ç­–ï¼Œéœ€è¦å…¨åŠ›ä»¥èµ´ã€‚
            """,
            "source": "decisions/memory_optimization.md",
            "type": "decision",
            "metadata": {"user_marked_important": True}
        },
        {
            "content": """
            æ—¥å¸¸ä¼šè®®è®°å½• - å›¢é˜Ÿå‘¨ä¼š
            
            å‚ä¼šäººå‘˜ï¼šCEO Kimi Claw, research-lead, dev-arch
            ä¼šè®®æ—¶é—´ï¼š2026-02-27 10:00
            
            è®¨è®ºå†…å®¹ï¼š
            - reviewä¸Šå‘¨è¿›åº¦
            - è®¨è®ºä¸‹å‘¨è®¡åˆ’
            - èµ„æºåˆ†é…è°ƒæ•´
            
            æ²¡æœ‰ç‰¹åˆ«é‡è¦çš„å†³ç­–ï¼Œä¸»è¦æ˜¯ä¾‹è¡Œæ²Ÿé€šã€‚
            """,
            "source": "meetings/weekly_2026-02-27.md",
            "type": "meeting",
            "metadata": {}
        },
        {
            "content": """
            æŠ€æœ¯è°ƒç ”ç¬”è®° - ChromaDBå‘é‡æ•°æ®åº“
            
            ChromaDBæ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œé€‚åˆå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡ã€‚
            ä¸»è¦ç‰¹ç‚¹ï¼š
            - æ”¯æŒå¤šç§ç›¸ä¼¼åº¦åº¦é‡ï¼ˆä½™å¼¦ã€æ¬§æ°è·ç¦»ç­‰ï¼‰
            - æä¾›Pythonå®¢æˆ·ç«¯ï¼Œæ˜“äºé›†æˆ
            - æ”¯æŒæŒä¹…åŒ–å­˜å‚¨
            - æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤
            
            åœ¨æˆ‘ä»¬çš„è®°å¿†ç³»ç»Ÿä¸­ï¼ŒChromaDBå¯ä»¥ç”¨æ¥å­˜å‚¨è®°å¿†å‘é‡ï¼Œå®ç°è¯­ä¹‰æ£€ç´¢ã€‚
            """,
            "source": "research/chromadb_notes.md",
            "type": "research",
            "metadata": {}
        },
        {
            "content": """
            é‡å¤å†…å®¹æµ‹è¯• - è®°å¿†å‹ç¼©çš„é‡è¦æ€§
            
            è®°å¿†å‹ç¼©å¯¹äºé•¿æœŸè®°å¿†ç³»ç»Ÿéå¸¸é‡è¦ã€‚é€šè¿‡å‹ç¼©ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
            1. å‡å°‘å­˜å‚¨ç©ºé—´
            2. æé«˜æ£€ç´¢æ•ˆç‡
            3. ä¿ç•™å…³é”®ä¿¡æ¯
            
            å‹ç¼©ç®—æ³•éœ€è¦æ ¹æ®é‡è¦æ€§è¿›è¡Œåˆ†å±‚å¤„ç†ã€‚
            """,
            "source": "test/duplicate_test.md",
            "type": "test",
            "metadata": {}
        },
        {
            "content": """
            è®°å¿†å‹ç¼©çš„é‡è¦æ€§è¯´æ˜
            
            è®°å¿†å‹ç¼©æ˜¯é•¿æœŸè®°å¿†ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ã€‚å®ƒçš„é‡è¦æ€§ä½“ç°åœ¨ï¼š
            1. å¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨ç©ºé—´éœ€æ±‚
            2. èƒ½å¤Ÿæé«˜æ£€ç´¢é€Ÿåº¦å’Œæ•ˆç‡
            3. å¸®åŠ©ä¿ç•™æœ€å…³é”®çš„ä¿¡æ¯
            
            æˆ‘ä»¬éœ€è¦æ ¹æ®è®°å¿†çš„é‡è¦æ€§è¿›è¡Œåˆ†å±‚å‹ç¼©å¤„ç†ã€‚
            """,
            "source": "docs/compression_importance.md",
            "type": "documentation",
            "metadata": {}
        }
    ]
    
    print("\nğŸ“¥ æ­¥éª¤1: æ·»åŠ ç¤ºä¾‹è®°å¿†")
    print("-" * 40)
    
    for i, mem_data in enumerate(sample_memories, 1):
        memory = system.add_memory(
            content=mem_data["content"],
            source=mem_data["source"],
            memory_type=mem_data["type"],
            metadata=mem_data["metadata"]
        )
        print(f"  [{i}] {memory.memory_type:12} | "
              f"é‡è¦æ€§: {memory.importance_score:.2f} | "
              f"å‹ç¼©çº§åˆ«: {memory.compression_level} | "
              f"æ¥æº: {memory.source}")
    
    print("\nğŸ“Š æ­¥éª¤2: æŸ¥çœ‹ç³»ç»Ÿç»Ÿè®¡")
    print("-" * 40)
    stats = system.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ” æ­¥éª¤3: æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("-" * 40)
    
    test_queries = [
        "è®°å¿†å‹ç¼©ä¼˜åŒ–",
        "ChromaDBå‘é‡æ•°æ®åº“",
        "é‡è¦å†³ç­–",
        "å›¢é˜Ÿä¼šè®®"
    ]
    
    for query in test_queries:
        print(f"\n  æŸ¥è¯¢: '{query}'")
        results = system.search(query, top_k=3)
        for r in results:
            memory = r['memory']
            print(f"    â†’ [{memory.memory_type}] ç›¸å…³åº¦: {r['score']:.3f} | "
                  f"é‡è¦æ€§: {memory.importance_score:.2f} | "
                  f"æ¥æº: {memory.source}")
    
    print("\nğŸ—‘ï¸  æ­¥éª¤4: å»é‡æ£€æµ‹")
    print("-" * 40)
    dedup_result = system.deduplicate()
    print(f"  å‘ç°é‡å¤: {dedup_result['duplicates_found']} å¯¹")
    print(f"  ç§»é™¤ID: {dedup_result['removed_ids']}")
    
    print("\nğŸ’¾ æ­¥éª¤5: ä¿å­˜ç³»ç»ŸçŠ¶æ€")
    print("-" * 40)
    system.save()
    print(f"  å·²ä¿å­˜åˆ°: {system.storage_dir}/memory_store.json")
    
    print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡")
    print("-" * 40)
    final_stats = system.get_stats()
    for key, value in final_stats.items():
        print(f"  {key}: {value}")
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    
    return system


if __name__ == "__main__":
    demo()
