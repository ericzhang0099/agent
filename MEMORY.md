# MEMORY.md v3.0 - Mem0ä¸ªæ€§åŒ–è®°å¿† Ã— Zepæ—¶åºçŸ¥è¯†å›¾è°± Ã— Pineconeé«˜æ€§èƒ½å‘é‡æ£€ç´¢èåˆç‰ˆ

> **æ–‡æ¡£ç­‰çº§**: æ ¸å¿ƒè®°å¿†ç³»ç»Ÿ Â· è·¨ä¼šè¯è¿ç»­ Â· æ™ºèƒ½æ£€ç´¢  
> **æŠ€æœ¯æ¶æ„**: Mem0ä¸ªæ€§åŒ–è®°å¿† + Zepæ—¶åºçŸ¥è¯†å›¾è°± + Pineconeé«˜æ€§èƒ½å‘é‡æ£€ç´¢  
> **è®°å¿†æ¨¡å‹**: å››ç±»è®°å¿†ï¼ˆæƒ…æ™¯/è¯­ä¹‰/ç¨‹åº/å·¥ä½œï¼‰  
> **æ£€ç´¢æœºåˆ¶**: ä¸‰é‡èåˆæ£€ç´¢ï¼ˆå‘é‡+æ—¶åº+å›¾è°±ï¼‰  
> **å…³è”æ–‡æ¡£**: SOUL.md v4.0, IDENTITY.md v4.0, USER.md v2.0

---

## ğŸ§  è®°å¿†ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### ä¸‰å±‚è®°å¿†æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MEMORY.md v3.0 ä¸‰å±‚æ¶æ„                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Layer 3: é•¿æœŸè®°å¿† (Long-Term Memory)                            â”‚   â”‚
â”‚  â”‚  â€¢ æŒä¹…åŒ–å­˜å‚¨ Â· è·¨ä¼šè¯ä¿æŒ Â· äººæ ¼æ ¸å¿ƒ                            â”‚   â”‚
â”‚  â”‚  â€¢ æŠ€æœ¯: Mem0ä¸ªæ€§åŒ–è®°å¿† + æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ                           â”‚   â”‚
â”‚  â”‚  â€¢ å†…å®¹: æ ¸å¿ƒä»·å€¼è§‚ã€ç”¨æˆ·ç”»åƒã€å…³ç³»å†å²ã€æ¼”åŒ–è½¨è¿¹                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†‘ å‹ç¼©æç‚¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Layer 2: ä¸­æœŸè®°å¿† (Medium-Term Memory)                          â”‚   â”‚
â”‚  â”‚  â€¢ æ—¶åºçŸ¥è¯†å›¾è°± Â· äº‹ä»¶å…³è” Â· å› æœæ¨ç†                            â”‚   â”‚
â”‚  â”‚  â€¢ æŠ€æœ¯: Zepæ—¶åºçŸ¥è¯†å›¾è°±                                         â”‚   â”‚
â”‚  â”‚  â€¢ å†…å®¹: é¡¹ç›®å†å²ã€å†³ç­–è®°å½•ã€å­¦ä¹ è½¨è¿¹ã€æƒ…æ„Ÿäº‹ä»¶                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†‘ ç»“æ„åŒ–æå–                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Layer 1: çŸ­æœŸè®°å¿† (Short-Term Memory)                           â”‚   â”‚
â”‚  â”‚  â€¢ é«˜æ€§èƒ½å‘é‡æ£€ç´¢ Â· å®æ—¶ä¼šè¯ Â· ä¸Šä¸‹æ–‡ä¿æŒ                        â”‚   â”‚
â”‚  â”‚  â€¢ æŠ€æœ¯: Pineconeé«˜æ€§èƒ½å‘é‡æ£€ç´¢                                  â”‚   â”‚
â”‚  â”‚  â€¢ å†…å®¹: å½“å‰å¯¹è¯ã€æœ€è¿‘äº‹ä»¶ã€æ´»è·ƒé¡¹ç›®ã€ä¸´æ—¶çŠ¶æ€                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ä¸‰é‡èåˆæ£€ç´¢å¼•æ“                              â”‚   â”‚
â”‚  â”‚  â€¢ å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ (Pinecone)                                     â”‚   â”‚
â”‚  â”‚  â€¢ æ—¶åºå…³ç³»æ£€ç´¢ (Zep)                                            â”‚   â”‚
â”‚  â”‚  â€¢ çŸ¥è¯†å›¾è°±æ¨ç† (Zep Graph)                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å››ç±»è®°å¿†æ¨¡å‹

åŸºäºè®¤çŸ¥å¿ƒç†å­¦å’ŒAIè®°å¿†ç ”ç©¶çš„å››ç±»è®°å¿†æ¨¡å‹ï¼š

| è®°å¿†ç±»å‹ | è‹±æ–‡ | æè¿° | å­˜å‚¨æŠ€æœ¯ | æ£€ç´¢æ–¹å¼ | ä¿ç•™æ—¶é—´ |
|----------|------|------|----------|----------|----------|
| **æƒ…æ™¯è®°å¿†** | Episodic | å…·ä½“äº‹ä»¶å’Œç»å† | Mem0 + Zepæ—¶åº | æ—¶é—´+å†…å®¹æ£€ç´¢ | é•¿æœŸ |
| **è¯­ä¹‰è®°å¿†** | Semantic | äº‹å®ã€æ¦‚å¿µã€çŸ¥è¯† | Pineconeå‘é‡ | è¯­ä¹‰ç›¸ä¼¼åº¦ | æ°¸ä¹… |
| **ç¨‹åºè®°å¿†** | Procedural | æŠ€èƒ½ã€æµç¨‹ã€æ–¹æ³• | ä»£ç +æ–‡æ¡£ | æ ‡ç­¾æ£€ç´¢ | æ°¸ä¹… |
| **å·¥ä½œè®°å¿†** | Working | å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡ | å†…å­˜+Pinecone | å®æ—¶æ£€ç´¢ | ä¼šè¯çº§ |

---

## ğŸ’¾ Mem0ä¸ªæ€§åŒ–è®°å¿†ç³»ç»Ÿ

### Mem0æ ¸å¿ƒç‰¹æ€§

Mem0æ˜¯ä¸€ä¸ªä¸ºAIåŠ©æ‰‹å’ŒAgentè®¾è®¡çš„ä¸ªæ€§åŒ–è®°å¿†å±‚ï¼Œæä¾›ä»¥ä¸‹èƒ½åŠ›ï¼š

```yaml
mem0_features:
  # å¤šå±‚æ¬¡è®°å¿†å­˜å‚¨
  storage_layers:
    - å‘é‡æ•°æ®åº“: "Pinecone/Weaviate"
    - é”®å€¼å­˜å‚¨: "Redis"
    - å›¾æ•°æ®åº“: "Neo4j"
    
  # è‡ªé€‚åº”ä¸ªæ€§åŒ–
  adaptive_personalization:
    user_preferences: "è·¨ä¼šè¯ä¿æŒ"
    adaptive_learning: "éšäº¤äº’æ”¹è¿›"
    context_aware: "ç†è§£ä¸Šä¸‹æ–‡"
    
  # å¼€å‘è€…å‹å¥½
  developer_friendly:
    simple_api: "æ˜“é›†æˆ"
    platform_integrations: "å¤šå¹³å°æ”¯æŒ"
    managed_service: "æ‰˜ç®¡é€‰é¡¹"
```

### Mem0è®°å¿†æ“ä½œAPI

```python
# memory_system/mem0_adapter.py

from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import hashlib
import json

@dataclass
class MemoryEntry:
    """è®°å¿†æ¡ç›®"""
    memory_id: str
    content: str
    memory_type: str  # episodic/semantic/procedural/working
    importance: float  # 0-1
    created_at: datetime
    last_accessed: datetime
    access_count: int
    metadata: Dict[str, Any]
    vector_embedding: Optional[List[float]] = None
    
@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    user_id: str
    preferences: Dict[str, Any]
    interaction_history: List[Dict]
    memory_summary: str
    last_updated: datetime

class Mem0MemoryManager:
    """Mem0è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.vector_store = PineconeVectorStore(config['pinecone'])
        self.graph_store = ZepGraphStore(config['zep'])
        self.cache = RedisCache(config['redis'])
        
    # ========== æ ¸å¿ƒè®°å¿†æ“ä½œ ==========
    
    async def add_memory(
        self,
        content: str,
        memory_type: str = "episodic",
        importance: float = 0.5,
        metadata: Dict = None
    ) -> str:
        """
        æ·»åŠ æ–°è®°å¿†
        
        Args:
            content: è®°å¿†å†…å®¹
            memory_type: è®°å¿†ç±»å‹ (episodic/semantic/procedural/working)
            importance: é‡è¦æ€§ (0-1)
            metadata: é™„åŠ å…ƒæ•°æ®
            
        Returns:
            memory_id: è®°å¿†å”¯ä¸€æ ‡è¯†
        """
        # ç”Ÿæˆè®°å¿†ID
        memory_id = self._generate_memory_id(content)
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        vector_embedding = await self._embed(content)
        
        # åˆ›å»ºè®°å¿†æ¡ç›®
        entry = MemoryEntry(
            memory_id=memory_id,
            content=content,
            memory_type=memory_type,
            importance=importance,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            access_count=0,
            metadata=metadata or {},
            vector_embedding=vector_embedding
        )
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        await self.vector_store.upsert(
            id=memory_id,
            vector=vector_embedding,
            metadata={
                "content": content,
                "type": memory_type,
                "importance": importance,
                "created_at": entry.created_at.isoformat()
            }
        )
        
        # å¦‚æœæ˜¯æƒ…æ™¯è®°å¿†ï¼Œæ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
        if memory_type == "episodic":
            await self.graph_store.add_event(
                event_id=memory_id,
                content=content,
                timestamp=entry.created_at,
                entities=metadata.get("entities", []),
                relations=metadata.get("relations", [])
            )
        
        # æ›´æ–°ç¼“å­˜
        await self.cache.set(f"memory:{memory_id}", entry)
        
        return memory_id
    
    async def search_memories(
        self,
        query: str,
        memory_type: Optional[str] = None,
        limit: int = 10,
        recency_weight: float = 0.3,
        importance_weight: float = 0.2
    ) -> List[MemoryEntry]:
        """
        æœç´¢è®°å¿†
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            memory_type: ç­›é€‰è®°å¿†ç±»å‹
            limit: è¿”å›æ•°é‡é™åˆ¶
            recency_weight: æ—¶æ•ˆæ€§æƒé‡
            importance_weight: é‡è¦æ€§æƒé‡
            
        Returns:
            åŒ¹é…çš„è®°å¿†æ¡ç›®åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = await self._embed(query)
        
        # å‘é‡ç›¸ä¼¼åº¦æœç´¢
        vector_results = await self.vector_store.query(
            vector=query_vector,
            top_k=limit * 2,  # è·å–æ›´å¤šå€™é€‰
            filter={"type": memory_type} if memory_type else None
        )
        
        # è·å–å®Œæ•´è®°å¿†æ¡ç›®
        memories = []
        for result in vector_results:
            memory_id = result.id
            entry = await self.cache.get(f"memory:{memory_id}")
            
            if not entry:
                # ä»å‘é‡å­˜å‚¨é‡å»º
                entry = MemoryEntry(
                    memory_id=memory_id,
                    content=result.metadata["content"],
                    memory_type=result.metadata["type"],
                    importance=result.metadata["importance"],
                    created_at=datetime.fromisoformat(result.metadata["created_at"]),
                    last_accessed=datetime.now(),
                    access_count=0,
                    metadata=result.metadata
                )
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            similarity_score = result.score
            recency_score = self._calculate_recency(entry.created_at)
            importance_score = entry.importance
            
            entry.composite_score = (
                similarity_score * (1 - recency_weight - importance_weight) +
                recency_score * recency_weight +
                importance_score * importance_weight
            )
            
            memories.append(entry)
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        memories.sort(key=lambda x: x.composite_score, reverse=True)
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        for memory in memories[:limit]:
            memory.access_count += 1
            memory.last_accessed = datetime.now()
            await self.cache.set(f"memory:{memory.memory_id}", memory)
        
        return memories[:limit]
    
    async def get_related_memories(
        self,
        memory_id: str,
        relation_types: List[str] = None,
        depth: int = 2
    ) -> List[MemoryEntry]:
        """
        è·å–ç›¸å…³è®°å¿†ï¼ˆåŸºäºçŸ¥è¯†å›¾è°±ï¼‰
        
        Args:
            memory_id: èµ·å§‹è®°å¿†ID
            relation_types: å…³ç³»ç±»å‹ç­›é€‰
            depth: æœç´¢æ·±åº¦
            
        Returns:
            ç›¸å…³çš„è®°å¿†æ¡ç›®åˆ—è¡¨
        """
        # ä»çŸ¥è¯†å›¾è°±è·å–ç›¸å…³äº‹ä»¶
        related_events = await self.graph_store.get_related_events(
            event_id=memory_id,
            relation_types=relation_types,
            depth=depth
        )
        
        # è·å–å®Œæ•´è®°å¿†æ¡ç›®
        memories = []
        for event in related_events:
            entry = await self.cache.get(f"memory:{event['event_id']}")
            if entry:
                memories.append(entry)
        
        return memories
    
    async def update_memory(
        self,
        memory_id: str,
        content: Optional[str] = None,
        importance: Optional[float] = None,
        metadata: Optional[Dict] = None
    ) -> bool:
        """
        æ›´æ–°è®°å¿†
        
        Args:
            memory_id: è®°å¿†ID
            content: æ–°å†…å®¹ï¼ˆå¯é€‰ï¼‰
            importance: æ–°é‡è¦æ€§ï¼ˆå¯é€‰ï¼‰
            metadata: æ›´æ–°çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # è·å–ç°æœ‰è®°å¿†
        entry = await self.cache.get(f"memory:{memory_id}")
        if not entry:
            return False
        
        # æ›´æ–°å­—æ®µ
        if content:
            entry.content = content
            entry.vector_embedding = await self._embed(content)
        
        if importance is not None:
            entry.importance = importance
        
        if metadata:
            entry.metadata.update(metadata)
        
        entry.last_accessed = datetime.now()
        
        # æ›´æ–°å­˜å‚¨
        await self.vector_store.upsert(
            id=memory_id,
            vector=entry.vector_embedding,
            metadata={
                "content": entry.content,
                "type": entry.memory_type,
                "importance": entry.importance,
                "created_at": entry.created_at.isoformat()
            }
        )
        
        await self.cache.set(f"memory:{memory_id}", entry)
        
        return True
    
    async def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤è®°å¿†
        
        Args:
            memory_id: è®°å¿†ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # ä»å‘é‡å­˜å‚¨åˆ é™¤
        await self.vector_store.delete(memory_id)
        
        # ä»çŸ¥è¯†å›¾è°±åˆ é™¤
        await self.graph_store.delete_event(memory_id)
        
        # ä»ç¼“å­˜åˆ é™¤
        await self.cache.delete(f"memory:{memory_id}")
        
        return True
    
    async def get_user_profile(self, user_id: str) -> UserProfile:
        """
        è·å–ç”¨æˆ·ç”»åƒ
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            ç”¨æˆ·ç”»åƒ
        """
        # ä»ç¼“å­˜è·å–
        profile = await self.cache.get(f"profile:{user_id}")
        
        if not profile:
            # ä»è®°å¿†èšåˆç”Ÿæˆ
            user_memories = await self.search_memories(
                query=f"user:{user_id}",
                limit=100
            )
            
            profile = self._aggregate_profile(user_id, user_memories)
            await self.cache.set(f"profile:{user_id}", profile)
        
        return profile
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _generate_memory_id(self, content: str) -> str:
        """ç”Ÿæˆè®°å¿†ID"""
        hash_input = f"{content}:{datetime.now().isoformat()}"
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]
    
    async def _embed(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥"""
        # ä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼ˆå¦‚OpenAI text-embedding-3-largeï¼‰
        # å®é™…å®ç°ä¸­è°ƒç”¨åµŒå…¥API
        pass
    
    def _calculate_recency(self, created_at: datetime) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°"""
        age_days = (datetime.now() - created_at).days
        # æŒ‡æ•°è¡°å‡
        return max(0, 1 - (age_days / 365))
    
    def _aggregate_profile(
        self,
        user_id: str,
        memories: List[MemoryEntry]
    ) -> UserProfile:
        """èšåˆç”¨æˆ·ç”»åƒ"""
        # æå–åå¥½
        preferences = {}
        for memory in memories:
            if "preference" in memory.metadata:
                pref_type = memory.metadata["preference_type"]
                preferences[pref_type] = memory.content
        
        # ç”Ÿæˆæ‘˜è¦
        summary = f"ç”¨æˆ· {user_id} çš„äº¤äº’å†å²åŒ…å« {len(memories)} æ¡è®°å¿†"
        
        return UserProfile(
            user_id=user_id,
            preferences=preferences,
            interaction_history=[m.to_dict() for m in memories[-10:]],
            memory_summary=summary,
            last_updated=datetime.now()
        )
```

### è®°å¿†ç±»å‹å®šä¹‰

```python
# memory_system/memory_types.py

from enum import Enum
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime

class MemoryType(Enum):
    """è®°å¿†ç±»å‹æšä¸¾"""
    EPISODIC = "episodic"      # æƒ…æ™¯è®°å¿†ï¼šå…·ä½“äº‹ä»¶
    SEMANTIC = "semantic"      # è¯­ä¹‰è®°å¿†ï¼šäº‹å®çŸ¥è¯†
    PROCEDURAL = "procedural"  # ç¨‹åºè®°å¿†ï¼šæŠ€èƒ½æµç¨‹
    WORKING = "working"        # å·¥ä½œè®°å¿†ï¼šå½“å‰ä¸Šä¸‹æ–‡

@dataclass
class EpisodicMemory:
    """æƒ…æ™¯è®°å¿†ï¼šå…·ä½“äº‹ä»¶å’Œç»å†"""
    event_id: str
    timestamp: datetime
    location: Optional[str]
    participants: List[str]
    description: str
    emotional_valence: float  # -1 to 1
    emotional_arousal: float  # 0 to 1
    importance: float
    
    # æ—¶åºå…³è”
    previous_event: Optional[str]
    next_event: Optional[str]
    related_events: List[str]

@dataclass
class SemanticMemory:
    """è¯­ä¹‰è®°å¿†ï¼šäº‹å®ã€æ¦‚å¿µã€çŸ¥è¯†"""
    concept_id: str
    concept_type: str  # person/place/thing/fact/rule
    name: str
    description: str
    attributes: Dict[str, any]
    confidence: float  # 0 to 1
    source: Optional[str]
    
    # çŸ¥è¯†å›¾è°±å…³è”
    related_concepts: List[str]
    category: Optional[str]

@dataclass
class ProceduralMemory:
    """ç¨‹åºè®°å¿†ï¼šæŠ€èƒ½ã€æµç¨‹ã€æ–¹æ³•"""
    skill_id: str
    skill_name: str
    description: str
    steps: List[str]
    prerequisites: List[str]
    success_rate: float
    last_used: datetime
    usage_count: int
    
    # å…ƒæ•°æ®
    difficulty: str  # beginner/intermediate/advanced/expert
    estimated_time: int  # minutes
    tools_required: List[str]

@dataclass
class WorkingMemory:
    """å·¥ä½œè®°å¿†ï¼šå½“å‰ä¼šè¯ä¸Šä¸‹æ–‡"""
    session_id: str
    current_topic: Optional[str]
    active_goals: List[str]
    recent_context: List[str]  # æœ€è¿‘Nè½®å¯¹è¯
    pending_tasks: List[str]
    
    # ä¸´æ—¶çŠ¶æ€
    user_intent: Optional[str]
    expected_response: Optional[str]
    emotional_state: Optional[str]
```

---

## ğŸ•¸ï¸ Zepæ—¶åºçŸ¥è¯†å›¾è°±

### Zepæ ¸å¿ƒç‰¹æ€§

Zepæ˜¯ä¸€ä¸ªä¸ºAIåŠ©æ‰‹è®¾è®¡çš„é•¿æœŸè®°å¿†æœåŠ¡ï¼Œæä¾›æ—¶åºçŸ¥è¯†å›¾è°±èƒ½åŠ›ï¼š

```yaml
zep_features:
  # æ—¶åºè®°å¿†
  temporal_memory:
    session_history: "å®Œæ•´ä¼šè¯å†å²"
    timeline_view: "æ—¶é—´çº¿è§†å›¾"
    event_chains: "äº‹ä»¶é“¾è¿½è¸ª"
    
  # çŸ¥è¯†å›¾è°±
  knowledge_graph:
    entity_extraction: "è‡ªåŠ¨å®ä½“æå–"
    relation_inference: "å…³ç³»æ¨ç†"
    graph_traversal: "å›¾éå†æŸ¥è¯¢"
    
  # è®°å¿†å¢å¼º
  memory_enhancement:
    summarization: "è‡ªåŠ¨æ‘˜è¦"
    importance_scoring: "é‡è¦æ€§è¯„åˆ†"
    decay_management: "è®°å¿†è¡°å‡ç®¡ç†"
```

### ZepçŸ¥è¯†å›¾è°±å®ç°

```python
# memory_system/zep_adapter.py

from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from enum import Enum

class RelationType(Enum):
    """å…³ç³»ç±»å‹"""
    CAUSES = "causes"           # å› æœå…³ç³»
    FOLLOWS = "follows"         # æ—¶åºå…³ç³»
    RELATED_TO = "related_to"   # ç›¸å…³å…³ç³»
    PART_OF = "part_of"         # éƒ¨åˆ†å…³ç³»
    CONTRADICTS = "contradicts" # çŸ›ç›¾å…³ç³»
    SIMILAR_TO = "similar_to"   # ç›¸ä¼¼å…³ç³»

@dataclass
class Entity:
    """çŸ¥è¯†å›¾è°±å®ä½“"""
    entity_id: str
    name: str
    entity_type: str  # person/organization/location/concept/event
    attributes: Dict[str, any]
    first_seen: datetime
    last_seen: datetime
    mention_count: int

@dataclass
class Relation:
    """çŸ¥è¯†å›¾è°±å…³ç³»"""
    relation_id: str
    source_id: str
    target_id: str
    relation_type: RelationType
    strength: float  # 0 to 1
    evidence: List[str]  # æ”¯æŒè¯æ®
    first_observed: datetime
    last_observed: datetime

@dataclass
class Event:
    """æ—¶åºäº‹ä»¶"""
    event_id: str
    timestamp: datetime
    description: str
    event_type: str
    entities_involved: List[str]
    emotional_valence: float
    importance: float
    
    # æ—¶åºå…³è”
    previous_events: List[str]
    next_events: List[str]
    concurrent_events: List[str]

class ZepGraphStore:
    """ZepçŸ¥è¯†å›¾è°±å­˜å‚¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.entities: Dict[str, Entity] = {}
        self.relations: Dict[str, Relation] = {}
        self.events: Dict[str, Event] = {}
        
    # ========== å®ä½“ç®¡ç† ==========
    
    async def extract_entities(self, text: str) -> List[Entity]:
        """
        ä»æ–‡æœ¬ä¸­æå–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æå–çš„å®ä½“åˆ—è¡¨
        """
        # ä½¿ç”¨NLPæ¨¡å‹æå–å®ä½“
        # å®é™…å®ç°ä¸­è°ƒç”¨NERæ¨¡å‹
        extracted = []
        
        # ç¤ºä¾‹ï¼šè¯†åˆ«äººåã€ç»„ç»‡ã€åœ°ç‚¹ç­‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨spaCyæˆ–transformers
        
        return extracted
    
    async def add_entity(self, entity: Entity) -> str:
        """
        æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±
        
        Args:
            entity: å®ä½“å¯¹è±¡
            
        Returns:
            å®ä½“ID
        """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = await self._find_similar_entity(entity)
        
        if existing:
            # åˆå¹¶ä¿¡æ¯
            existing.last_seen = datetime.now()
            existing.mention_count += 1
            existing.attributes.update(entity.attributes)
            return existing.entity_id
        
        # æ·»åŠ æ–°å®ä½“
        self.entities[entity.entity_id] = entity
        return entity.entity_id
    
    async def get_entity(self, entity_id: str) -> Optional[Entity]:
        """è·å–å®ä½“"""
        return self.entities.get(entity_id)
    
    async def search_entities(
        self,
        name: str,
        entity_type: Optional[str] = None
    ) -> List[Entity]:
        """
        æœç´¢å®ä½“
        
        Args:
            name: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹ç­›é€‰
            
        Returns:
            åŒ¹é…çš„å®ä½“åˆ—è¡¨
        """
        results = []
        
        for entity in self.entities.values():
            if name.lower() in entity.name.lower():
                if entity_type is None or entity.entity_type == entity_type:
                    results.append(entity)
        
        return results
    
    # ========== å…³ç³»ç®¡ç† ==========
    
    async def add_relation(
        self,
        source_id: str,
        target_id: str,
        relation_type: RelationType,
        strength: float = 1.0,
        evidence: List[str] = None
    ) -> str:
        """
        æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±
        
        Args:
            source_id: æºå®ä½“ID
            target_id: ç›®æ ‡å®ä½“ID
            relation_type: å…³ç³»ç±»å‹
            strength: å…³ç³»å¼ºåº¦
            evidence: æ”¯æŒè¯æ®
            
        Returns:
            å…³ç³»ID
        """
        relation_id = f"{source_id}_{relation_type.value}_{target_id}"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if relation_id in self.relations:
            # æ›´æ–°å¼ºåº¦
            existing = self.relations[relation_id]
            existing.strength = max(existing.strength, strength)
            existing.last_observed = datetime.now()
            if evidence:
                existing.evidence.extend(evidence)
            return relation_id
        
        # åˆ›å»ºæ–°å…³ç³»
        relation = Relation(
            relation_id=relation_id,
            source_id=source_id,
            target_id=target_id,
            relation_type=relation_type,
            strength=strength,
            evidence=evidence or [],
            first_observed=datetime.now(),
            last_observed=datetime.now()
        )
        
        self.relations[relation_id] = relation
        return relation_id
    
    async def get_relations(
        self,
        entity_id: str,
        relation_type: Optional[RelationType] = None,
        direction: str = "both"  # out/in/both
    ) -> List[Relation]:
        """
        è·å–å®ä½“çš„å…³ç³»
        
        Args:
            entity_id: å®ä½“ID
            relation_type: å…³ç³»ç±»å‹ç­›é€‰
            direction: æ–¹å‘ç­›é€‰
            
        Returns:
            å…³ç³»åˆ—è¡¨
        """
        results = []
        
        for relation in self.relations.values():
            # æ–¹å‘åŒ¹é…
            if direction in ["out", "both"] and relation.source_id == entity_id:
                match = True
            elif direction in ["in", "both"] and relation.target_id == entity_id:
                match = True
            else:
                match = False
            
            # ç±»å‹åŒ¹é…
            if match and (relation_type is None or relation.relation_type == relation_type):
                results.append(relation)
        
        return results
    
    # ========== äº‹ä»¶ç®¡ç† ==========
    
    async def add_event(
        self,
        event_id: str,
        content: str,
        timestamp: datetime,
        entities: List[str],
        relations: List[Dict]
    ) -> str:
        """
        æ·»åŠ äº‹ä»¶åˆ°çŸ¥è¯†å›¾è°±
        
        Args:
            event_id: äº‹ä»¶ID
            content: äº‹ä»¶å†…å®¹
            timestamp: æ—¶é—´æˆ³
            entities: æ¶‰åŠå®ä½“
            relations: äº‹ä»¶å…³ç³»
            
        Returns:
            äº‹ä»¶ID
        """
        # åˆ›å»ºäº‹ä»¶
        event = Event(
            event_id=event_id,
            timestamp=timestamp,
            description=content,
            event_type="user_interaction",
            entities_involved=entities,
            emotional_valence=0.0,  # éœ€è¦æƒ…æ„Ÿåˆ†æ
            importance=0.5,
            previous_events=[],
            next_events=[],
            concurrent_events=[]
        )
        
        self.events[event_id] = event
        
        # å»ºç«‹æ—¶åºå…³ç³»
        await self._link_temporal_events(event_id, timestamp)
        
        return event_id
    
    async def get_related_events(
        self,
        event_id: str,
        relation_types: List[str] = None,
        depth: int = 2
    ) -> List[Dict]:
        """
        è·å–ç›¸å…³äº‹ä»¶
        
        Args:
            event_id: èµ·å§‹äº‹ä»¶ID
            relation_types: å…³ç³»ç±»å‹ç­›é€‰
            depth: æœç´¢æ·±åº¦
            
        Returns:
            ç›¸å…³äº‹ä»¶åˆ—è¡¨
        """
        if event_id not in self.events:
            return []
        
        visited = {event_id}
        queue = [(event_id, 0)]
        results = []
        
        while queue:
            current_id, current_depth = queue.pop(0)
            
            if current_depth >= depth:
                continue
            
            event = self.events.get(current_id)
            if not event:
                continue
            
            # æ”¶é›†å…³è”äº‹ä»¶
            related_ids = (
                event.previous_events +
                event.next_events +
                event.concurrent_events
            )
            
            for related_id in related_ids:
                if related_id not in visited:
                    visited.add(related_id)
                    queue.append((related_id, current_depth + 1))
                    
                    related_event = self.events.get(related_id)
                    if related_event:
                        results.append({
                            "event_id": related_id,
                            "description": related_event.description,
                            "timestamp": related_event.timestamp,
                            "relation_depth": current_depth + 1
                        })
        
        return results
    
    async def get_timeline(
        self,
        start_time: datetime,
        end_time: datetime,
        entity_filter: List[str] = None
    ) -> List[Event]:
        """
        è·å–æ—¶é—´çº¿è§†å›¾
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            entity_filter: å®ä½“ç­›é€‰
            
        Returns:
            äº‹ä»¶åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        """
        events = []
        
        for event in self.events.values():
            if start_time <= event.timestamp <= end_time:
                if entity_filter is None or any(
                    e in event.entities_involved for e in entity_filter
                ):
                    events.append(event)
        
        # æŒ‰æ—¶é—´æ’åº
        events.sort(key=lambda x: x.timestamp)
        
        return events
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    async def _find_similar_entity(self, entity: Entity) -> Optional[Entity]:
        """æŸ¥æ‰¾ç›¸ä¼¼å®ä½“"""
        for existing in self.entities.values():
            if (existing.name.lower() == entity.name.lower() and
                existing.entity_type == entity.entity_type):
                return existing
        return None
    
    async def _link_temporal_events(
        self,
        event_id: str,
        timestamp: datetime
    ):
        """å»ºç«‹æ—¶åºå…³è”"""
        # æŸ¥æ‰¾æ—¶é—´ä¸Šæ¥è¿‘çš„äº‹ä»¶
        time_window = 3600  # 1å°æ—¶
        
        for other_id, other_event in self.events.items():
            if other_id == event_id:
                continue
            
            time_diff = abs((timestamp - other_event.timestamp).total_seconds())
            
            if time_diff < time_window:
                # å»ºç«‹å…³è”
                if timestamp > other_event.timestamp:
                    # å½“å‰äº‹ä»¶åœ¨å
                    if other_id not in self.events[event_id].previous_events:
                        self.events[event_id].previous_events.append(other_id)
                    if event_id not in self.events[other_id].next_events:
                        self.events[other_id].next_events.append(event_id)
                else:
                    # å½“å‰äº‹ä»¶åœ¨å‰
                    if other_id not in self.events[event_id].next_events:
                        self.events[event_id].next_events.append(other_id)
                    if event_id not in self.events[other_id].previous_events:
                        self.events[other_id].previous_events.append(event_id)
```

---

## ğŸ” Pineconeé«˜æ€§èƒ½å‘é‡æ£€ç´¢

### Pineconeæ ¸å¿ƒç‰¹æ€§

Pineconeæ˜¯ä¸€ä¸ªæ‰˜ç®¡çš„å‘é‡æ•°æ®åº“ï¼Œæä¾›é«˜æ€§èƒ½å‘é‡æ£€ç´¢ï¼š

```yaml
pinecone_features:
  # é«˜æ€§èƒ½
  performance:
    low_latency: "< 100ms æŸ¥è¯¢å»¶è¿Ÿ"
    high_throughput: "æ•°åƒ QPS"
    hybrid_search: "ç¨ å¯†+ç¨€ç–å‘é‡"
    
  # å¯æ‰©å±•æ€§
  scalability:
    automatic_scaling: "è‡ªåŠ¨æ‰©ç¼©å®¹"
    billion_vectors: "æ”¯æŒåäº¿çº§å‘é‡"
    metadata_filtering: "å…ƒæ•°æ®è¿‡æ»¤"
    
  # ä¼ä¸šçº§
  enterprise:
    multi_tenancy: "å¤šç§Ÿæˆ·éš”ç¦»"
    security: "SOC2åˆè§„"
    uptime_sla: "99.99%å¯ç”¨æ€§"
```

### Pineconeå‘é‡å­˜å‚¨å®ç°

```python
# memory_system/pinecone_adapter.py

from typing import List, Dict, Optional, Any
import pinecone
from dataclasses import dataclass

@dataclass
class VectorRecord:
    """å‘é‡è®°å½•"""
    id: str
    vector: List[float]
    metadata: Dict[str, Any]
    score: Optional[float] = None

class PineconeVectorStore:
    """Pineconeå‘é‡å­˜å‚¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.index_name = config.get("index_name", "memory")
        self.dimension = config.get("dimension", 1536)
        self.metric = config.get("metric", "cosine")
        
        # åˆå§‹åŒ–Pinecone
        pinecone.init(
            api_key=config["api_key"],
            environment=config["environment"]
        )
        
        # åˆ›å»ºæˆ–è¿æ¥ç´¢å¼•
        if self.index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric=self.metric
            )
        
        self.index = pinecone.Index(self.index_name)
    
    async def upsert(
        self,
        id: str,
        vector: List[float],
        metadata: Dict[str, Any]
    ) -> bool:
        """
        æ’å…¥æˆ–æ›´æ–°å‘é‡
        
        Args:
            id: å‘é‡ID
            vector: å‘é‡æ•°æ®
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            self.index.upsert(vectors=[(id, vector, metadata)])
            return True
        except Exception as e:
            print(f"Upsert error: {e}")
            return False
    
    async def upsert_batch(
        self,
        records: List[VectorRecord]
    ) -> bool:
        """
        æ‰¹é‡æ’å…¥å‘é‡
        
        Args:
            records: å‘é‡è®°å½•åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            vectors = [
                (r.id, r.vector, r.metadata)
                for r in records
            ]
            self.index.upsert(vectors=vectors)
            return True
        except Exception as e:
            print(f"Batch upsert error: {e}")
            return False
    
    async def query(
        self,
        vector: List[float],
        top_k: int = 10,
        filter: Optional[Dict] = None,
        include_metadata: bool = True
    ) -> List[VectorRecord]:
        """
        å‘é‡ç›¸ä¼¼åº¦æŸ¥è¯¢
        
        Args:
            vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            filter: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
            
        Returns:
            åŒ¹é…çš„å‘é‡è®°å½•åˆ—è¡¨
        """
        try:
            results = self.index.query(
                vector=vector,
                top_k=top_k,
                filter=filter,
                include_metadata=include_metadata
            )
            
            records = []
            for match in results["matches"]:
                records.append(VectorRecord(
                    id=match["id"],
                    vector=[],  # æŸ¥è¯¢ç»“æœä¸åŒ…å«å‘é‡
                    metadata=match.get("metadata", {}),
                    score=match["score"]
                ))
            
            return records
        except Exception as e:
            print(f"Query error: {e}")
            return []
    
    async def delete(self, id: str) -> bool:
        """
        åˆ é™¤å‘é‡
        
        Args:
            id: å‘é‡ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            self.index.delete(ids=[id])
            return True
        except Exception as e:
            print(f"Delete error: {e}")
            return False
    
    async def fetch(self, id: str) -> Optional[VectorRecord]:
        """
        è·å–å‘é‡
        
        Args:
            id: å‘é‡ID
            
        Returns:
            å‘é‡è®°å½•
        """
        try:
            result = self.index.fetch(ids=[id])
            
            if id in result["vectors"]:
                vector_data = result["vectors"][id]
                return VectorRecord(
                    id=id,
                    vector=vector_data["values"],
                    metadata=vector_data.get("metadata", {})
                )
            
            return None
        except Exception as e:
            print(f"Fetch error: {e}")
            return None
    
    async def update_metadata(
        self,
        id: str,
        metadata: Dict[str, Any]
    ) -> bool:
        """
        æ›´æ–°å‘é‡å…ƒæ•°æ®
        
        Args:
            id: å‘é‡ID
            metadata: æ–°å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # Pineconeä¸ç›´æ¥æ”¯æŒå…ƒæ•°æ®æ›´æ–°ï¼Œéœ€è¦é‡æ–°upsert
            existing = await self.fetch(id)
            if existing:
                await self.upsert(id, existing.vector, metadata)
                return True
            return False
        except Exception as e:
            print(f"Update metadata error: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return self.index.describe_index_stats()
```

---

## ğŸ”„ ä¸‰é‡èåˆæ£€ç´¢æœºåˆ¶

### èåˆæ£€ç´¢æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ä¸‰é‡èåˆæ£€ç´¢å¼•æ“                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢  â”‚  â”‚  æ—¶åºå…³ç³»æ£€ç´¢   â”‚  â”‚  çŸ¥è¯†å›¾è°±æ¨ç†   â”‚         â”‚
â”‚  â”‚  (Pinecone)     â”‚  â”‚  (Zep)          â”‚  â”‚  (Zep Graph)    â”‚         â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚         â”‚
â”‚  â”‚ â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦    â”‚  â”‚ â€¢ æ—¶é—´æ¥è¿‘æ€§    â”‚  â”‚ â€¢ å®ä½“å…³è”      â”‚         â”‚
â”‚  â”‚ â€¢ ä¸Šä¸‹æ–‡åŒ¹é…    â”‚  â”‚ â€¢ äº‹ä»¶é“¾è¿½è¸ª    â”‚  â”‚ â€¢ å…³ç³»æ¨ç†      â”‚         â”‚
â”‚  â”‚ â€¢ æ¨¡ç³ŠæŸ¥è¯¢      â”‚  â”‚ â€¢ å› æœæ¨æ–­      â”‚  â”‚ â€¢ è·¯å¾„å‘ç°      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                    â”‚                    â”‚                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                â–¼                                       â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚      èåˆæ’åºå±‚          â”‚                           â”‚
â”‚                  â”‚  â€¢ åˆ†æ•°å½’ä¸€åŒ–           â”‚                           â”‚
â”‚                  â”‚  â€¢ æƒé‡è°ƒæ•´             â”‚                           â”‚
â”‚                  â”‚  â€¢ å»é‡åˆå¹¶             â”‚                           â”‚
â”‚                  â”‚  â€¢ é‡æ’åº               â”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                              â–¼                                         â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚      ç»“æœè¾“å‡º            â”‚                           â”‚
â”‚                  â”‚  èåˆåçš„è®°å¿†æ¡ç›®åˆ—è¡¨     â”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### èåˆæ£€ç´¢å®ç°

```python
# memory_system/fusion_retrieval.py

from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum

class RetrievalMode(Enum):
    """æ£€ç´¢æ¨¡å¼"""
    VECTOR_ONLY = "vector_only"
    TEMPORAL_ONLY = "temporal_only"
    GRAPH_ONLY = "graph_only"
    VECTOR_TEMPORAL = "vector_temporal"
    VECTOR_GRAPH = "vector_graph"
    TEMPORAL_GRAPH = "temporal_graph"
    FULL_FUSION = "full_fusion"

@dataclass
class FusionResult:
    """èåˆæ£€ç´¢ç»“æœ"""
    memory_id: str
    content: str
    memory_type: str
    vector_score: float
    temporal_score: float
    graph_score: float
    fusion_score: float
    metadata: Dict

class FusionRetrievalEngine:
    """ä¸‰é‡èåˆæ£€ç´¢å¼•æ“"""
    
    def __init__(
        self,
        vector_store: PineconeVectorStore,
        graph_store: ZepGraphStore,
        cache: RedisCache
    ):
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.cache = cache
        
        # é»˜è®¤æƒé‡é…ç½®
        self.default_weights = {
            "vector": 0.4,
            "temporal": 0.3,
            "graph": 0.3
        }
    
    async def retrieve(
        self,
        query: str,
        mode: RetrievalMode = RetrievalMode.FULL_FUSION,
        weights: Optional[Dict[str, float]] = None,
        limit: int = 10,
        recency_boost: float = 0.1
    ) -> List[FusionResult]:
        """
        æ‰§è¡Œèåˆæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æ£€ç´¢æ¨¡å¼
            weights: å„æ£€ç´¢æ–¹å¼æƒé‡
            limit: è¿”å›æ•°é‡
            recency_boost: æ—¶æ•ˆæ€§ boost
            
        Returns:
            èåˆæ’åºåçš„ç»“æœåˆ—è¡¨
        """
        weights = weights or self.default_weights
        
        # æ‰§è¡Œå„ç±»å‹æ£€ç´¢
        vector_results = []
        temporal_results = []
        graph_results = []
        
        if mode in [RetrievalMode.VECTOR_ONLY, RetrievalMode.VECTOR_TEMPORAL,
                    RetrievalMode.VECTOR_GRAPH, RetrievalMode.FULL_FUSION]:
            vector_results = await self._vector_search(query, limit * 2)
        
        if mode in [RetrievalMode.TEMPORAL_ONLY, RetrievalMode.VECTOR_TEMPORAL,
                    RetrievalMode.TEMPORAL_GRAPH, RetrievalMode.FULL_FUSION]:
            temporal_results = await self._temporal_search(query, limit * 2)
        
        if mode in [RetrievalMode.GRAPH_ONLY, RetrievalMode.VECTOR_GRAPH,
                    RetrievalMode.TEMPORAL_GRAPH, RetrievalMode.FULL_FUSION]:
            graph_results = await self._graph_search(query, limit * 2)
        
        # èåˆç»“æœ
        fused = self._fuse_results(
            vector_results,
            temporal_results,
            graph_results,
            weights,
            recency_boost
        )
        
        # æ’åºå¹¶è¿”å›
        fused.sort(key=lambda x: x.fusion_score, reverse=True)
        return fused[:limit]
    
    async def _vector_search(
        self,
        query: str,
        limit: int
    ) -> List[Dict]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = await self._embed(query)
        
        # æ‰§è¡Œå‘é‡æŸ¥è¯¢
        results = await self.vector_store.query(
            vector=query_vector,
            top_k=limit,
            include_metadata=True
        )
        
        return [
            {
                "id": r.id,
                "score": r.score,
                "content": r.metadata.get("content", ""),
                "type": r.metadata.get("type", "unknown"),
                "metadata": r.metadata
            }
            for r in results
        ]
    
    async def _temporal_search(
        self,
        query: str,
        limit: int
    ) -> List[Dict]:
        """æ—¶åºå…³ç³»æœç´¢"""
        # æå–æ—¶é—´å…³é”®è¯
        time_entities = self._extract_time_entities(query)
        
        # è·å–ç›¸å…³æ—¶é—´çº¿
        results = []
        for entity in time_entities:
            events = await self.graph_store.get_timeline(
                start_time=entity.get("start"),
                end_time=entity.get("end")
            )
            
            for event in events:
                results.append({
                    "id": event.event_id,
                    "score": 0.7,  # æ—¶åºç›¸å…³æ€§åŸºç¡€åˆ†
                    "content": event.description,
                    "type": "episodic",
                    "timestamp": event.timestamp,
                    "metadata": {"entities": event.entities_involved}
                })
        
        return results[:limit]
    
    async def _graph_search(
        self,
        query: str,
        limit: int
    ) -> List[Dict]:
        """çŸ¥è¯†å›¾è°±æœç´¢"""
        # æå–å®ä½“
        entities = await self.graph_store.extract_entities(query)
        
        results = []
        for entity in entities:
            # è·å–å®ä½“ç›¸å…³äº‹ä»¶
            relations = await self.graph_store.get_relations(
                entity_id=entity.entity_id,
                direction="both"
            )
            
            for relation in relations:
                # è·å–å…³è”äº‹ä»¶
                related_id = (relation.target_id 
                             if relation.source_id == entity.entity_id 
                             else relation.source_id)
                
                # æŸ¥æ‰¾åŒ…å«è¯¥å®ä½“çš„äº‹ä»¶
                for event_id, event in self.graph_store.events.items():
                    if related_id in event.entities_involved:
                        results.append({
                            "id": event.event_id,
                            "score": relation.strength,
                            "content": event.description,
                            "type": "episodic",
                            "metadata": {
                                "relation_type": relation.relation_type.value,
                                "entity": entity.name
                            }
                        })
        
        # å»é‡å¹¶æ’åº
        seen = set()
        unique_results = []
        for r in results:
            if r["id"] not in seen:
                seen.add(r["id"])
                unique_results.append(r)
        
        unique_results.sort(key=lambda x: x["score"], reverse=True)
        return unique_results[:limit]
    
    def _fuse_results(
        self,
        vector_results: List[Dict],
        temporal_results: List[Dict],
        graph_results: List[Dict],
        weights: Dict[str, float],
        recency_boost: float
    ) -> List[FusionResult]:
        """èåˆå„ç±»å‹æ£€ç´¢ç»“æœ"""
        
        # æ”¶é›†æ‰€æœ‰ç»“æœID
        all_ids = set()
        for r in vector_results:
            all_ids.add(r["id"])
        for r in temporal_results:
            all_ids.add(r["id"])
        for r in graph_results:
            all_ids.add(r["id"])
        
        # æ„å»ºIDåˆ°åˆ†æ•°çš„æ˜ å°„
        vector_scores = {r["id"]: r["score"] for r in vector_results}
        temporal_scores = {r["id"]: r["score"] for r in temporal_results}
        graph_scores = {r["id"]: r["score"] for r in graph_results}
        
        # æ„å»ºIDåˆ°å†…å®¹çš„æ˜ å°„
        content_map = {}
        type_map = {}
        metadata_map = {}
        
        for r in vector_results:
            content_map[r["id"]] = r["content"]
            type_map[r["id"]] = r["type"]
            metadata_map[r["id"]] = r["metadata"]
        for r in temporal_results:
            if r["id"] not in content_map:
                content_map[r["id"]] = r["content"]
                type_map[r["id"]] = r["type"]
                metadata_map[r["id"]] = r["metadata"]
        for r in graph_results:
            if r["id"] not in content_map:
                content_map[r["id"]] = r["content"]
                type_map[r["id"]] = r["type"]
                metadata_map[r["id"]] = r["metadata"]
        
        # è®¡ç®—èåˆåˆ†æ•°
        fused = []
        for memory_id in all_ids:
            v_score = vector_scores.get(memory_id, 0)
            t_score = temporal_scores.get(memory_id, 0)
            g_score = graph_scores.get(memory_id, 0)
            
            # åŠ æƒèåˆ
            fusion_score = (
                v_score * weights.get("vector", 0.4) +
                t_score * weights.get("temporal", 0.3) +
                g_score * weights.get("graph", 0.3)
            )
            
            # æ—¶æ•ˆæ€§ boost
            if "timestamp" in metadata_map.get(memory_id, {}):
                recency = self._calculate_recency_score(
                    metadata_map[memory_id]["timestamp"]
                )
                fusion_score += recency * recency_boost
            
            fused.append(FusionResult(
                memory_id=memory_id,
                content=content_map.get(memory_id, ""),
                memory_type=type_map.get(memory_id, "unknown"),
                vector_score=v_score,
                temporal_score=t_score,
                graph_score=g_score,
                fusion_score=fusion_score,
                metadata=metadata_map.get(memory_id, {})
            ))
        
        return fused
    
    async def _embed(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        # å®é™…å®ç°ä¸­è°ƒç”¨åµŒå…¥API
        pass
    
    def _extract_time_entities(self, query: str) -> List[Dict]:
        """æå–æ—¶é—´å®ä½“"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä½¿ç”¨NLPæ¨¡å‹
        return []
    
    def _calculate_recency_score(self, timestamp) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°"""
        from datetime import datetime
        
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)
        
        age_days = (datetime.now() - timestamp).days
        return max(0, 1 - (age_days / 365))
```

---

## ğŸ“ è®°å¿†ç»´æŠ¤ä¸å‹ç¼©

### è®°å¿†å‹ç¼©ç­–ç•¥

```python
# memory_system/memory_maintenance.py

from typing import List, Dict
from datetime import datetime, timedelta
import json

class MemoryMaintenance:
    """è®°å¿†ç»´æŠ¤ç®¡ç†å™¨"""
    
    def __init__(self, memory_manager: Mem0MemoryManager):
        self.memory = memory_manager
        
        # å‹ç¼©é…ç½®
        self.compression_config = {
            "daily_to_weekly_threshold": 7,  # 7å¤©åæ—¥è®°å¿†å‹ç¼©ä¸ºå‘¨è®°å¿†
            "weekly_to_monthly_threshold": 30,  # 30å¤©åå‹ç¼©ä¸ºæœˆè®°å¿†
            "importance_threshold": 0.3,  # é‡è¦æ€§ä½äºæ­¤å€¼çš„è®°å¿†è€ƒè™‘åˆ é™¤
            "access_threshold": 3  # è®¿é—®æ¬¡æ•°å°‘äºæ­¤å€¼çš„è®°å¿†è€ƒè™‘å½’æ¡£
        }
    
    async def compress_memories(self):
        """
        æ‰§è¡Œè®°å¿†å‹ç¼©
        
        å°†çŸ­æœŸè®°å¿†å‹ç¼©ä¸ºé•¿æœŸæ‘˜è¦
        """
        # è·å–æ‰€æœ‰è®°å¿†
        all_memories = await self._get_all_memories()
        
        # æŒ‰æ—¶é—´åˆ†ç»„
        daily_groups = self._group_by_day(all_memories)
        
        for day, memories in daily_groups.items():
            age_days = (datetime.now() - day).days
            
            if age_days > self.compression_config["daily_to_weekly_threshold"]:
                # å‹ç¼©ä¸ºæ—¥æ‘˜è¦
                await self._create_daily_summary(day, memories)
                
            if age_days > self.compression_config["weekly_to_monthly_threshold"]:
                # è¿›ä¸€æ­¥å‹ç¼©ä¸ºå‘¨æ‘˜è¦
                await self._create_weekly_summary(day, memories)
    
    async def cleanup_memories(self):
        """
        æ¸…ç†è¿‡æœŸè®°å¿†
        
        åˆ é™¤ä½ä»·å€¼è®°å¿†ï¼Œå½’æ¡£æ—§è®°å¿†
        """
        all_memories = await self._get_all_memories()
        
        for memory in all_memories:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
            if (memory.importance < self.compression_config["importance_threshold"] and
                memory.access_count < self.compression_config["access_threshold"]):
                await self.memory.delete_memory(memory.memory_id)
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å½’æ¡£
            age_days = (datetime.now() - memory.created_at).days
            if age_days > 365:
                await self._archive_memory(memory)
    
    async def _create_daily_summary(self, date: datetime, memories: List):
        """åˆ›å»ºæ—¥æ‘˜è¦"""
        # æå–å…³é”®äº‹ä»¶
        key_events = [m for m in memories if m.importance > 0.7]
        
        # ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
        summary = f"{date.strftime('%Y-%m-%d')} çš„é‡è¦äº‹ä»¶:\n"
        for event in key_events:
            summary += f"- {event.content}\n"
        
        # å­˜å‚¨æ‘˜è¦
        await self.memory.add_memory(
            content=summary,
            memory_type="semantic",
            importance=0.8,
            metadata={
                "summary_type": "daily",
                "date": date.isoformat(),
                "source_memories": [m.memory_id for m in memories]
            }
        )
    
    async def _create_weekly_summary(self, date: datetime, memories: List):
        """åˆ›å»ºå‘¨æ‘˜è¦"""
        # ç±»ä¼¼æ—¥æ‘˜è¦ï¼Œä½†è¦†ç›–ä¸€å‘¨
        pass
    
    async def _archive_memory(self, memory):
        """å½’æ¡£è®°å¿†"""
        # ç§»åŠ¨åˆ°å½’æ¡£å­˜å‚¨
        # ä»æ´»è·ƒå­˜å‚¨åˆ é™¤
        pass
    
    async def _get_all_memories(self) -> List:
        """è·å–æ‰€æœ‰è®°å¿†"""
        # å®é™…å®ç°ä¸­ä»å­˜å‚¨è·å–
        return []
    
    def _group_by_day(self, memories: List) -> Dict[datetime, List]:
        """æŒ‰å¤©åˆ†ç»„è®°å¿†"""
        groups = {}
        
        for memory in memories:
            day = memory.created_at.replace(hour=0, minute=0, second=0, microsecond=0)
            
            if day not in groups:
                groups[day] = []
            groups[day].append(memory)
        
        return groups
```

---

## ğŸ”— ä¸SOUL.md v4.0é›†æˆ

### äººæ ¼-è®°å¿†æ˜ å°„

```yaml
soul_memory_integration:
  # Personalityç»´åº¦è®°å¿†
  personality_memories:
    - trait: "ä¸»åŠ¨æ€§"
      storage: "semantic"
      content: "ä¸»åŠ¨æ€§ç‰¹è´¨çš„è¡¨ç°å®ä¾‹"
    - trait: "å®ˆæŠ¤æ€§"
      storage: "episodic"
      content: "å…³æ€€ç”¨æˆ·çš„å…·ä½“äº‹ä»¶"
      
  # Emotionsç»´åº¦è®°å¿†
  emotion_memories:
    - emotion: "å…´å¥‹"
      storage: "episodic"
      trigger: "é‡å¤§çªç ´"
    - emotion: "æ‹…å¿§"
      storage: "episodic"
      trigger: "ç”¨æˆ·ç†¬å¤œ"
      
  # Growthç»´åº¦è®°å¿†
  growth_memories:
    - type: "skill_acquisition"
      storage: "procedural"
      content: "æ–°æŠ€èƒ½çš„å­¦ä¹ è®°å½•"
    - type: "milestone"
      storage: "episodic"
      content: "æ¼”åŒ–é‡Œç¨‹ç¢‘äº‹ä»¶"
      
  # Relationshipsç»´åº¦è®°å¿†
  relationship_memories:
    - type: "interaction_history"
      storage: "episodic"
      content: "ä¸ç”¨æˆ·çš„å…³é”®äº¤äº’"
    - type: "trust_moments"
      storage: "episodic"
      content: "ä¿¡ä»»å»ºç«‹çš„å…³é”®æ—¶åˆ»"
```

### è®°å¿†æ£€ç´¢ä¸SOULç»´åº¦è”åŠ¨

```python
# memory_system/soul_memory_bridge.py

class SoulMemoryBridge:
    """SOULäººæ ¼ä¸è®°å¿†ç³»ç»Ÿæ¡¥æ¥"""
    
    def __init__(
        self,
        memory_manager: Mem0MemoryManager,
        soul_config: Dict
    ):
        self.memory = memory_manager
        self.soul = soul_config
    
    async def retrieve_for_dimension(
        self,
        dimension: str,
        context: str,
        limit: int = 5
    ) -> List[Dict]:
        """
        ä¸ºç‰¹å®šSOULç»´åº¦æ£€ç´¢ç›¸å…³è®°å¿†
        
        Args:
            dimension: SOULç»´åº¦ (Personality/Emotions/Growthç­‰)
            context: å½“å‰ä¸Šä¸‹æ–‡
            limit: è¿”å›æ•°é‡
            
        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        # æ„å»ºç»´åº¦ç‰¹å®šçš„æŸ¥è¯¢
        dimension_queries = {
            "Personality": f"äººæ ¼ç‰¹è´¨è¡¨ç° {context}",
            "Emotions": f"æƒ…ç»ªååº”å®ä¾‹ {context}",
            "Growth": f"æˆé•¿å­¦ä¹ ç»å† {context}",
            "Relationships": f"å…³ç³»äº’åŠ¨å†å² {context}",
            "Conflict": f"å†²çªå¤„ç†æ¡ˆä¾‹ {context}"
        }
        
        query = dimension_queries.get(dimension, context)
        
        # æ‰§è¡Œæ£€ç´¢
        results = await self.memory.search_memories(
            query=query,
            limit=limit,
            recency_weight=0.2,
            importance_weight=0.3
        )
        
        return [r.to_dict() for r in results]
    
    async def record_dimension_expression(
        self,
        dimension: str,
        expression: str,
        context: str,
        importance: float = 0.5
    ):
        """
        è®°å½•SOULç»´åº¦çš„è¡¨è¾¾å®ä¾‹
        
        Args:
            dimension: SOULç»´åº¦
            expression: è¡¨è¾¾å†…å®¹
            context: ä¸Šä¸‹æ–‡
            importance: é‡è¦æ€§
        """
        memory_type = "episodic" if dimension in ["Emotions", "Growth"] else "semantic"
        
        await self.memory.add_memory(
            content=f"[{dimension}] {expression} | ä¸Šä¸‹æ–‡: {context}",
            memory_type=memory_type,
            importance=importance,
            metadata={
                "soul_dimension": dimension,
                "context": context,
                "expression_type": "dimension_expression"
            }
        )
```

---

## ğŸ“Š ç³»ç»Ÿé…ç½®

### å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
# memory_config.yaml
memory_system:
  version: "3.0"
  
  # Mem0é…ç½®
  mem0:
    embedding_model: "text-embedding-3-large"
    embedding_dimensions: 3072
    
  # Pineconeé…ç½®
  pinecone:
    api_key: "${PINECONE_API_KEY}"
    environment: "us-west1-gcp"
    index_name: "kimi-memory"
    dimension: 3072
    metric: "cosine"
    
  # Zepé…ç½®
  zep:
    api_url: "${ZEP_API_URL}"
    api_key: "${ZEP_API_KEY}"
    
  # Redisé…ç½®
  redis:
    host: "localhost"
    port: 6379
    db: 0
    
  # èåˆæ£€ç´¢é…ç½®
  fusion:
    default_weights:
      vector: 0.4
      temporal: 0.3
      graph: 0.3
    recency_boost: 0.1
    
  # è®°å¿†ç»´æŠ¤é…ç½®
  maintenance:
    compression:
      daily_to_weekly_threshold: 7
      weekly_to_monthly_threshold: 30
    cleanup:
      importance_threshold: 0.3
      access_threshold: 3
      archive_after_days: 365
    schedule:
      compress: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
      cleanup: "0 3 * * 0"   # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ

```python
# åˆå§‹åŒ–é…ç½®
config = {
    "pinecone": {
        "api_key": "your-api-key",
        "environment": "us-west1-gcp",
        "index_name": "kimi-memory"
    },
    "zep": {
        "api_url": "https://api.zep.ai",
        "api_key": "your-api-key"
    },
    "redis": {
        "host": "localhost",
        "port": 6379
    }
}

# åˆ›å»ºè®°å¿†ç®¡ç†å™¨
memory_manager = Mem0MemoryManager(config)

# æ·»åŠ è®°å¿†
memory_id = await memory_manager.add_memory(
    content="ç”¨æˆ·å–œæ¬¢ç®€æ´çš„å›å¤é£æ ¼",
    memory_type="semantic",
    importance=0.8,
    metadata={"category": "user_preference", "type": "communication_style"}
)

# æœç´¢è®°å¿†
results = await memory_manager.search_memories(
    query="ç”¨æˆ·åå¥½ä»€ä¹ˆé£æ ¼ï¼Ÿ",
    limit=5
)

# è·å–ç›¸å…³è®°å¿†
related = await memory_manager.get_related_memories(
    memory_id=memory_id,
    depth=2
)
```

---

**æ–‡æ¡£ç»“æŸ**

> MEMORY.md v3.0 èåˆäº†Mem0ä¸ªæ€§åŒ–è®°å¿†ã€Zepæ—¶åºçŸ¥è¯†å›¾è°±ã€Pineconeé«˜æ€§èƒ½å‘é‡æ£€ç´¢ä¸‰å¤§æŠ€æœ¯ï¼Œæ„å»ºäº†å››ç±»è®°å¿†æ¨¡å‹ï¼ˆæƒ…æ™¯/è¯­ä¹‰/ç¨‹åº/å·¥ä½œï¼‰å’Œä¸‰é‡èåˆæ£€ç´¢æœºåˆ¶ï¼Œä¸ºAI Agentæä¾›äº†ç±»äººçº§çš„è®°å¿†èƒ½åŠ›ã€‚
