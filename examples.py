#!/usr/bin/env python3
"""
å¤šæ¨¡æ€Agentä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå„ç§ä½¿ç”¨åœºæ™¯
"""

import asyncio
import os
from multimodal_agent import MultimodalAgent, VisionInput, AudioInput


async def example_1_basic_vision():
    """ç¤ºä¾‹1: åŸºç¡€è§†è§‰ç†è§£"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹1: åŸºç¡€è§†è§‰ç†è§£")
    print("="*50)
    
    async with MultimodalAgent() as agent:
        # ä½¿ç”¨ç½‘ç»œå›¾ç‰‡
        vision_input = VisionInput(
            image_url="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            prompt="è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„è‡ªç„¶é£æ™¯"
        )
        
        result = await agent.vision_understand(vision_input)
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æè¿°: {result}")


async def example_2_local_image():
    """ç¤ºä¾‹2: æœ¬åœ°å›¾ç‰‡åˆ†æ"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹2: æœ¬åœ°å›¾ç‰‡åˆ†æ")
    print("="*50)
    
    # æ£€æŸ¥æœ¬åœ°å›¾ç‰‡æ˜¯å¦å­˜åœ¨
    local_image = "test_image.jpg"
    if not os.path.exists(local_image):
        print(f"âš ï¸  æœ¬åœ°å›¾ç‰‡ {local_image} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤ç¤ºä¾‹")
        return
    
    async with MultimodalAgent() as agent:
        vision_input = VisionInput(
            image_path=local_image,
            prompt="è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹"
        )
        
        result = await agent.vision_understand(vision_input)
        print(f"ğŸ–¼ï¸  å›¾ç‰‡åˆ†æ: {result}")


async def example_3_tts_various_voices():
    """ç¤ºä¾‹3: ä¸åŒå£°éŸ³çš„TTS"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹3: ä¸åŒå£°éŸ³çš„TTS")
    print("="*50)
    
    voices = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    test_text = "ä½ å¥½ï¼Œæˆ‘æ˜¯å¤šæ¨¡æ€AIåŠ©æ‰‹ã€‚"
    
    async with MultimodalAgent() as agent:
        for voice in voices:
            tts_output = await agent.text_to_speech(
                text=test_text,
                voice=voice
            )
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            output_file = f"tts_{voice}.mp3"
            with open(output_file, "wb") as f:
                f.write(tts_output.audio_bytes)
            
            print(f"ğŸ”Š å£°éŸ³ {voice}: å·²ä¿å­˜åˆ° {output_file}")


async def example_4_multimodal_conversation():
    """ç¤ºä¾‹4: å¤šæ¨¡æ€å¯¹è¯"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹4: å¤šæ¨¡æ€å¯¹è¯")
    print("="*50)
    
    async with MultimodalAgent() as agent:
        # å›¾æ–‡å¯¹è¯
        vision_input = VisionInput(
            image_url="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            prompt="è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä»€ä¹ˆåœºæ™¯ï¼Ÿ"
        )
        
        result = await agent.multimodal_chat(
            text="è¯·è¯¦ç»†æè¿°",
            vision_input=vision_input,
            enable_tts=True  # åŒæ—¶ç”Ÿæˆè¯­éŸ³å›å¤
        )
        
        print(f"ğŸ“ æ–‡æœ¬å›å¤: {result['text'][:200]}...")
        
        if result['audio']:
            with open("multimodal_response.mp3", "wb") as f:
                f.write(result['audio'].audio_bytes)
            print(f"ğŸ”Š è¯­éŸ³å›å¤å·²ä¿å­˜åˆ° multimodal_response.mp3")


async def example_5_voice_to_text():
    """ç¤ºä¾‹5: è¯­éŸ³è½¬æ–‡å­—"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹5: è¯­éŸ³è½¬æ–‡å­—")
    print("="*50)
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
    audio_file = "test_audio.mp3"
    if not os.path.exists(audio_file):
        print(f"âš ï¸  éŸ³é¢‘æ–‡ä»¶ {audio_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤ç¤ºä¾‹")
        print("ğŸ’¡ è¯·å‡†å¤‡ä¸€ä¸ªMP3æ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶")
        return
    
    async with MultimodalAgent() as agent:
        audio_input = AudioInput(
            audio_path=audio_file,
            language="zh"  # æŒ‡å®šè¯­è¨€ä¸ºä¸­æ–‡
        )
        
        text = await agent.speech_to_text(audio_input)
        print(f"ğŸ¤ è½¬å½•ç»“æœ: {text}")


async def example_6_complete_workflow():
    """ç¤ºä¾‹6: å®Œæ•´å·¥ä½œæµç¨‹ - è¯­éŸ³+è§†è§‰+è¯­éŸ³å›å¤"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹6: å®Œæ•´å·¥ä½œæµç¨‹")
    print("="*50)
    
    async with MultimodalAgent() as agent:
        # æ¨¡æ‹Ÿç”¨æˆ·è¯­éŸ³æé—®
        print("1ï¸âƒ£ ç”¨æˆ·è¯­éŸ³æé—®: 'è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ'")
        
        # è¿™é‡Œå‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†è¯­éŸ³è½¬å½•çš„æ–‡æœ¬
        user_question = "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"
        
        # åˆ†æå›¾ç‰‡
        vision_input = VisionInput(
            image_url="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            prompt=user_question
        )
        
        # è·å–è§†è§‰ç†è§£ç»“æœ
        vision_result = await agent.vision_understand(vision_input)
        print(f"2ï¸âƒ£ AIè§†è§‰åˆ†æ: {vision_result[:150]}...")
        
        # ç”Ÿæˆè¯­éŸ³å›å¤
        tts_output = await agent.text_to_speech(
            text=vision_result,
            voice="nova"
        )
        
        with open("workflow_response.mp3", "wb") as f:
            f.write(tts_output.audio_bytes)
        
        print(f"3ï¸âƒ£ è¯­éŸ³å›å¤å·²ç”Ÿæˆ: workflow_response.mp3")


async def run_all_examples():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸš€" * 25)
    print("   å¤šæ¨¡æ€Agentä½¿ç”¨ç¤ºä¾‹")
    print("ğŸš€" * 25)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("\nâš ï¸  è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        print("ğŸ’¡ è¯·è®¾ç½®: export OPENAI_API_KEY='your-api-key'")
        return
    
    examples = [
        example_1_basic_vision,
        example_2_local_image,
        example_3_tts_various_voices,
        example_4_multimodal_conversation,
        example_5_voice_to_text,
        example_6_complete_workflow,
    ]
    
    for example in examples:
        try:
            await example()
        except Exception as e:
            print(f"âŒ ç¤ºä¾‹å¤±è´¥: {e}")
    
    print("\n" + "="*50)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹æ‰§è¡Œå®Œæ¯•")
    print("="*50)


if __name__ == "__main__":
    asyncio.run(run_all_examples())
