#!/usr/bin/env python3
"""
Log Anomaly Detector - æ—¥å¿—åˆ†æä¸å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
è‡ªåŠ¨åˆ†æç³»ç»Ÿæ—¥å¿—ï¼Œæ£€æµ‹å¼‚å¸¸æ¨¡å¼å¹¶ç”ŸæˆæŠ¥å‘Š

Author: KCGS
Version: 1.0.0
Date: 2026-02-27
"""

import re
import os
import json
import hashlib
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Callable, Any
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict, Counter
import statistics


class AnomalyLevel(Enum):
    """å¼‚å¸¸å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""
    ERROR_SPIKE = "error_spike"
    EXCEPTION = "exception"
    TIMEOUT = "timeout"
    MEMORY_ISSUE = "memory_issue"
    CONNECTION_ISSUE = "connection_issue"
    FREQUENCY_ANOMALY = "frequency_anomaly"
    PATTERN_MATCH = "pattern_match"


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: str
    message: str
    source: str
    raw_line: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Anomaly:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    id: str
    level: AnomalyLevel
    type: AnomalyType
    timestamp: datetime
    source: str
    description: str
    log_entry: Optional[LogEntry]
    impact: str
    suggested_action: Optional[str] = None
    related_anomalies: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            "id": self.id,
            "level": self.level.value,
            "type": self.type.value,
            "timestamp": self.timestamp.isoformat(),
            "source": self.source,
            "description": self.description,
            "impact": self.impact,
            "suggested_action": self.suggested_action,
            "related_anomalies": self.related_anomalies
        }


@dataclass
class ErrorStats:
    """é”™è¯¯ç»Ÿè®¡"""
    total_entries: int
    error_count: int
    warning_count: int
    unique_error_types: int
    top_errors: List[Dict]
    error_rate: float  # é”™è¯¯ç‡
    trend: str  # "increasing", "decreasing", "stable"


@dataclass
class AnomalyConfig:
    """å¼‚å¸¸æ£€æµ‹é…ç½®"""
    error_patterns: List[str] = field(default_factory=lambda: [
        r"ERROR|FATAL|CRITICAL",
        r"Exception|Traceback",
        r"timeout|timed out",
        r"memory leak|out of memory",
        r"connection refused|ECONNREFUSED"
    ])
    
    frequency_thresholds: Dict[str, int] = field(default_factory=lambda: {
        "error_per_minute": 10,
        "warning_per_minute": 50,
        "unique_errors": 5
    })
    
    time_series_config: Dict[str, float] = field(default_factory=lambda: {
        "window_size": 10,
        "std_threshold": 3.0,
        "trend_change_threshold": 0.3
    })


class LogParser:
    """æ—¥å¿—è§£æå™¨"""
    
    def __init__(self):
        self.parsers = {
            "json": self._parse_json,
            "plain": self._parse_plain,
            "syslog": self._parse_syslog
        }
    
    def detect_format(self, sample_lines: List[str]) -> str:
        """æ£€æµ‹æ—¥å¿—æ ¼å¼"""
        if not sample_lines:
            return "plain"
        
        # å°è¯•JSONæ ¼å¼
        try:
            json.loads(sample_lines[0])
            return "json"
        except:
            pass
        
        # æ£€æµ‹syslogæ ¼å¼
        if re.match(r'^\w+\s+\d+\s+\d{2}:\d{2}:\d{2}', sample_lines[0]):
            return "syslog"
        
        return "plain"
    
    def _parse_json(self, line: str, source: str) -> Optional[LogEntry]:
        """è§£æJSONæ ¼å¼æ—¥å¿—"""
        try:
            data = json.loads(line)
            return LogEntry(
                timestamp=self._parse_timestamp(data.get('timestamp', '')),
                level=data.get('level', 'INFO'),
                message=data.get('message', ''),
                source=source,
                raw_line=line,
                metadata={k: v for k, v in data.items() if k not in ['timestamp', 'level', 'message']}
            )
        except:
            return None
    
    def _parse_plain(self, line: str, source: str) -> Optional[LogEntry]:
        """è§£ææ™®é€šæ–‡æœ¬æ ¼å¼æ—¥å¿—"""
        # å°è¯•åŒ¹é…å¸¸è§æ ¼å¼: 2024-01-01 10:00:00 INFO message
        patterns = [
            r'(\d{4}-\d{2}-\d{2}[\sT]\d{2}:\d{2}:\d{2})\s+(\w+)\s+(.*)',
            r'\[(\d{4}-\d{2}-\d{2}[\sT]\d{2}:\d{2}:\d{2})\]\s+(\w+)\s+(.*)',
            r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})\s+(\w+)\s+(.*)'
        ]
        
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                return LogEntry(
                    timestamp=self._parse_timestamp(match.group(1)),
                    level=match.group(2).upper(),
                    message=match.group(3),
                    source=source,
                    raw_line=line
                )
        
        # æ— æ³•è§£æï¼Œä½œä¸ºåŸå§‹æ—¥å¿—
        return LogEntry(
            timestamp=datetime.now(),
            level="UNKNOWN",
            message=line,
            source=source,
            raw_line=line
        )
    
    def _parse_syslog(self, line: str, source: str) -> Optional[LogEntry]:
        """è§£æsyslogæ ¼å¼"""
        pattern = r'^(\w+\s+\d+\s+\d{2}:\d{2}:\d{2})\s+(\S+)\s+(.*)'
        match = re.match(pattern, line)
        if match:
            return LogEntry(
                timestamp=self._parse_timestamp(match.group(1)),
                level="INFO",
                message=match.group(3),
                source=source,
                raw_line=line
            )
        return None
    
    def _parse_timestamp(self, ts_str: str) -> datetime:
        """è§£ææ—¶é—´æˆ³"""
        formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%dT%H:%M:%S.%f",
            "%m/%d/%Y %H:%M:%S",
            "%b %d %H:%M:%S"
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(ts_str, fmt)
            except:
                continue
        
        return datetime.now()
    
    def parse_file(self, file_path: str, format_type: Optional[str] = None) -> List[LogEntry]:
        """è§£ææ—¥å¿—æ–‡ä»¶"""
        entries = []
        
        if not os.path.exists(file_path):
            return entries
        
        # æ£€æµ‹æ ¼å¼
        if format_type is None:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                sample = [f.readline().strip() for _ in range(5)]
                format_type = self.detect_format(sample)
        
        # è§£ææ–‡ä»¶
        parser = self.parsers.get(format_type, self._parse_plain)
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if line:
                    entry = parser(line, file_path)
                    if entry:
                        entries.append(entry)
        
        return entries


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å¼•æ“"""
    
    def __init__(self, config: AnomalyConfig):
        self.config = config
        self.compiled_patterns = [re.compile(p, re.IGNORECASE) for p in config.error_patterns]
    
    def detect_pattern_anomalies(self, entries: List[LogEntry]) -> List[Anomaly]:
        """åŸºäºæ¨¡å¼åŒ¹é…æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        for entry in entries:
            for i, pattern in enumerate(self.compiled_patterns):
                if pattern.search(entry.message):
                    anomaly_type = self._get_anomaly_type(i)
                    level = self._get_level_for_type(anomaly_type)
                    
                    anomaly = Anomaly(
                        id=self._generate_id(entry),
                        level=level,
                        type=anomaly_type,
                        timestamp=entry.timestamp,
                        source=entry.source,
                        description=f"æ£€æµ‹åˆ° {anomaly_type.value}: {entry.message[:100]}",
                        log_entry=entry,
                        impact=self._assess_impact(anomaly_type),
                        suggested_action=self._get_suggested_action(anomaly_type)
                    )
                    anomalies.append(anomaly)
        
        return anomalies
    
    def detect_frequency_anomalies(self, entries: List[LogEntry], window_minutes: int = 5) -> List[Anomaly]:
        """åŸºäºé¢‘ç‡æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
        windows = defaultdict(list)
        for entry in entries:
            window_key = entry.timestamp.replace(
                second=0, microsecond=0
            ).replace(minute=(entry.timestamp.minute // window_minutes) * window_minutes)
            windows[window_key].append(entry)
        
        # æ£€æµ‹é”™è¯¯é¢‘ç‡å¼‚å¸¸
        for window, window_entries in sorted(windows.items()):
            error_count = sum(1 for e in window_entries if e.level in ['ERROR', 'FATAL', 'CRITICAL'])
            
            if error_count > self.config.frequency_thresholds.get('error_per_minute', 10):
                anomaly = Anomaly(
                    id=f"freq_{window.isoformat()}",
                    level=AnomalyLevel.WARNING,
                    type=AnomalyType.FREQUENCY_ANOMALY,
                    timestamp=window,
                    source=window_entries[0].source if window_entries else "unknown",
                    description=f"é”™è¯¯é¢‘ç‡å¼‚å¸¸: {error_count} ä¸ªé”™è¯¯åœ¨ {window_minutes} åˆ†é’Ÿå†…",
                    log_entry=None,
                    impact="å¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜æˆ–çªå‘æ•…éšœ",
                    suggested_action="æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å’Œæœ€è¿‘éƒ¨ç½²çš„å˜æ›´"
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def detect_time_series_anomalies(self, entries: List[LogEntry]) -> List[Anomaly]:
        """åŸºäºæ—¶åºåˆ†ææ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        
        if len(entries) < self.config.time_series_config.get('window_size', 10):
            return anomalies
        
        # æŒ‰åˆ†é’Ÿç»Ÿè®¡é”™è¯¯æ•°
        minute_counts = defaultdict(int)
        for entry in entries:
            if entry.level in ['ERROR', 'FATAL', 'CRITICAL']:
                minute_key = entry.timestamp.replace(second=0, microsecond=0)
                minute_counts[minute_key] += 1
        
        if not minute_counts:
            return anomalies
        
        # è®¡ç®—ç»Ÿè®¡å€¼
        counts = list(minute_counts.values())
        mean = statistics.mean(counts)
        std = statistics.stdev(counts) if len(counts) > 1 else 0
        
        if std == 0:
            return anomalies
        
        # æ£€æµ‹Z-scoreå¼‚å¸¸
        threshold = self.config.time_series_config.get('std_threshold', 3.0)
        for timestamp, count in minute_counts.items():
            z_score = (count - mean) / std
            if abs(z_score) > threshold:
                level = AnomalyLevel.CRITICAL if z_score > 0 else AnomalyLevel.INFO
                anomaly = Anomaly(
                    id=f"ts_{timestamp.isoformat()}",
                    level=level,
                    type=AnomalyType.FREQUENCY_ANOMALY,
                    timestamp=timestamp,
                    source=entries[0].source if entries else "unknown",
                    description=f"æ—¶åºå¼‚å¸¸: {count} ä¸ªé”™è¯¯ (Z-score: {z_score:.2f})",
                    log_entry=None,
                    impact="é”™è¯¯æ•°é‡æ˜¾è‘—åç¦»æ­£å¸¸èŒƒå›´" if z_score > 0 else "é”™è¯¯æ•°é‡å¼‚å¸¸ä½",
                    suggested_action="è°ƒæŸ¥å¼‚å¸¸æ—¶æ®µçš„ç³»ç»Ÿæ´»åŠ¨"
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _get_anomaly_type(self, pattern_index: int) -> AnomalyType:
        """æ ¹æ®æ¨¡å¼ç´¢å¼•è·å–å¼‚å¸¸ç±»å‹"""
        type_map = {
            0: AnomalyType.ERROR_SPIKE,
            1: AnomalyType.EXCEPTION,
            2: AnomalyType.TIMEOUT,
            3: AnomalyType.MEMORY_ISSUE,
            4: AnomalyType.CONNECTION_ISSUE
        }
        return type_map.get(pattern_index, AnomalyType.PATTERN_MATCH)
    
    def _get_level_for_type(self, anomaly_type: AnomalyType) -> AnomalyLevel:
        """æ ¹æ®å¼‚å¸¸ç±»å‹è·å–å‘Šè­¦çº§åˆ«"""
        level_map = {
            AnomalyType.ERROR_SPIKE: AnomalyLevel.WARNING,
            AnomalyType.EXCEPTION: AnomalyLevel.CRITICAL,
            AnomalyType.TIMEOUT: AnomalyLevel.WARNING,
            AnomalyType.MEMORY_ISSUE: AnomalyLevel.CRITICAL,
            AnomalyType.CONNECTION_ISSUE: AnomalyLevel.WARNING,
            AnomalyType.FREQUENCY_ANOMALY: AnomalyLevel.WARNING,
            AnomalyType.PATTERN_MATCH: AnomalyLevel.INFO
        }
        return level_map.get(anomaly_type, AnomalyLevel.INFO)
    
    def _assess_impact(self, anomaly_type: AnomalyType) -> str:
        """è¯„ä¼°å½±å“"""
        impact_map = {
            AnomalyType.ERROR_SPIKE: "å¯èƒ½å½±å“ç³»ç»Ÿç¨³å®šæ€§",
            AnomalyType.EXCEPTION: "åŠŸèƒ½å¼‚å¸¸ï¼Œéœ€è¦ç«‹å³å¤„ç†",
            AnomalyType.TIMEOUT: "å“åº”å»¶è¿Ÿï¼Œå½±å“ç”¨æˆ·ä½“éªŒ",
            AnomalyType.MEMORY_ISSUE: "å¯èƒ½å¯¼è‡´æœåŠ¡å´©æºƒ",
            AnomalyType.CONNECTION_ISSUE: "å¤–éƒ¨ä¾èµ–é—®é¢˜",
            AnomalyType.FREQUENCY_ANOMALY: "ç³»ç»Ÿè¡Œä¸ºå¼‚å¸¸",
            AnomalyType.PATTERN_MATCH: "éœ€è¦å…³æ³¨çš„æ—¥å¿—äº‹ä»¶"
        }
        return impact_map.get(anomaly_type, "æœªçŸ¥å½±å“")
    
    def _get_suggested_action(self, anomaly_type: AnomalyType) -> Optional[str]:
        """è·å–å»ºè®®æ“ä½œ"""
        action_map = {
            AnomalyType.ERROR_SPIKE: "æ£€æŸ¥é”™è¯¯æ—¥å¿—è¯¦æƒ…å’Œç›¸å…³æœåŠ¡çŠ¶æ€",
            AnomalyType.EXCEPTION: "æŸ¥çœ‹å®Œæ•´å †æ ˆè·Ÿè¸ªï¼Œå®šä½ä»£ç é—®é¢˜",
            AnomalyType.TIMEOUT: "æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–æœåŠ¡å“åº”æ—¶é—´",
            AnomalyType.MEMORY_ISSUE: "æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œè€ƒè™‘é‡å¯æœåŠ¡",
            AnomalyType.CONNECTION_ISSUE: "éªŒè¯ç½‘ç»œé…ç½®å’Œå¤–éƒ¨æœåŠ¡å¯ç”¨æ€§",
            AnomalyType.FREQUENCY_ANOMALY: "åˆ†æå¼‚å¸¸æ—¶æ®µçš„ç³»ç»Ÿè´Ÿè½½å’Œå˜æ›´",
            AnomalyType.PATTERN_MATCH: "æ ¹æ®å…·ä½“æƒ…å†µè¯„ä¼°æ˜¯å¦éœ€è¦å¤„ç†"
        }
        return action_map.get(anomaly_type)
    
    def _generate_id(self, entry: LogEntry) -> str:
        """ç”Ÿæˆå¼‚å¸¸ID"""
        content = f"{entry.timestamp.isoformat()}:{entry.source}:{entry.message[:50]}"
        return hashlib.md5(content.encode()).hexdigest()[:12]


@dataclass
class AnalysisReport:
    """åˆ†ææŠ¥å‘Š"""
    generated_at: datetime
    time_range: Tuple[datetime, datetime]
    total_entries: int
    anomalies: List[Anomaly]
    error_stats: ErrorStats
    recommendations: List[str]
    
    def to_dict(self) -> Dict:
        return {
            "generated_at": self.generated_at.isoformat(),
            "time_range": [self.time_range[0].isoformat(), self.time_range[1].isoformat()],
            "total_entries": self.total_entries,
            "anomaly_count": len(self.anomalies),
            "anomalies_by_level": self._count_by_level(),
            "error_stats": asdict(self.error_stats) if self.error_stats else None,
            "recommendations": self.recommendations
        }
    
    def _count_by_level(self) -> Dict[str, int]:
        counts = defaultdict(int)
        for a in self.anomalies:
            counts[a.level.value] += 1
        return dict(counts)
    
    def to_html(self) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æ—¥å¿—åˆ†ææŠ¥å‘Š - {self.generated_at.strftime('%Y-%m-%d %H:%M')}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; }}
        h1 {{ color: #333; border-bottom: 2px solid #4CAF50; padding-bottom: 10px; }}
        .summary {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin: 20px 0; }}
        .stat-box {{ background: #f8f9fa; padding: 15px; border-radius: 6px; text-align: center; }}
        .stat-value {{ font-size: 24px; font-weight: bold; color: #4CAF50; }}
        .stat-label {{ color: #666; margin-top: 5px; }}
        .anomaly {{ margin: 10px 0; padding: 15px; border-left: 4px solid; border-radius: 4px; }}
        .anomaly.critical {{ background: #ffebee; border-color: #f44336; }}
        .anomaly.warning {{ background: #fff3e0; border-color: #ff9800; }}
        .anomaly.info {{ background: #e3f2fd; border-color: #2196f3; }}
        .anomaly.emergency {{ background: #fce4ec; border-color: #e91e63; }}
        .level-badge {{ display: inline-block; padding: 2px 8px; border-radius: 12px; font-size: 12px; font-weight: bold; }}
        .level-critical {{ background: #f44336; color: white; }}
        .level-warning {{ background: #ff9800; color: white; }}
        .level-info {{ background: #2196f3; color: white; }}
        .level-emergency {{ background: #e91e63; color: white; }}
        .recommendations {{ background: #e8f5e9; padding: 15px; border-radius: 6px; margin-top: 20px; }}
        .recommendations ul {{ margin: 10px 0; }}
        .recommendations li {{ margin: 5px 0; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š æ—¥å¿—åˆ†ææŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {self.generated_at.strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>åˆ†æèŒƒå›´: {self.time_range[0].strftime('%Y-%m-%d %H:%M')} ~ {self.time_range[1].strftime('%Y-%m-%d %H:%M')}</p>
        
        <div class="summary">
            <div class="stat-box">
                <div class="stat-value">{self.total_entries}</div>
                <div class="stat-label">æ€»æ—¥å¿—æ•°</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">{len(self.anomalies)}</div>
                <div class="stat-label">å¼‚å¸¸æ•°é‡</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">{len([a for a in self.anomalies if a.level == AnomalyLevel.CRITICAL])}</div>
                <div class="stat-label">ä¸¥é‡å¼‚å¸¸</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">{self.error_stats.error_rate:.1%}</div>
                <div class="stat-label">é”™è¯¯ç‡</div>
            </div>
        </div>
        
        <h2>ğŸ” æ£€æµ‹åˆ°çš„å¼‚å¸¸</h2>
"""
        
        for anomaly in sorted(self.anomalies, key=lambda x: x.timestamp, reverse=True):
            level_class = anomaly.level.value
            html += f"""
        <div class="anomaly {level_class}">
            <span class="level-badge level-{level_class}">{anomaly.level.value.upper()}</span>
            <strong>{anomaly.type.value}</strong> - {anomaly.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
            <p><strong>æè¿°:</strong> {anomaly.description}</p>
            <p><strong>å½±å“:</strong> {anomaly.impact}</p>
            {f"<p><strong>å»ºè®®:</strong> {anomaly.suggested_action}</p>" if anomaly.suggested_action else ""}
        </div>
"""
        
        if self.recommendations:
            html += """
        <div class="recommendations">
            <h3>ğŸ’¡ æ”¹è¿›å»ºè®®</h3>
            <ul>
"""
            for rec in self.recommendations:
                html += f"                <li>{rec}</li>\n"
            html += """            </ul>
        </div>
"""
        
        html += """
    </div>
</body>
</html>
"""
        return html


class LogAnomalyDetector:
    """æ—¥å¿—å¼‚å¸¸æ£€æµ‹å™¨ä¸»ç±»"""
    
    def __init__(self, data_dir: str = "./log_analysis", config: Optional[AnomalyConfig] = None):
        self.data_dir = data_dir
        self.config = config or AnomalyConfig()
        self.parser = LogParser()
        self.detector = AnomalyDetector(self.config)
        self.anomaly_callbacks: Dict[AnomalyLevel, List[Callable]] = defaultdict(list)
        self._monitoring = False
        self._monitor_task = None
        
        os.makedirs(data_dir, exist_ok=True)
    
    def analyze_log_file(self, log_path: str, log_type: Optional[str] = None, 
                         time_window: Optional[str] = None) -> AnalysisReport:
        """åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        entries = self.parser.parse_file(log_path, log_type)
        
        # åº”ç”¨æ—¶é—´çª—å£è¿‡æ»¤
        if time_window:
            entries = self._filter_by_time_window(entries, time_window)
        
        return self._analyze_entries(entries)
    
    def analyze_multiple_files(self, log_paths: List[str], 
                               time_range: Optional[Tuple[datetime, datetime]] = None) -> AnalysisReport:
        """åˆ†æå¤šä¸ªæ—¥å¿—æ–‡ä»¶"""
        all_entries = []
        
        for path in log_paths:
            entries = self.parser.parse_file(path)
            if time_range:
                entries = [e for e in entries if time_range[0] <= e.timestamp <= time_range[1]]
            all_entries.extend(entries)
        
        all_entries.sort(key=lambda x: x.timestamp)
        return self._analyze_entries(all_entries)
    
    def _analyze_entries(self, entries: List[LogEntry]) -> AnalysisReport:
        """åˆ†ææ—¥å¿—æ¡ç›®"""
        if not entries:
            return AnalysisReport(
                generated_at=datetime.now(),
                time_range=(datetime.now(), datetime.now()),
                total_entries=0,
                anomalies=[],
                error_stats=None,
                recommendations=["æ— æ—¥å¿—æ•°æ®å¯ä¾›åˆ†æ"]
            )
        
        # æ‰§è¡Œå¤šç§å¼‚å¸¸æ£€æµ‹
        anomalies = []
        anomalies.extend(self.detector.detect_pattern_anomalies(entries))
        anomalies.extend(self.detector.detect_frequency_anomalies(entries))
        anomalies.extend(self.detector.detect_time_series_anomalies(entries))
        
        # å»é‡
        seen_ids = set()
        unique_anomalies = []
        for a in anomalies:
            if a.id not in seen_ids:
                seen_ids.add(a.id)
                unique_anomalies.append(a)
        
        # ç”Ÿæˆç»Ÿè®¡
        error_stats = self._calculate_error_stats(entries)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(unique_anomalies, error_stats)
        
        return AnalysisReport(
            generated_at=datetime.now(),
            time_range=(entries[0].timestamp, entries[-1].timestamp),
            total_entries=len(entries),
            anomalies=unique_anomalies,
            error_stats=error_stats,
            recommendations=recommendations
        )
    
    def _filter_by_time_window(self, entries: List[LogEntry], window: str) -> List[LogEntry]:
        """æŒ‰æ—¶é—´çª—å£è¿‡æ»¤"""
        now = datetime.now()
        
        if window.endswith('h'):
            hours = int(window[:-1])
            cutoff = now - timedelta(hours=hours)
        elif window.endswith('d'):
            days = int(window[:-1])
            cutoff = now - timedelta(days=days)
        elif window.endswith('m'):
            minutes = int(window[:-1])
            cutoff = now - timedelta(minutes=minutes)
        else:
            return entries
        
        return [e for e in entries if e.timestamp >= cutoff]
    
    def _calculate_error_stats(self, entries: List[LogEntry]) -> ErrorStats:
        """è®¡ç®—é”™è¯¯ç»Ÿè®¡"""
        error_levels = ['ERROR', 'FATAL', 'CRITICAL']
        warning_levels = ['WARNING', 'WARN']
        
        error_count = sum(1 for e in entries if e.level in error_levels)
        warning_count = sum(1 for e in entries if e.level in warning_levels)
        
        # ç»Ÿè®¡å”¯ä¸€é”™è¯¯ç±»å‹
        error_messages = [e.message for e in entries if e.level in error_levels]
        error_types = Counter(error_messages)
        
        top_errors = [
            {"message": msg[:100], "count": count}
            for msg, count in error_types.most_common(5)
        ]
        
        error_rate = error_count / len(entries) if entries else 0
        
        return ErrorStats(
            total_entries=len(entries),
            error_count=error_count,
            warning_count=warning_count,
            unique_error_types=len(error_types),
            top_errors=top_errors,
            error_rate=error_rate,
            trend="stable"  # ç®€åŒ–å¤„ç†
        )
    
    def _generate_recommendations(self, anomalies: List[Anomaly], stats: ErrorStats) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if stats and stats.error_rate > 0.1:
            recommendations.append("é”™è¯¯ç‡è¶…è¿‡10%ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§")
        
        critical_count = len([a for a in anomalies if a.level == AnomalyLevel.CRITICAL])
        if critical_count > 0:
            recommendations.append(f"å‘ç° {critical_count} ä¸ªä¸¥é‡å¼‚å¸¸ï¼Œéœ€è¦ä¼˜å…ˆå¤„ç†")
        
        if stats and stats.unique_error_types > 10:
            recommendations.append("é”™è¯¯ç±»å‹è¾ƒå¤šï¼Œå»ºè®®è¿›è¡Œé”™è¯¯åˆ†ç±»å’Œæ ¹å› åˆ†æ")
        
        memory_anomalies = [a for a in anomalies if a.type == AnomalyType.MEMORY_ISSUE]
        if memory_anomalies:
            recommendations.append("æ£€æµ‹åˆ°å†…å­˜ç›¸å…³é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œç»§ç»­ä¿æŒç›‘æ§")
        
        return recommendations
    
    def set_anomaly_callback(self, level: AnomalyLevel, callback: Callable):
        """è®¾ç½®å¼‚å¸¸å›è°ƒ"""
        self.anomaly_callbacks[level].append(callback)
    
    def start_monitoring(self, log_paths: List[str], check_interval: int = 60,
                        alert_callback: Optional[Callable] = None):
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        self._monitoring = True
        self._monitor_task = asyncio.create_task(
            self._monitor_loop(log_paths, check_interval, alert_callback)
        )
    
    def stop_monitoring(self):
        """åœæ­¢å®æ—¶ç›‘æ§"""
        self._monitoring = False
        if self._monitor_task:
            self._monitor_task.cancel()
    
    async def _monitor_loop(self, log_paths: List[str], interval: int, 
                           alert_callback: Optional[Callable]):
        """ç›‘æ§å¾ªç¯"""
        last_positions = {path: 0 for path in log_paths}
        
        while self._monitoring:
            try:
                for path in log_paths:
                    if not os.path.exists(path):
                        continue
                    
                    # è¯»å–æ–°å†…å®¹
                    current_size = os.path.getsize(path)
                    if current_size <= last_positions[path]:
                        continue
                    
                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                        f.seek(last_positions[path])
                        new_lines = f.readlines()
                        last_positions[path] = f.tell()
                    
                    # è§£ææ–°æ¡ç›®
                    entries = []
                    for line in new_lines:
                        entry = self.parser._parse_plain(line.strip(), path)
                        if entry:
                            entries.append(entry)
                    
                    # æ£€æµ‹å¼‚å¸¸
                    if entries:
                        anomalies = self.detector.detect_pattern_anomalies(entries)
                        
                        for anomaly in anomalies:
                            # è§¦å‘å›è°ƒ
                            for callback in self.anomaly_callbacks.get(anomaly.level, []):
                                callback(anomaly)
                            
                            if alert_callback:
                                alert_callback(anomaly)
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(interval)
    
    def save_report(self, report: AnalysisReport, filename: Optional[str] = None):
        """ä¿å­˜æŠ¥å‘Š"""
        if filename is None:
            filename = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        
        filepath = os.path.join(self.data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report.to_html())
        
        return filepath
    
    def get_recent_anomalies(self, source: Optional[str] = None, 
                            time_range: str = "1h") -> List[Anomaly]:
        """è·å–æœ€è¿‘å¼‚å¸¸"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ä»æŒä¹…åŒ–å­˜å‚¨è¯»å–
        return []


def demo():
    """æ¼”ç¤º"""
    print("ğŸ” Log Anomaly Detector v1.0")
    print("=" * 50)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = LogAnomalyDetector(data_dir="./demo_analysis")
    
    # åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ•°æ®
    sample_logs = """
2026-02-27 14:30:00 INFO Application started successfully
2026-02-27 14:30:05 INFO Connected to database
2026-02-27 14:30:10 WARNING Slow query detected: 2.5s
2026-02-27 14:30:15 ERROR Connection timeout to external API
2026-02-27 14:30:20 ERROR Connection timeout to external API
2026-02-27 14:30:25 ERROR Connection timeout to external API
2026-02-27 14:30:30 CRITICAL Memory usage exceeded 90%
2026-02-27 14:30:35 ERROR Exception in worker thread: NullPointerException
2026-02-27 14:30:40 INFO Retrying connection...
2026-02-27 14:30:45 INFO Connection restored
"""
    
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    temp_log = "/tmp/demo_log.txt"
    with open(temp_log, 'w') as f:
        f.write(sample_logs)
    
    # åˆ†æ
    report = detector.analyze_log_file(temp_log)
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  æ€»æ—¥å¿—æ•°: {report.total_entries}")
    print(f"  å¼‚å¸¸æ•°é‡: {len(report.anomalies)}")
    
    if report.error_stats:
        print(f"  é”™è¯¯æ•°: {report.error_stats.error_count}")
        print(f"  é”™è¯¯ç‡: {report.error_stats.error_rate:.1%}")
    
    print(f"\nğŸ” æ£€æµ‹åˆ°çš„å¼‚å¸¸:")
    for anomaly in report.anomalies:
        print(f"  [{anomaly.level.value.upper()}] {anomaly.type.value}")
        print(f"    {anomaly.description[:60]}...")
        if anomaly.suggested_action:
            print(f"    ğŸ’¡ {anomaly.suggested_action}")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = detector.save_report(report, "demo_report.html")
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ¸…ç†
    os.remove(temp_log)
    
    return report


if __name__ == "__main__":
    demo()
