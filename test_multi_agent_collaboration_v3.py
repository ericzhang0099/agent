"""
Multi-Agentåä½œç³»ç»Ÿ v3.0 - å…¨é¢æµ‹è¯•éªŒè¯å¥—ä»¶

æµ‹è¯•è¦†ç›–ï¼š
1. å¯¹è¯å¼åä½œåè®®
2. Agenté—´æ·±åº¦å¯¹è¯æœºåˆ¶
3. ä»»åŠ¡åˆ†é…ä¸åé¦ˆå¾ªç¯
4. åä½œè´¨é‡è¯„ä¼°ç³»ç»Ÿ
5. AGENTS.md v2.0é›†æˆ
6. å¹¿æ’­å€’è½¬é—®é¢˜è§£å†³éªŒè¯
"""

import asyncio
import pytest
from typing import Dict, Any, List
from datetime import datetime

from multi_agent_collaboration_v3 import (
    DialogueManager, DialogueMessage, DialogueSession, DialogueType,
    MessageIntent, CollaborationTask, CollaborationMetrics,
    SoulState, AgentRole, CollaborationPhase,
    CollaborativeAgent, TaskAllocationSystem, CollaborationQualityMonitor,
    MultiAgentCollaborationSystem, ExampleCollaborativeAgent
)

from agents_v2_integration import (
    AGENTSv2Agent, AgentDefinition, LayerType, WorkflowMode,
    SoulDimensionProfile, AGENTSv2TeamFactory, AGENTSv2CollaborationSystem,
    WorkflowPatternExecutor
)


# ==================== æµ‹è¯•åŸºç±» ====================

class TestBase:
    """æµ‹è¯•åŸºç±»"""
    
    @pytest.fixture
    async def dialogue_manager(self):
        """å¯¹è¯ç®¡ç†å™¨fixture"""
        return DialogueManager()
    
    @pytest.fixture
    async def collaboration_system(self):
        """åä½œç³»ç»Ÿfixture"""
        return MultiAgentCollaborationSystem()
    
    @pytest.fixture
    async def agents_v2_system(self):
        """AGENTS v2ç³»ç»Ÿfixture"""
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        return system


# ==================== 1. å¯¹è¯å¼åä½œåè®®æµ‹è¯• ====================

class TestDialogueProtocol(TestBase):
    """æµ‹è¯•å¯¹è¯å¼åä½œåè®®"""
    
    @pytest.mark.asyncio
    async def test_dialogue_session_creation(self):
        """æµ‹è¯•å¯¹è¯ä¼šè¯åˆ›å»º"""
        dm = DialogueManager()
        
        session = await dm.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent_a",
            participants=["agent_a", "agent_b", "agent_c"],
            topic="Test Topic",
            goal="Test Goal"
        )
        
        assert session.dialogue_id is not None
        assert session.dialogue_type == DialogueType.DISCUSSION
        assert len(session.participants) == 3
        assert session.topic == "Test Topic"
        print("âœ… å¯¹è¯ä¼šè¯åˆ›å»ºæµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_message_adding_and_correlation(self):
        """æµ‹è¯•æ¶ˆæ¯æ·»åŠ å’Œå…³è”"""
        dm = DialogueManager()
        
        session = await dm.create_session(
            dialogue_type=DialogueType.DIALOGUE,
            initiator="agent_a",
            participants=["agent_a", "agent_b"],
            topic="Test",
            goal="Test"
        )
        
        # æ·»åŠ ç¬¬ä¸€æ¡æ¶ˆæ¯
        msg1 = await dm.add_message(
            dialogue_id=session.dialogue_id,
            sender_id="agent_a",
            content="Hello",
            intent=MessageIntent.INFORM
        )
        
        # æ·»åŠ å›å¤æ¶ˆæ¯
        msg2 = await dm.add_message(
            dialogue_id=session.dialogue_id,
            sender_id="agent_b",
            content="Hi there",
            intent=MessageIntent.INFORM,
            correlation_id=msg1.message_id
        )
        
        assert msg2.correlation_id == msg1.message_id
        assert msg2.is_response_to(msg1)
        assert len(session.messages) == 2
        print("âœ… æ¶ˆæ¯å…³è”æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_dialogue_types(self):
        """æµ‹è¯•ä¸åŒå¯¹è¯ç±»å‹"""
        dm = DialogueManager()
        
        dialogue_types = [
            DialogueType.DIALOGUE,
            DialogueType.DISCUSSION,
            DialogueType.DEBATE,
            DialogueType.NEGOTIATION,
            DialogueType.BRAINSTORM
        ]
        
        for dtype in dialogue_types:
            session = await dm.create_session(
                dialogue_type=dtype,
                initiator="agent_a",
                participants=["agent_a", "agent_b"],
                topic=f"{dtype.value} test",
                goal="Test"
            )
            assert session.dialogue_type == dtype
        
        print("âœ… å¯¹è¯ç±»å‹æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_soul_state_in_message(self):
        """æµ‹è¯•æ¶ˆæ¯ä¸­çš„SOULçŠ¶æ€"""
        dm = DialogueManager()
        
        soul = SoulState(
            personality=0.8,
            motivations=0.9,
            emotions=0.7
        )
        
        session = await dm.create_session(
            dialogue_type=DialogueType.DIALOGUE,
            initiator="agent_a",
            participants=["agent_a"],
            topic="Test",
            goal="Test"
        )
        
        msg = await dm.add_message(
            dialogue_id=session.dialogue_id,
            sender_id="agent_a",
            content="Test with soul",
            soul_state=soul
        )
        
        assert msg.soul_state is not None
        assert msg.soul_state.personality == 0.8
        print("âœ… SOULçŠ¶æ€æ¶ˆæ¯æµ‹è¯•é€šè¿‡")


# ==================== 2. å¹¿æ’­å€’è½¬é—®é¢˜è§£å†³æµ‹è¯• ====================

class TestBroadcastInversionFix(TestBase):
    """æµ‹è¯•å¹¿æ’­å€’è½¬é—®é¢˜ä¿®å¤"""
    
    @pytest.mark.asyncio
    async def test_dialogue_ratio_calculation(self):
        """æµ‹è¯•å¯¹è¯æ¯”ä¾‹è®¡ç®—"""
        dm = DialogueManager()
        
        session = await dm.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent_a",
            participants=["agent_a", "agent_b"],
            topic="Test",
            goal="Test"
        )
        
        # æ·»åŠ å¯¹è¯å¼æ¶ˆæ¯ï¼ˆåŒå‘ï¼‰
        msg1 = await dm.add_message(
            dialogue_id=session.dialogue_id,
            sender_id="agent_a",
            content="What do you think?",
            intent=MessageIntent.QUERY
        )
        
        msg2 = await dm.add_message(
            dialogue_id=session.dialogue_id,
            sender_id="agent_b",
            content="I think...",
            intent=MessageIntent.INFORM,
            correlation_id=msg1.message_id
        )
        
        # è®¡ç®—å¯¹è¯æ¯”ä¾‹
        ratio = session.get_dialogue_ratio()
        assert ratio == 1.0  # æ‰€æœ‰æ¶ˆæ¯éƒ½æ˜¯å¯¹è¯ç±»å‹
        
        # è®¡ç®—å“åº”ç‡
        response_rate = session.get_response_rate()
        assert response_rate == 1.0  # 100%å“åº”
        
        print(f"âœ… å¯¹è¯æ¯”ä¾‹: {ratio:.0%}, å“åº”ç‡: {response_rate:.0%}")
    
    @pytest.mark.asyncio
    async def test_monologue_vs_dialogue_detection(self):
        """æµ‹è¯•ç‹¬ç™½vså¯¹è¯æ£€æµ‹"""
        dm = DialogueManager()
        
        # åˆ›å»ºç‹¬ç™½ä¼šè¯
        mono_session = await dm.create_session(
            dialogue_type=DialogueType.MONOLOGUE,
            initiator="agent_a",
            participants=["agent_a"],
            topic="Monologue",
            goal="Test"
        )
        
        await dm.add_message(
            dialogue_id=mono_session.dialogue_id,
            sender_id="agent_a",
            content="This is a monologue..."
        )
        
        # åˆ›å»ºå¯¹è¯ä¼šè¯
        dia_session = await dm.create_session(
            dialogue_type=DialogueType.DIALOGUE,
            initiator="agent_b",
            participants=["agent_b", "agent_c"],
            topic="Dialogue",
            goal="Test"
        )
        
        await dm.add_message(
            dialogue_id=dia_session.dialogue_id,
            sender_id="agent_b",
            content="Hello?"
        )
        
        await dm.add_message(
            dialogue_id=dia_session.dialogue_id,
            sender_id="agent_c",
            content="Hi!"
        )
        
        assert mono_session.dialogue_type == DialogueType.MONOLOGUE
        assert dia_session.dialogue_type == DialogueType.DIALOGUE
        assert mono_session.get_dialogue_ratio() == 0.0
        assert dia_session.get_dialogue_ratio() == 1.0
        
        print("âœ… ç‹¬ç™½vså¯¹è¯æ£€æµ‹æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_target_70_percent_dialogue_ratio(self):
        """æµ‹è¯•ç›®æ ‡70%å¯¹è¯æ¯”ä¾‹è¾¾æˆ"""
        dm = DialogueManager()
        
        session = await dm.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent_a",
            participants=["agent_a", "agent_b", "agent_c"],
            topic="Collaborative Discussion",
            goal="Achieve high dialogue ratio"
        )
        
        # æ¨¡æ‹Ÿ10è½®å¯¹è¯
        prev_msg = None
        for i in range(10):
            sender = f"agent_{['a', 'b', 'c'][i % 3]}"
            
            msg = await dm.add_message(
                dialogue_id=session.dialogue_id,
                sender_id=sender,
                content=f"Message {i+1}",
                intent=MessageIntent.INFORM,
                correlation_id=prev_msg.message_id if prev_msg else None
            )
            prev_msg = msg
        
        ratio = session.get_dialogue_ratio()
        response_rate = session.get_response_rate()
        
        print(f"ğŸ“Š å¯¹è¯æ¯”ä¾‹: {ratio:.1%}")
        print(f"ğŸ“Š å“åº”ç‡: {response_rate:.1%}")
        
        # éªŒè¯è¾¾åˆ°ç›®æ ‡
        assert ratio >= 0.7, f"å¯¹è¯æ¯”ä¾‹ {ratio:.1%} æœªè¾¾åˆ°70%ç›®æ ‡"
        assert response_rate >= 0.8, f"å“åº”ç‡ {response_rate:.1%} æœªè¾¾åˆ°80%ç›®æ ‡"
        
        print("âœ… 70%å¯¹è¯æ¯”ä¾‹ç›®æ ‡è¾¾æˆ")


# ==================== 3. Agenté—´æ·±åº¦å¯¹è¯æœºåˆ¶æµ‹è¯• ====================

class TestDeepDialogueMechanism(TestBase):
    """æµ‹è¯•æ·±åº¦å¯¹è¯æœºåˆ¶"""
    
    @pytest.mark.asyncio
    async def test_collaborative_agent_message_exchange(self):
        """æµ‹è¯•åä½œAgentæ¶ˆæ¯äº¤æ¢"""
        system = MultiAgentCollaborationSystem()
        
        agent1 = ExampleCollaborativeAgent("agent1", AgentRole.DEVELOPER, "Dev1")
        agent2 = ExampleCollaborativeAgent("agent2", AgentRole.QA_ENGINEER, "QA1")
        
        system.register_agent(agent1)
        system.register_agent(agent2)
        
        # åˆ›å»ºå¯¹è¯
        session = await system.dialogue_manager.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent1",
            participants=["agent1", "agent2"],
            topic="Code Review",
            goal="Review implementation"
        )
        
        # Agent1å‘é€æ¶ˆæ¯
        msg1 = await agent1.send_message(
            dialogue_id=session.dialogue_id,
            content="Please review my code",
            intent=MessageIntent.REQUEST
        )
        
        # Agent2å“åº”
        msg2 = await agent2.respond_to_message(
            message=msg1,
            content="I found some issues...",
            intent=MessageIntent.FEEDBACK
        )
        
        assert msg2.correlation_id == msg1.message_id
        assert agent1.stats["messages_sent"] == 1
        assert agent2.stats["messages_sent"] == 1
        
        print("âœ… Agentæ¶ˆæ¯äº¤æ¢æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_multi_turn_dialogue(self):
        """æµ‹è¯•å¤šè½®å¯¹è¯"""
        system = MultiAgentCollaborationSystem()
        
        agent1 = ExampleCollaborativeAgent("agent1", AgentRole.RESEARCHER, "Researcher")
        agent2 = ExampleCollaborativeAgent("agent2", AgentRole.DEVELOPER, "Developer")
        
        system.register_agent(agent1)
        system.register_agent(agent2)
        
        task = CollaborationTask(
            task_type="discussion",
            description="Technical discussion",
            goal="Resolve design questions"
        )
        
        task_id, dialogue_id = await system.start_collaborative_task(
            task=task,
            dialogue_type=DialogueType.DISCUSSION,
            participants=["agent1", "agent2"]
        )
        
        # è¿è¡Œå¤šè½®
        for round_num in range(3):
            await system.run_collaboration_round(dialogue_id)
        
        session = await system.dialogue_manager.get_session(dialogue_id)
        assert len(session.messages) >= 6  # è‡³å°‘6æ¡æ¶ˆæ¯ï¼ˆ3è½® x 2 Agentï¼‰
        
        print(f"âœ… å¤šè½®å¯¹è¯æµ‹è¯•é€šè¿‡: {len(session.messages)} æ¡æ¶ˆæ¯")
    
    @pytest.mark.asyncio
    async def test_feedback_loop(self):
        """æµ‹è¯•åé¦ˆå¾ªç¯"""
        system = MultiAgentCollaborationSystem()
        
        agent = ExampleCollaborativeAgent("agent1", AgentRole.DEVELOPER, "Dev")
        system.register_agent(agent)
        
        task = CollaborationTask(
            task_id="test_task",
            task_type="development",
            description="Implement feature",
            goal="Complete implementation"
        )
        
        agent.active_tasks["test_task"] = task
        
        # æä¾›åé¦ˆ
        await agent.provide_feedback(
            task_id="test_task",
            feedback="Code needs refactoring",
            score=0.6
        )
        
        await agent.provide_feedback(
            task_id="test_task",
            feedback="Much better now",
            score=0.9
        )
        
        assert len(task.feedback_history) == 2
        assert task.feedback_history[0]["score"] == 0.6
        assert task.feedback_history[1]["score"] == 0.9
        assert agent.stats["feedback_given"] == 2
        
        print("âœ… åé¦ˆå¾ªç¯æµ‹è¯•é€šè¿‡")


# ==================== 4. ä»»åŠ¡åˆ†é…ä¸åé¦ˆå¾ªç¯æµ‹è¯• ====================

class TestTaskAllocation(TestBase):
    """æµ‹è¯•ä»»åŠ¡åˆ†é…ç³»ç»Ÿ"""
    
    @pytest.mark.asyncio
    async def test_capability_based_allocation(self):
        """æµ‹è¯•åŸºäºèƒ½åŠ›çš„åˆ†é…"""
        dm = DialogueManager()
        allocator = TaskAllocationSystem(dm)
        
        # æ³¨å†ŒAgentèƒ½åŠ›
        allocator.register_agent_capabilities("agent_dev", ["coding", "debugging"])
        allocator.register_agent_capabilities("agent_qa", ["testing", "review"])
        allocator.register_agent_capabilities("agent_pm", ["planning", "coordination"])
        
        # åˆ›å»ºéœ€è¦codingèƒ½åŠ›çš„ä»»åŠ¡
        task = CollaborationTask(
            task_type="development",
            description="Implement API",
            requirements={
                "capabilities": ["coding"],
                "num_agents": 1
            }
        )
        
        assigned = allocator._allocate_via_algorithm(
            task,
            ["agent_dev", "agent_qa", "agent_pm"]
        )
        
        assert "agent_dev" in assigned
        print("âœ… åŸºäºèƒ½åŠ›çš„åˆ†é…æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_load_balancing(self):
        """æµ‹è¯•è´Ÿè½½å‡è¡¡"""
        dm = DialogueManager()
        allocator = TaskAllocationSystem(dm)
        
        allocator.register_agent_capabilities("agent1", ["coding"])
        allocator.register_agent_capabilities("agent2", ["coding"])
        
        # æ¨¡æ‹Ÿagent1å·²æœ‰è´Ÿè½½
        allocator.agent_load["agent1"] = 5
        allocator.agent_load["agent2"] = 1
        
        task = CollaborationTask(
            task_type="development",
            requirements={"capabilities": ["coding"], "num_agents": 1}
        )
        
        assigned = allocator._allocate_via_algorithm(task, ["agent1", "agent2"])
        
        # åº”è¯¥åˆ†é…ç»™è´Ÿè½½è¾ƒä½çš„agent2
        assert "agent2" in assigned
        print("âœ… è´Ÿè½½å‡è¡¡æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_feedback_loop_processing(self):
        """æµ‹è¯•åé¦ˆå¾ªç¯å¤„ç†"""
        dm = DialogueManager()
        allocator = TaskAllocationSystem(dm)
        
        task = CollaborationTask(
            task_id="test_task",
            task_type="development",
            acceptance_criteria={"min_score": 0.8}
        )
        
        # æ·»åŠ ä¸æ»¡è¶³æ ‡å‡†çš„åé¦ˆ
        task.feedback_history.append({
            "from": "reviewer",
            "feedback": "Needs improvement",
            "score": 0.6
        })
        
        result = await allocator.process_feedback_loop(task)
        
        assert result == False  # æœªæ»¡è¶³æ ‡å‡†
        assert task.revision_count == 1
        assert task.status == "in_progress"
        
        # æ·»åŠ æ»¡è¶³æ ‡å‡†çš„åé¦ˆ
        task.feedback_history.append({
            "from": "reviewer",
            "feedback": "Good enough",
            "score": 0.85
        })
        
        result = await allocator.process_feedback_loop(task)
        
        assert result == True  # æ»¡è¶³æ ‡å‡†
        assert task.status == "completed"
        
        print("âœ… åé¦ˆå¾ªç¯å¤„ç†æµ‹è¯•é€šè¿‡")


# ==================== 5. åä½œè´¨é‡è¯„ä¼°æµ‹è¯• ====================

class TestQualityAssessment(TestBase):
    """æµ‹è¯•åä½œè´¨é‡è¯„ä¼°"""
    
    @pytest.mark.asyncio
    async def test_collaboration_metrics_calculation(self):
        """æµ‹è¯•åä½œæŒ‡æ ‡è®¡ç®—"""
        dm = DialogueManager()
        monitor = CollaborationQualityMonitor(dm)
        
        # åˆ›å»ºæµ‹è¯•ä¼šè¯
        session = await dm.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent_a",
            participants=["agent_a", "agent_b", "agent_c"],
            topic="Quality Test",
            goal="Test metrics"
        )
        
        # æ·»åŠ å‡è¡¡çš„æ¶ˆæ¯
        for i in range(9):
            sender = f"agent_{['a', 'b', 'c'][i % 3]}"
            await dm.add_message(
                dialogue_id=session.dialogue_id,
                sender_id=sender,
                content=f"Message {i+1}"
            )
        
        metrics = await monitor.evaluate_session(session.dialogue_id)
        
        assert metrics.dialogue_ratio == 1.0
        assert metrics.participation_balance > 0.8  # ç›¸å¯¹å‡è¡¡
        assert metrics.overall_score > 0
        
        print(f"âœ… æŒ‡æ ‡è®¡ç®—æµ‹è¯•é€šè¿‡: æ•´ä½“è¯„åˆ† {metrics.overall_score:.2%}")
    
    @pytest.mark.asyncio
    async def test_health_check(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        dm = DialogueManager()
        monitor = CollaborationQualityMonitor(dm)
        
        # åˆ›å»ºå¥åº·ä¼šè¯
        session = await dm.create_session(
            dialogue_type=DialogueType.DISCUSSION,
            initiator="agent_a",
            participants=["agent_a", "agent_b"],
            topic="Healthy Collaboration",
            goal="Test health"
        )
        
        # æ·»åŠ é«˜è´¨é‡å¯¹è¯
        prev_msg = None
        for i in range(8):
            sender = f"agent_{['a', 'b'][i % 2]}"
            msg = await dm.add_message(
                dialogue_id=session.dialogue_id,
                sender_id=sender,
                content=f"Response {i+1}",
                correlation_id=prev_msg.message_id if prev_msg else None
            )
            prev_msg = msg
        
        metrics = await monitor.evaluate_session(session.dialogue_id)
        
        assert metrics.is_healthy() == True
        assert metrics.dialogue_ratio >= 0.7
        assert metrics.response_rate >= 0.8
        
        print("âœ… å¥åº·æ£€æŸ¥æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_alert_generation(self):
        """æµ‹è¯•å‘Šè­¦ç”Ÿæˆ"""
        dm = DialogueManager()
        monitor = CollaborationQualityMonitor(dm)
        
        # åˆ›å»ºä½è´¨é‡ä¼šè¯
        session = await dm.create_session(
            dialogue_type=DialogueType.MONOLOGUE,  # ç‹¬ç™½ç±»å‹
            initiator="agent_a",
            participants=["agent_a"],
            topic="Low Quality",
            goal="Test alerts"
        )
        
        # åªæ·»åŠ ç‹¬ç™½æ¶ˆæ¯
        for i in range(5):
            await dm.add_message(
                dialogue_id=session.dialogue_id,
                sender_id="agent_a",
                content=f"Monologue {i+1}"
            )
        
        await monitor.evaluate_session(session.dialogue_id)
        
        # åº”è¯¥ç”Ÿæˆå‘Šè­¦
        assert len(monitor.alerts) > 0
        
        # æ£€æŸ¥å‘Šè­¦ç±»å‹
        alert_types = [a["type"] for a in monitor.alerts]
        assert "low_dialogue_ratio" in alert_types
        
        print("âœ… å‘Šè­¦ç”Ÿæˆæµ‹è¯•é€šè¿‡")


# ==================== 6. AGENTS.md v2.0é›†æˆæµ‹è¯• ====================

class TestAGENTSv2Integration(TestBase):
    """æµ‹è¯•AGENTS.md v2.0é›†æˆ"""
    
    @pytest.mark.asyncio
    async def test_three_layer_architecture(self):
        """æµ‹è¯•ä¸‰å±‚æ¶æ„"""
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        
        # éªŒè¯ä¸‰å±‚
        assert len(system.strategic_agents) == 3
        assert len(system.coordination_agents) == 3
        assert len(system.execution_agents) == 5
        assert len(system.base_system.agents) == 11
        
        print("âœ… ä¸‰å±‚æ¶æ„æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_soul_dimension_profiles(self):
        """æµ‹è¯•SOULç»´åº¦æ¡£æ¡ˆ"""
        profiles = {
            AgentRole.CEO: SoulDimensionProfile.from_role(AgentRole.CEO),
            AgentRole.DEVELOPER: SoulDimensionProfile.from_role(AgentRole.DEVELOPER),
            AgentRole.RESEARCHER: SoulDimensionProfile.from_role(AgentRole.RESEARCHER)
        }
        
        # CEOåº”è¯¥æœ‰é«˜personalityå’Œmotivations
        assert profiles[AgentRole.CEO].personality > 0.9
        assert profiles[AgentRole.CEO].motivations > 0.85
        
        # Researcheråº”è¯¥æœ‰é«˜curiosity
        assert profiles[AgentRole.RESEARCHER].curiosity > 0.9
        
        print("âœ… SOULç»´åº¦æ¡£æ¡ˆæµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_workflow_mode_execution(self):
        """æµ‹è¯•å·¥ä½œæµæ¨¡å¼æ‰§è¡Œ"""
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        
        # æµ‹è¯•é¡ºåºå·¥ä½œæµ
        result = await system.workflow_executor.execute_sequential(
            workflow_id="test_seq",
            steps=[
                {"name": "step1", "agent_id": "researcher", "task_type": "research", "description": "Research"},
                {"name": "step2", "agent_id": "developer", "task_type": "design", "description": "Design"}
            ],
            initial_input={"topic": "Test"}
        )
        
        assert "steps" in result
        assert "step1" in result["steps"]
        assert "step2" in result["steps"]
        
        print("âœ… å·¥ä½œæµæ¨¡å¼æ‰§è¡Œæµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_cross_layer_workflow(self):
        """æµ‹è¯•è·¨å±‚å·¥ä½œæµ"""
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        
        result = await system.execute_project_workflow(
            project_description="Test cross-layer project",
            requirements={"priority": "high"}
        )
        
        assert "strategic" in result
        assert "coordination" in result
        assert "execution" in result
        
        print("âœ… è·¨å±‚å·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_agent_role_capabilities(self):
        """æµ‹è¯•Agentè§’è‰²èƒ½åŠ›"""
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        
        # æ£€æŸ¥CEOçš„èƒ½åŠ›
        ceo = system.strategic_agents.get("ceo_kimi_claw")
        assert ceo is not None
        assert "strategic_planning" in ceo.definition.capabilities
        assert ceo.can_make_decision("strategic") == True
        
        # æ£€æŸ¥Developerçš„èƒ½åŠ›
        dev = system.execution_agents.get("developer")
        assert dev is not None
        assert "coding" in dev.definition.skills
        assert dev.can_make_decision("execution") == True
        assert dev.can_make_decision("strategic") == False
        
        print("âœ… Agentè§’è‰²èƒ½åŠ›æµ‹è¯•é€šè¿‡")


# ==================== 7. ç«¯åˆ°ç«¯é›†æˆæµ‹è¯• ====================

class TestEndToEnd(TestBase):
    """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_full_collaboration_flow(self):
        """æµ‹è¯•å®Œæ•´åä½œæµç¨‹"""
        print("\n" + "=" * 70)
        print("ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•: å®Œæ•´åä½œæµç¨‹")
        print("=" * 70)
        
        # 1. åˆ›å»ºç³»ç»Ÿ
        system = AGENTSv2CollaborationSystem()
        system.initialize_standard_team()
        
        # 2. åˆ›å»ºä»»åŠ¡
        task = CollaborationTask(
            task_type="feature_development",
            description="Implement Multi-Agent Collaboration Dashboard",
            goal="Create a dashboard to monitor agent collaboration",
            requirements={
                "capabilities": ["coding", "ui_design", "testing"],
                "num_agents": 3
            },
            priority=4
        )
        
        # 3. å¯åŠ¨åä½œ
        task_id, dialogue_id = await system.base_system.start_collaborative_task(
            task=task,
            dialogue_type=DialogueType.DISCUSSION,
            participants=["developer", "data_analyst", "qa_engineer"]
        )
        
        print(f"âœ… ä»»åŠ¡å¯åŠ¨: {task_id}")
        print(f"âœ… å¯¹è¯åˆ›å»º: {dialogue_id}")
        
        # 4. è¿è¡Œåä½œè½®æ¬¡
        for i in range(3):
            await system.base_system.run_collaboration_round(dialogue_id)
        
        session = await system.base_system.dialogue_manager.get_session(dialogue_id)
        print(f"âœ… åä½œå®Œæˆ: {len(session.messages)} æ¡æ¶ˆæ¯")
        
        # 5. è¯„ä¼°è´¨é‡
        metrics = await system.base_system.quality_monitor.evaluate_session(dialogue_id)
        
        print(f"\nğŸ“Š åä½œè´¨é‡æŠ¥å‘Š:")
        print(f"   - å¯¹è¯æ¯”ä¾‹: {metrics.dialogue_ratio:.1%}")
        print(f"   - å“åº”ç‡: {metrics.response_rate:.1%}")
        print(f"   - å‚ä¸å‡è¡¡: {metrics.participation_balance:.1%}")
        print(f"   - æ•´ä½“è¯„åˆ†: {metrics.overall_score:.1%}")
        print(f"   - å¥åº·çŠ¶æ€: {'âœ… å¥åº·' if metrics.is_healthy() else 'âš ï¸ éœ€æ”¹è¿›'}")
        
        # 6. å…³é—­ä¼šè¯
        await system.base_system.evaluate_and_close(dialogue_id)
        
        # 7. ç³»ç»ŸæŠ¥å‘Š
        report = system.get_architecture_report()
        print(f"\nğŸ“‹ ç³»ç»Ÿæ¶æ„æŠ¥å‘Š:")
        print(f"   - ç‰ˆæœ¬: {report['version']}")
        print(f"   - æ€»Agentæ•°: {report['architecture']['total_agents']}")
        print(f"   - å·¥ä½œæµæ¨¡å¼: {len(report['workflow_modes'])} ç§")
        
        # éªŒè¯
        assert metrics.dialogue_ratio >= 0.7
        assert len(session.messages) >= 6
        
        print("\n" + "=" * 70)
        print("âœ… ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•é€šè¿‡!")
        print("=" * 70)
    
    @pytest.mark.asyncio
    async def test_broadcast_inversion_fix_verification(self):
        """éªŒè¯å¹¿æ’­å€’è½¬é—®é¢˜å·²è§£å†³"""
        print("\n" + "=" * 70)
        print("å¹¿æ’­å€’è½¬é—®é¢˜ä¿®å¤éªŒè¯")
        print("=" * 70)
        
        system = MultiAgentCollaborationSystem()
        
        # åˆ›å»º4ä¸ªAgent
        agents = [
            ExampleCollaborativeAgent(f"agent_{i}", AgentRole.DEVELOPER, f"Dev_{i}")
            for i in range(4)
        ]
        
        for agent in agents:
            system.register_agent(agent)
        
        # åˆ›å»ºåä½œä»»åŠ¡
        task = CollaborationTask(
            task_type="collaborative_design",
            description="Design system architecture",
            goal="Create optimal architecture through collaboration"
        )
        
        task_id, dialogue_id = await system.start_collaborative_task(
            task=task,
            dialogue_type=DialogueType.DISCUSSION,
            participants=[a.agent_id for a in agents]
        )
        
        # è¿è¡Œå¤šè½®åä½œ
        for i in range(5):
            await system.run_collaboration_round(dialogue_id)
        
        # è·å–ä¼šè¯
        session = await system.dialogue_manager.get_session(dialogue_id)
        
        # è®¡ç®—æŒ‡æ ‡
        dialogue_ratio = session.get_dialogue_ratio()
        response_rate = session.get_response_rate()
        
        print(f"\nğŸ“Š åä½œç»Ÿè®¡:")
        print(f"   - æ€»æ¶ˆæ¯æ•°: {len(session.messages)}")
        print(f"   - å¯¹è¯æ¯”ä¾‹: {dialogue_ratio:.1%}")
        print(f"   - å“åº”ç‡: {response_rate:.1%}")
        
        # éªŒè¯å¹¿æ’­å€’è½¬é—®é¢˜å·²è§£å†³
        print(f"\nğŸ¯ å¹¿æ’­å€’è½¬é—®é¢˜ä¿®å¤éªŒè¯:")
        print(f"   ä¿®å¤å‰: 93%ç‹¬ç™½ / 7%å¯¹è¯")
        print(f"   ä¿®å¤å: {(1-dialogue_ratio)*100:.0f}%ç‹¬ç™½ / {dialogue_ratio*100:.0f}%å¯¹è¯")
        
        assert dialogue_ratio >= 0.70, f"å¯¹è¯æ¯”ä¾‹ {dialogue_ratio:.1%} æœªè¾¾åˆ°70%ç›®æ ‡"
        assert response_rate >= 0.80, f"å“åº”ç‡ {response_rate:.1%} æœªè¾¾åˆ°80%ç›®æ ‡"
        
        print(f"\nâœ… å¹¿æ’­å€’è½¬é—®é¢˜å·²è§£å†³!")
        print(f"âœ… å¯¹è¯æ¯”ä¾‹ä»7%æå‡åˆ° {dialogue_ratio:.1%}")
        print("=" * 70)


# ==================== è¿è¡Œæµ‹è¯• ====================

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    
    print("\n" + "=" * 70)
    print("Multi-Agent Collaboration System v3.0 - å…¨é¢æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    test_classes = [
        TestDialogueProtocol(),
        TestBroadcastInversionFix(),
        TestDeepDialogueMechanism(),
        TestTaskAllocation(),
        TestQualityAssessment(),
        TestAGENTSv2Integration(),
        TestEndToEnd()
    ]
    
    passed = 0
    failed = 0
    
    for test_class in test_classes:
        class_name = test_class.__class__.__name__
        print(f"\nğŸ“¦ æµ‹è¯•ç±»: {class_name}")
        print("-" * 50)
        
        methods = [m for m in dir(test_class) if m.startswith("test_")]
        
        for method_name in methods:
            try:
                method = getattr(test_class, method_name)
                if asyncio.iscoroutinefunction(method):
                    await method()
                else:
                    method()
                passed += 1
            except Exception as e:
                print(f"âŒ {method_name} å¤±è´¥: {e}")
                failed += 1
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    print(f"ğŸ“Š æ€»è®¡: {passed + failed}")
    print(f"ğŸ¯ é€šè¿‡ç‡: {passed/(passed+failed)*100:.1f}%")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Multi-Agentåä½œç³»ç»Ÿv3.0å·²å°±ç»ª!")
    
    print("=" * 70)
    
    return failed == 0


if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    exit(0 if success else 1)
