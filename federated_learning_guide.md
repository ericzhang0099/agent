# 联邦学习（Federated Learning）- 分布式隐私保护技术深度解析

> **文档版本**: v1.0  
> **最后更新**: 2026-02-27  
> **研究范围**: FedAvg算法、差分隐私、安全聚合、Non-IID处理

---

## 目录

1. [联邦学习概述](#1-联邦学习概述)
2. [联邦平均算法（FedAvg）](#2-联邦平均算法fedavg)
3. [差分隐私保护](#3-差分隐私保护)
4. [安全聚合](#4-安全聚合)
5. [非独立同分布（Non-IID）处理](#5-非独立同分布non-iid处理)
6. [总结与展望](#6-总结与展望)

---

## 1. 联邦学习概述

### 1.1 什么是联邦学习？

联邦学习（Federated Learning, FL）是一种分布式机器学习范式，允许多个参与方在不共享原始数据的情况下协作训练模型。其核心思想是**"数据不动模型动"**，即各参与方在本地训练模型，只将模型参数（如梯度或权重）上传到中央服务器进行聚合。

### 1.2 联邦学习的分类

| 类型 | 描述 | 适用场景 |
|------|------|----------|
| **横向联邦学习（HFL）** | 参与方拥有不同样本但相同特征空间 | 跨设备学习（如手机键盘预测） |
| **纵向联邦学习（VFL）** | 参与方拥有相同样本但不同特征 | 跨机构协作（如银行与电商） |
| **迁移联邦学习（TFL）** | 样本和特征空间都有限重叠 | 跨领域知识迁移 |

### 1.3 核心挑战

```
┌─────────────────────────────────────────────────────────────────┐
│                    联邦学习核心挑战                              │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐             │
│  │  数据异构性  │  │  隐私泄露   │  │  通信开销   │             │
│  │  (Non-IID)  │  │   风险     │  │   限制     │             │
│  └─────────────┘  └─────────────┘  └─────────────┘             │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐             │
│  │  系统异构性  │  │  拜占庭节点 │  │  公平性问题 │             │
│  │             │  │   攻击     │  │            │             │
│  └─────────────┘  └─────────────┘  └─────────────┘             │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. 联邦平均算法（FedAvg）

### 2.1 算法原理

联邦平均算法（Federated Averaging, FedAvg）由 McMahan 等人于 2017 年提出，是联邦学习最基础的聚合算法。

#### 核心思想

FedAvg 结合了**本地多轮训练**和**周期性聚合**，减少通信轮次的同时保持模型收敛。

#### 数学定义

**全局目标函数：**

$$
w^* \triangleq \min_w F(w), \quad \text{其中} \quad F(w) = \sum_{k=1}^{K} p_k L_k(w)$$

其中：
- $K$：参与客户端数量
- $p_k$：客户端 $k$ 的聚合权重（通常为数据量占比 $n_k/n$）
- $L_k(w)$：客户端 $k$ 的本地损失函数

**FedAvg 更新规则：**

1. **本地更新**（执行 $E$ 个 epoch）：
   $$w_{t+1}^k = w_t - \eta \nabla L_k(w_t)$$

2. **服务器聚合**：
   $$w_{t+1} = \sum_{k=1}^{K} p_k w_{t+1}^k$$

### 2.2 算法流程

```
算法: FedAvg
─────────────────────────────────────────────────────────────
输入: K个客户端，本地epoch数E，批次大小B，学习率η
输出: 全局模型w_T
─────────────────────────────────────────────────────────────
服务器执行:
  初始化全局模型w_0
  for t = 1, 2, ..., T do
    S_t ← 随机选择m个客户端（或全部）
    for 每个客户端k ∈ S_t do
      w_{t+1}^k ← ClientUpdate(k, w_t)
    end for
    w_{t+1} ← Σ_{k∈S_t} (n_k/n) * w_{t+1}^k  // 加权平均
  end for

ClientUpdate(k, w):
  w' ← w
  for i = 1, 2, ..., E do
    for batch b ∈ 客户端k的数据 do
      w' ← w' - η∇L(w'; b)  // SGD更新
    end for
  end for
  return w'
─────────────────────────────────────────────────────────────
```

### 2.3 算法特性

| 特性 | 说明 |
|------|------|
| **通信效率** | 本地多轮训练减少通信轮次 |
| **计算并行** | 各客户端独立训练，天然并行 |
| **收敛性** | 在凸问题上保证收敛，非凸问题经验收敛 |
| **隐私基础** | 原始数据不出本地，提供基础隐私保护 |

### 2.4 超参数影响

- **本地epoch数 E**：
  - E 越大 → 通信轮次减少，但可能增加本地计算和收敛不稳定
  - E = 1 时退化为 FedSGD

- **客户端采样比例**：
  - 部分参与可减少每轮通信开销
  - 但可能增加收敛所需的轮次

---

## 3. 差分隐私保护

### 3.1 差分隐私基础

#### 定义

**$(\epsilon, \delta)$-差分隐私**：随机算法 $\mathcal{A}$ 满足 $(\epsilon, \delta)$-DP，如果对于任意两个相邻数据集 $D, D'$（相差一个样本）和任意输出集合 $S$：

$$\Pr[\mathcal{A}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D') \in S] + \delta$$

- **$\epsilon$（隐私预算）**：越小隐私保护越强
- **$\delta$（失败概率）**：通常设为 $1/|D|$ 或更小

#### 隐私级别分类

| 级别 | 定义 | 保护对象 |
|------|------|----------|
| **样本级DP (SL-DP)** | 相邻数据集相差一个样本 | 单个数据记录 |
| **客户端级DP (CL-DP)** | 相邻数据集相差一个客户端 | 单个参与方 |
| **用户级DP (UL-DP)** | 相邻数据集相差一个用户的所有记录 | 单个用户 |

### 3.2 DP-FedAvg 实现

#### 高斯机制

在模型参数上添加高斯噪声：

$$\tilde{w} = w + \mathcal{N}(0, \sigma^2 C^2 \mathbf{I})$$

其中：
- $C$：梯度裁剪阈值（控制敏感度）
- $\sigma$：噪声乘数（由隐私预算决定）

#### 完整算法流程

```
算法: DP-FedAvg (客户端级差分隐私)
─────────────────────────────────────────────────────────────
输入: 隐私预算(ε, δ), 裁剪阈值C, 噪声乘数σ
─────────────────────────────────────────────────────────────
客户端k执行:
  接收全局模型w_t
  在本地数据上训练E个epoch得到w_{t+1}^k
  
  // 梯度裁剪
  Δw_k = w_{t+1}^k - w_t
  Δw̃_k = Δw_k / max(1, ||Δw_k||_2 / C)  // L2裁剪
  
  // 添加噪声
  n_k ~ N(0, σ²C²I)
  w̃_{t+1}^k = w_t + Δw̃_k + n_k
  
  上传w̃_{t+1}^k到服务器

服务器执行:
  聚合: w_{t+1} = (1/K) Σ_k w̃_{t+1}^k
  // 噪声在聚合后相互抵消一部分
─────────────────────────────────────────────────────────────
```

### 3.3 隐私预算计算

#### 矩会计（Moments Accountant）

用于跟踪多轮训练的累积隐私损失：

$$\epsilon \approx q\sqrt{T} \cdot \frac{\sqrt{2\ln(1/\delta)}}{\sigma}$$

其中：
- $q = m/K$：客户端采样率
- $T$：通信轮次
- $\sigma$：噪声乘数

#### 隐私预算分配策略

| 策略 | 描述 |
|------|------|
| **均匀分配** | 每轮使用相同的隐私预算 |
| **自适应分配** | 根据训练进度动态调整 |
| **个性化分配** | 不同客户端使用不同预算 |

### 3.4 本地差分隐私（LDP）

当不信任中央服务器时，使用本地差分隐私：

$$\mathcal{R}(x) = x + \text{Lap}(0, \frac{\Delta f}{\epsilon})$$

**常用机制**：
- **拉普拉斯机制**：添加拉普拉斯噪声
- **随机响应**：以一定概率翻转输出
- **Duchi机制**：适用于高维数据
- **分段机制（PM）**：平衡噪声方差

---

## 4. 安全聚合

### 4.1 安全聚合概述

安全聚合（Secure Aggregation, SA）确保服务器只能看到聚合后的模型更新，无法获取单个客户端的更新。

**威胁模型**：
- 诚实但好奇的服务器
- 外部窃听者

### 4.2 基于加密的聚合方案

#### 安全多方计算（SMPC）

```
┌─────────────────────────────────────────────────────────────────┐
│                    安全聚合流程                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   客户端1        客户端2        客户端3           服务器          │
│      │              │              │                │            │
│      │  w₁+s₁₂-s₁₃ │              │                │            │
│      ├──────────────┼──────────────┤                │            │
│      │              │  w₂+s₂₃-s₂₁  │                │            │
│      │              ├──────────────┼──────────────►│            │
│      │              │              │  w₃+s₃₁-s₃₂   │            │
│      │              │              ├──────────────►│            │
│      │              │              │                │            │
│      │◄─────────────┴──────────────┴────────────────┤            │
│      │              聚合结果: w₁+w₂+w₃               │            │
│      │              (掩码相互抵消)                    │            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### 密钥协商流程

1. **成对密钥生成**：每对客户端协商共享密钥
2. **掩码添加**：每个客户端用共享密钥生成掩码
3. **安全聚合**：服务器聚合时掩码相互抵消

### 4.3 离散高斯机制与安全聚合

传统高斯机制使用连续噪声，与安全聚合的有限域运算不兼容。解决方案：

#### 离散高斯分布

$$\forall x \in \mathbb{Z}: \quad \mathbb{P}[X=x] = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sum_{y \in \mathbb{Z}} e^{-(y-\mu)^2/2\sigma^2}}$$

#### Skellam机制

使用两个独立泊松分布之差生成噪声，天然支持加法封闭性：

$$X \sim \text{Sk}_{\Delta, \mu}: \quad P(X=k) = e^{-\mu} I_{k-\Delta}(\mu)$$

### 4.4 安全聚合协议对比

| 协议 | 通信复杂度 | 计算复杂度 | 容错性 | 适用场景 |
|------|-----------|-----------|--------|----------|
| **Bonawitz et al.** | $O(K^2)$ | $O(K)$ | 可容忍部分掉线 | 中小规模FL |
| **基于LWE的方案** | $O(K)$ | $O(K)$ | 较好 | 大规模FL |
| **TEE辅助方案** | $O(K)$ | $O(1)$ | 依赖硬件 | 高安全需求 |

---

## 5. 非独立同分布（Non-IID）处理

### 5.1 Non-IID数据问题

在真实场景中，各客户端的数据通常**非独立同分布**（Non-IID），主要表现为：

| 类型 | 描述 | 示例 |
|------|------|------|
| **特征分布偏移** | 相同标签，不同特征分布 | 不同手机拍摄的照片风格差异 |
| **标签分布偏移** | 不同客户端标签分布不同 | 不同地区用户偏好差异 |
| **标签空间不一致** | 各客户端拥有不同标签子集 | 不同医院诊断不同疾病 |
| **数据量不平衡** | 各客户端数据量差异大 | 活跃用户vs冷启动用户 |

### 5.2 Non-IID对FL的影响

```
┌─────────────────────────────────────────────────────────────────┐
│              Non-IID导致的问题                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   全局模型 ←─────────────────────────────── 客户端本地模型        │
│      │                                          │               │
│      │    本地训练方向发散                        │               │
│      ▼                                          ▼               │
│   ┌─────────┐    ┌─────────┐    ┌─────────┐                    │
│   │  客户端A │    │  客户端B │    │  客户端C │                    │
│   │  (猫多)  │    │  (狗多)  │    │ (鸟多)  │                    │
│   └────┬────┘    └────┬────┘    └────┬────┘                    │
│        │              │              │                          │
│        └──────────────┼──────────────┘                          │
│                       ▼                                          │
│                 聚合后模型偏向                                    │
│              (某些类别性能下降)                                    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 5.3 解决Non-IID的算法

#### FedProx

在本地目标函数中添加近端项：

$$\min_w \left[ L_k(w) + \frac{\mu}{2} ||w - w_t||^2 \right]$$

- 限制本地模型偏离全局模型过远
- $\mu$ 控制正则化强度

#### SCAFFOLD

使用控制变量（control variates）校正本地更新方向：

$$w_{t+1}^k = w_t - \eta (g_k(w_t) - c_k + c)$$

其中 $c_k$ 和 $c$ 分别记录客户端和服务器的梯度修正项。

#### FedNova

解决本地epoch数不同导致的聚合偏差：

$$w_{t+1} = w_t - \eta \sum_{k=1}^{K} \frac{n_k}{n} \cdot \frac{\Delta w_k}{\tau_k}$$

其中 $\tau_k$ 是客户端 $k$ 的本地步数。

### 5.4 数据异构性与隐私的交互

**研究发现**：Non-IID数据会加剧差分隐私噪声的负面影响

- 在极端Non-IID情况下，相同噪声强度导致的精度下降比IID情况高2-3倍
- 原因：Non-IID导致梯度方向更加分散，噪声进一步放大这种分散

**缓解策略**：
1. **自适应噪声调整**：根据数据异构程度动态调整噪声
2. **分层聚合**：先按数据分布聚类，再分层聚合
3. **个性化DP**：不同客户端使用不同的隐私预算

---

## 6. 总结与展望

### 6.1 技术对比总结

| 技术 | 核心思想 | 优势 | 局限 |
|------|----------|------|------|
| **FedAvg** | 本地训练+加权聚合 | 简单高效，通信友好 | Non-IID下性能下降 |
| **差分隐私** | 添加校准噪声 | 数学可证明的隐私保证 | 模型精度损失 |
| **安全聚合** | 加密保护个体更新 | 防止服务器窥探 | 通信开销增加 |
| **FedProx** | 近端正则化 | 缓解Non-IID问题 | 需调参 |

### 6.2 前沿研究方向

1. **大模型联邦学习**
   - DP-LoRA：低秩适配的差分隐私微调
   - 通信高效的联邦大模型训练

2. **用户级差分隐私**
   - 保护单个用户的所有贡献
   - 处理用户数据量不平衡问题

3. **隐私审计技术**
   -  membership inference攻击评估
   -  数据重建攻击评估

4. **跨域联邦学习**
   - 处理特征空间不一致
   - 结合迁移学习与差分隐私

5. **多模态联邦学习**
   - 保护复杂数据组合的隐私
   - 防止跨模态信息泄露

### 6.3 实践建议

```
┌─────────────────────────────────────────────────────────────────┐
│                    联邦学习实践建议                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. 基础场景（数据相对均衡）                                      │
│     → 使用FedAvg + 样本级DP                                      │
│                                                                  │
│  2. 高异构场景（Non-IID严重）                                     │
│     → 使用FedProx/SCAFFOLD + 自适应DP                            │
│                                                                  │
│  3. 高隐私需求（不信任服务器）                                    │
│     → 使用LDP + 安全聚合                                         │
│                                                                  │
│  4. 大规模跨设备场景                                              │
│     → 使用客户端级DP + 离散高斯机制 + 安全聚合                    │
│                                                                  │
│  5. 跨机构协作（数据量差异大）                                    │
│     → 使用个性化隐私预算 + 分层聚合                               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 参考文献

1. McMahan et al. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. AISTATS.
2. Dwork et al. (2014). The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science.
3. Bonawitz et al. (2017). Practical Secure Aggregation for Privacy-Preserving Machine Learning. CCS.
4. Li et al. (2020). Federated Optimization in Heterogeneous Networks. MLSys.
5. Karimireddy et al. (2020). SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. ICML.
6. Fu et al. (2024). Differentially Private Federated Learning: A Systematic Review. arXiv:2405.08299.

---

*本文档由AI助手基于最新研究文献整理生成，仅供学习参考。*
